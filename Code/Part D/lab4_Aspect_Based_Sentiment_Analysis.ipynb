{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hExKCzh6doIW"
      },
      "source": [
        "# Lab 4 - Aspect-Based Sentiment Analysis\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HixoFOoCIJ7V"
      },
      "source": [
        "In this session, we demonstrate how to deal with the aspect-based sentiment analysis (ABSA). You can find the whole task description from (https://aclanthology.org/D19-1654.pdf).\n",
        "This task provides a review text dataset with aspect.\n",
        "Given a review and an aspect, we need to classify the sentiment conveyed towards that aspect on a  three-point scale:   POSITIVE, NEUTRAL, and NEGATIVE.\n",
        "This is a multi-class classification task, and it needs to analyze the text and its aspect. \n",
        "\n",
        "Same as before, we are going to use Keras Sequential API in this session. The Sequential API allows you to make models layer-by-layer. You could modify the previous models to fit in the new task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8fpBfhBpupy"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers import LSTM, RNN, Dropout, Input, LeakyReLU, Bidirectional,Conv1D, GlobalMaxPooling1D\n",
        "from keras.layers.core import Dense\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqvPQvgvPv1W"
      },
      "source": [
        "### Downloading and preprocessing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EundMtGPpCdf"
      },
      "source": [
        "Unlike the IMDB dataset that is included and preprocessed by the Keras, the dataset we will be using is the aspect-term sentiment analysis (ATSA) dataset, which consists of 5297 labeled reviews. These are split into 4,297 reviews for training and 500 reviews for testing and validation, respectively. \n",
        "\n",
        "For ATSA, the annotators extract aspect terms in the sentences and label the sentiment polarities with respect to the  aspect  terms.   The  sentences  that  consist  of only one aspect term or multiple aspects with the same  sentiment  polarities  are  deleted.  ATSA also provides the start and end positions in a sentence for each aspect term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P27sPFg7f0CJ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "def downloadfile(url):\n",
        "  rq = requests.get(url)\n",
        "  open(url.split('/')[-1], 'wb').write(rq.content)\n",
        "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/train.xml')\n",
        "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/val.xml')\n",
        "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/test.xml')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3Voc6S_gT2X",
        "outputId": "77b01daf-ee8f-48f3-c3b4-759093f5c457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training entries: 11186\n",
            "Test entries: 1336\n"
          ]
        }
      ],
      "source": [
        "# The code is modified from https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data_process/utils.py\n",
        "from xml.etree.ElementTree import parse\n",
        "\n",
        "def parse_sentence_term(path, lowercase=False):\n",
        "    tree = parse(path)\n",
        "    sentences = tree.getroot()\n",
        "    data = []\n",
        "    split_char = '__split__'\n",
        "    for sentence in sentences:\n",
        "        text = sentence.find('text')\n",
        "        if text is None:\n",
        "            continue\n",
        "        text = text.text\n",
        "        if lowercase:\n",
        "            text = text.lower()\n",
        "        aspectTerms = sentence.find('aspectTerms')\n",
        "        if aspectTerms is None:\n",
        "            continue\n",
        "        for aspectTerm in aspectTerms:\n",
        "            term = aspectTerm.get('term')\n",
        "            if lowercase:\n",
        "                term = term.lower()\n",
        "            polarity = aspectTerm.get('polarity')\n",
        "            start = aspectTerm.get('from')\n",
        "            end = aspectTerm.get('to')\n",
        "            piece = [text , term,  polarity , start , end]\n",
        "            data.append(piece)\n",
        "    return data\n",
        "train = parse_sentence_term(\"train.xml\",True)\n",
        "val = parse_sentence_term(\"val.xml\",True)\n",
        "test = parse_sentence_term(\"test.xml\",True)\n",
        "\n",
        "print(\"Training entries: {}\".format(len(train)))\n",
        "print(\"Test entries: {}\".format(len(test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U4iCV9-rmay"
      },
      "source": [
        "We now can start playing around with the data, letâ€™s first see some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-gjWRAuqg5s",
        "outputId": "8d211ca2-9674-40c4-d87c-692ecef6612a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SENTENCE \t ASPECT \t LABEL \t ASPECT-START-INDEX \t ASPECT-END-INDEX\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'decor', 'negative', '4', '9']\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'food', 'positive', '42', '46']\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'prices', 'positive', '59', '65']\n",
            "['when tables opened up, the manager sat another party before us.', 'tables', 'neutral', '5', '11']\n",
            "['when tables opened up, the manager sat another party before us.', 'manager', 'negative', '27', '34']\n"
          ]
        }
      ],
      "source": [
        "print(\"SENTENCE \\t ASPECT \\t LABEL \\t ASPECT-START-INDEX \\t ASPECT-END-INDEX\")\n",
        "print(train[0])\n",
        "print(train[1])\n",
        "print(train[2])\n",
        "print(train[3])\n",
        "print(train[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTRZrpcyr-4x"
      },
      "source": [
        "We could use this dataset to try an \"unknown aspect\" task, if we assume that the ASPECT, LABEL and START/END-INDEX fields are what the model must predict. But here we will attempt a simpler \"known aspect\" task: we will assume that we know ASPECT and START/END-INDEX and the model must just predict the LABEL for a given combination of aspect and sentence.\n",
        "\n",
        "First, build a vocabulary based on the training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79Ev72Kgq4XL",
        "outputId": "46bf2603-bf64-4528-da7f-04952174f2c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7894\n",
            "7898\n"
          ]
        }
      ],
      "source": [
        "voc = []\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "for example in train:\n",
        "  text_tokens = text_to_word_sequence(example[0])\n",
        "  aspect_tokens = text_to_word_sequence(example[1])\n",
        "  voc.extend(aspect_tokens)\n",
        "  voc.extend(text_tokens)\n",
        "voc = set(voc)\n",
        "print(len(voc))\n",
        "\n",
        "word_index = dict()\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  \n",
        "word_index[\"<EOS>\"] = 3\n",
        "for w in voc:\n",
        "  word_index[w] = len(word_index)\n",
        "print(len(word_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvuu4KhStqei"
      },
      "source": [
        "According to the word_index and the tokenizer function (text_to_word_sequence), we can convert the review text and aspect words to word tokens and integers separately:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMCH1OoDrSNR",
        "outputId": "6ab3c62a-ee8b-4852-ffb9-8012e4082c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11186\n",
            "11186\n",
            "x_train_review[0]:\n",
            "['the', 'decor', 'is', 'not', 'special', 'at', 'all', 'but', 'their', 'food', 'and', 'amazing', 'prices', 'make', 'up', 'for', 'it']\n",
            "x_train_aspect[0]:\n",
            "['decor']\n",
            "x_train_review_int[0]:\n",
            "[5091, 3809, 2213, 2458, 1226, 6865, 7811, 3861, 908, 6802, 2702, 769, 7022, 2873, 7248, 6182, 491]\n",
            "x_train_aspect_int[0]:\n",
            "[3809]\n"
          ]
        }
      ],
      "source": [
        "# Please write your code to generate the following data\n",
        "\n",
        "x_train_aspect = []\n",
        "x_train_review = []\n",
        "for example in train:\n",
        "    text_tokens = text_to_word_sequence(example[0])\n",
        "    aspect_tokens = text_to_word_sequence(example[1])\n",
        "    x_train_review.append(text_tokens)\n",
        "    x_train_aspect.append(aspect_tokens)\n",
        "\n",
        "print(len(train))\n",
        "print(len(x_train_review))\n",
        "\n",
        "x_train_review_int = []\n",
        "for review in x_train_review:\n",
        "    review_int = []\n",
        "    for word in review:\n",
        "        if word in word_index:\n",
        "            review_int.append(word_index[word])\n",
        "        else:\n",
        "            review_int.append(word_index[\"<UNK>\"])\n",
        "    x_train_review_int.append(review_int)\n",
        "\n",
        "\n",
        "x_train_aspect_int =  []\n",
        "for aspect in x_train_aspect:\n",
        "    aspect_int = []\n",
        "    for word in aspect:\n",
        "        if word in word_index:\n",
        "            aspect_int.append(word_index[word])\n",
        "        else:\n",
        "            aspect_int.append(word_index[\"<UNK>\"])\n",
        "    x_train_aspect_int.append(aspect_int)\n",
        "\n",
        "x_dev_review = []\n",
        "for example in val:\n",
        "    text_tokens = text_to_word_sequence(example[0])\n",
        "    aspect_tokens = text_to_word_sequence(example[1])\n",
        "    x_dev_review.append(text_tokens)\n",
        "\n",
        "x_dev_aspect = []\n",
        "for example in val:\n",
        "    text_tokens = text_to_word_sequence(example[0])\n",
        "    aspect_tokens = text_to_word_sequence(example[1])\n",
        "    x_dev_aspect.append(aspect_tokens)\n",
        "\n",
        "x_dev_review_int = []\n",
        "# find the index of each word in the vocab\n",
        "for review in x_dev_review:\n",
        "    review_int = []\n",
        "    for word in review:\n",
        "        if word in word_index:\n",
        "            review_int.append(word_index[word])\n",
        "        else:\n",
        "            review_int.append(word_index[\"<UNK>\"])\n",
        "    x_dev_review_int.append(review_int)\n",
        "\n",
        "x_dev_aspect_int =  []\n",
        "for aspect in x_dev_aspect:\n",
        "    aspect_int = []\n",
        "    for word in aspect:\n",
        "        if word in word_index:\n",
        "            aspect_int.append(word_index[word])\n",
        "        else:\n",
        "            aspect_int.append(word_index[\"<UNK>\"])\n",
        "    x_dev_aspect_int.append(aspect_int)\n",
        "\n",
        "x_test_review = []\n",
        "for example in test:\n",
        "    text_tokens = text_to_word_sequence(example[0])\n",
        "    aspect_tokens = text_to_word_sequence(example[1])\n",
        "    x_test_review.append(text_tokens)\n",
        "\n",
        "x_test_aspect = []\n",
        "for example in test:\n",
        "    text_tokens = text_to_word_sequence(example[0])\n",
        "    aspect_tokens = text_to_word_sequence(example[1])\n",
        "    x_test_aspect.append(aspect_tokens)\n",
        "\n",
        "x_test_review_int = []\n",
        "# find the index of each word in the vocab\n",
        "for review in x_test_review:\n",
        "    review_int = []\n",
        "    for word in review:\n",
        "        if word in word_index:\n",
        "            review_int.append(word_index[word])\n",
        "        else:\n",
        "            review_int.append(word_index[\"<UNK>\"])\n",
        "    x_test_review_int.append(review_int)\n",
        "\n",
        "x_test_aspect_int =  []\n",
        "for aspect in x_test_aspect:\n",
        "    aspect_int = []\n",
        "    for word in aspect:\n",
        "        if word in word_index:\n",
        "            aspect_int.append(word_index[word])\n",
        "        else:\n",
        "            aspect_int.append(word_index[\"<UNK>\"])\n",
        "    x_test_aspect_int.append(aspect_int)\n",
        "\n",
        "assert len(x_train_aspect) == len(train)\n",
        "assert len(x_train_aspect) == len(x_train_aspect_int)\n",
        "assert len(x_test_aspect) == len(test)\n",
        "assert len(x_test_aspect) == len(x_test_aspect_int)\n",
        "print(\"x_train_review[0]:\")\n",
        "print(x_train_review[0])\n",
        "print(\"x_train_aspect[0]:\")\n",
        "print(x_train_aspect[0])\n",
        "print(\"x_train_review_int[0]:\")\n",
        "print(x_train_review_int[0])\n",
        "print(\"x_train_aspect_int[0]:\")\n",
        "print(x_train_aspect_int[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IreFXgruZot"
      },
      "source": [
        "We use 4 to represent \"positive\", 2 for \"neutral\", and 1 for \"negative\". Then we can convert the lables to numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abIb7Fe5u3GQ",
        "outputId": "62e1cd92-f718-47e3-92aa-9cc81297551d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1]\n",
            "[1 0 0]\n",
            "[1 0 0]\n",
            "[0 1 0]\n",
            "[0 0 1]\n"
          ]
        }
      ],
      "source": [
        "def label2int(dataset):\n",
        "  y = []\n",
        "  for example in dataset:\n",
        "    if example[2].lower() == \"negative\":\n",
        "      y.append([0,0,1])\n",
        "    elif example[2].lower() == \"neutral\":\n",
        "      y.append([0,1,0])\n",
        "    else:\n",
        "      # assert example[2].lower() == \"positive\"\n",
        "      y.append([1,0,0])\n",
        "  return y\n",
        "  \n",
        "y_train = label2int(train)\n",
        "y_dev = label2int(val)\n",
        "y_test = label2int(test)\n",
        "y_train = np.array(y_train)\n",
        "y_dev = np.array(y_dev)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train[1])\n",
        "print(y_train[2])\n",
        "print(y_train[3])\n",
        "print(y_train[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnnSuspvC5b"
      },
      "source": [
        "Now we have almost done the data preprocessing. Unlike the previous lab, there are two x (review and aspect) to input the model in here. The easiest way is to combine the review and aspect into one sentence and then input it into the model. Thus we can use the previous model directly.\n",
        "\n",
        "(This means our model is similar to a simplified version of the Vo & Zhang model from the lectures: we have an input sequence containing an aspect embedding paired with the sentence word embeddings (but not separating into left & right sentence context as Vo & Zhang do)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKOiVVXQu-_I",
        "outputId": "baf5d5b9-d1ca-486a-ee4f-ca643e4ab5eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before paded:\n",
            "['decor', '<START>', 'the', 'decor', 'is', 'not', 'special', 'at', 'all', 'but', 'their', 'food', 'and', 'amazing', 'prices', 'make', 'up', 'for', 'it']\n",
            "[5091, 3809, 2213, 2458, 1226, 6865, 7811, 3861, 908, 6802, 2702, 769, 7022, 2873, 7248, 6182, 491, 1, 3809]\n",
            "After paded:\n",
            "[5091 3809 2213 2458 1226 6865 7811 3861  908 6802 2702  769 7022 2873\n",
            " 7248 6182  491    1 3809    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ]
        }
      ],
      "source": [
        "# Please write your code to combine the x_*_review and x_*_aspect into the following varibles\n",
        "import keras\n",
        "\n",
        "x_train = []\n",
        "x_train_int = []\n",
        "x_dev = []\n",
        "x_dev_int = []\n",
        "x_test = []\n",
        "x_test_int = []\n",
        "\n",
        "for i in range(len(x_train_review)):\n",
        "    x_train.append(x_train_aspect[i] + [\"<START>\"] + x_train_review[i])\n",
        "    x_train_int.append(x_train_review_int[i] + [word_index[\"<START>\"]] + x_train_aspect_int[i])\n",
        "\n",
        "for i in range(len(x_dev_review)):\n",
        "    x_dev.append(x_dev_review[i] + [\"<START>\"] + x_dev_aspect[i])\n",
        "    x_dev_int.append(x_dev_review_int[i] + [word_index[\"<START>\"]] + x_dev_aspect_int[i])\n",
        "\n",
        "for i in range(len(x_test_review)):\n",
        "    x_test.append(x_test_review[i] + [\"<START>\"] + x_test_aspect[i])\n",
        "    x_test_int.append(x_test_review_int[i] + [word_index[\"<START>\"]] + x_test_aspect_int[i])\n",
        "\n",
        "x_train_pad = keras.preprocessing.sequence.pad_sequences(x_train_int,\n",
        "                                          value=word_index[\"<PAD>\"],\n",
        "                                          padding='post',\n",
        "                                          maxlen=128)\n",
        "\n",
        "x_dev_pad = keras.preprocessing.sequence.pad_sequences(x_dev_int,\n",
        "                                          value=word_index[\"<PAD>\"],\n",
        "                                          padding='post',\n",
        "                                          maxlen=128)\n",
        "\n",
        "x_test_pad = keras.preprocessing.sequence.pad_sequences(x_test_int,\n",
        "                                          value=word_index[\"<PAD>\"],\n",
        "                                          padding='post',\n",
        "                                          maxlen=128)\n",
        "\n",
        "print(\"Before paded:\")\n",
        "print(x_train[0])\n",
        "print(x_train_int[0])\n",
        "print(\"After paded:\")\n",
        "print(x_train_pad[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7OwOQw4h8RX"
      },
      "source": [
        "#Model 1: Previous models without pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CuAHDGQ3Mmp"
      },
      "source": [
        "## Model 1-1: Neural bag of words without pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-QzOMO_P4jc"
      },
      "source": [
        "Now we use the model2 in lab4 to deal with our task. However, the previous model works only for the binary classification task. Therefore, we need to modify the output layer to fix the multi-class problem. You can read this tutorial for more details: https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yi04MLIvJOGZ"
      },
      "outputs": [],
      "source": [
        "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
        "    def call(self, x, mask=None):\n",
        "        if mask != None:\n",
        "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
        "        else:\n",
        "            return super().call(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFrCsL-NBFVL",
        "outputId": "219e1ae8-7e51-468a-fa45-2718226b29c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 128, 100)          789800    \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 100)              0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                1616      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 791,467\n",
            "Trainable params: 791,467\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "22/22 [==============================] - 6s 45ms/step - loss: 1.0778 - accuracy: 0.4507 - val_loss: 1.0654 - val_accuracy: 0.4535\n",
            "Epoch 2/30\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 1.0643 - accuracy: 0.4507 - val_loss: 1.0603 - val_accuracy: 0.4535\n",
            "Epoch 3/30\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 1.0590 - accuracy: 0.4507 - val_loss: 1.0560 - val_accuracy: 0.4535\n",
            "Epoch 4/30\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 1.0535 - accuracy: 0.4507 - val_loss: 1.0501 - val_accuracy: 0.4535\n",
            "Epoch 5/30\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0452 - accuracy: 0.4507 - val_loss: 1.0418 - val_accuracy: 0.4535\n",
            "Epoch 6/30\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 1.0341 - accuracy: 0.4524 - val_loss: 1.0312 - val_accuracy: 0.4572\n",
            "Epoch 7/30\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0202 - accuracy: 0.4666 - val_loss: 1.0194 - val_accuracy: 0.4625\n",
            "Epoch 8/30\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0042 - accuracy: 0.4918 - val_loss: 1.0048 - val_accuracy: 0.4857\n",
            "Epoch 9/30\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.9858 - accuracy: 0.5118 - val_loss: 0.9899 - val_accuracy: 0.4970\n",
            "Epoch 10/30\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.9658 - accuracy: 0.5206 - val_loss: 0.9733 - val_accuracy: 0.5135\n",
            "Epoch 11/30\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.9428 - accuracy: 0.5275 - val_loss: 0.9559 - val_accuracy: 0.5120\n",
            "Epoch 12/30\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.9187 - accuracy: 0.5424 - val_loss: 0.9406 - val_accuracy: 0.5158\n",
            "Epoch 13/30\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.8978 - accuracy: 0.5569 - val_loss: 0.9289 - val_accuracy: 0.5240\n",
            "Epoch 14/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.8779 - accuracy: 0.5704 - val_loss: 0.9161 - val_accuracy: 0.5315\n",
            "Epoch 15/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.8601 - accuracy: 0.5840 - val_loss: 0.9102 - val_accuracy: 0.5330\n",
            "Epoch 16/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.8455 - accuracy: 0.5907 - val_loss: 0.8990 - val_accuracy: 0.5443\n",
            "Epoch 17/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.8293 - accuracy: 0.5986 - val_loss: 0.8955 - val_accuracy: 0.5420\n",
            "Epoch 18/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.8161 - accuracy: 0.6092 - val_loss: 0.8929 - val_accuracy: 0.5480\n",
            "Epoch 19/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.8029 - accuracy: 0.6164 - val_loss: 0.8854 - val_accuracy: 0.5548\n",
            "Epoch 20/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.7921 - accuracy: 0.6214 - val_loss: 0.8803 - val_accuracy: 0.5646\n",
            "Epoch 21/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.7813 - accuracy: 0.6302 - val_loss: 0.8815 - val_accuracy: 0.5653\n",
            "Epoch 22/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.7727 - accuracy: 0.6312 - val_loss: 0.8767 - val_accuracy: 0.5676\n",
            "Epoch 23/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.7630 - accuracy: 0.6412 - val_loss: 0.8788 - val_accuracy: 0.5676\n",
            "Epoch 24/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.7534 - accuracy: 0.6456 - val_loss: 0.8792 - val_accuracy: 0.5586\n",
            "Epoch 25/30\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 0.7459 - accuracy: 0.6505 - val_loss: 0.8831 - val_accuracy: 0.5593\n",
            "Epoch 26/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.7398 - accuracy: 0.6502 - val_loss: 0.8809 - val_accuracy: 0.5631\n",
            "Epoch 27/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.7326 - accuracy: 0.6581 - val_loss: 0.8797 - val_accuracy: 0.5638\n",
            "Epoch 28/30\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.7258 - accuracy: 0.6614 - val_loss: 0.8810 - val_accuracy: 0.5661\n",
            "Epoch 29/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.7195 - accuracy: 0.6643 - val_loss: 0.8922 - val_accuracy: 0.5593\n",
            "Epoch 30/30\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 0.7173 - accuracy: 0.6677 - val_loss: 0.8891 - val_accuracy: 0.5646\n",
            "42/42 [==============================] - 0s 3ms/step - loss: 0.8840 - accuracy: 0.6003\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "# Tips: The activation function of the output layer is softmax.\n",
        "\n",
        "from keras.layers import Input, Embedding, Dense, LSTM, Reshape\n",
        "from keras.models import Model\n",
        "\n",
        "# input layer\n",
        "input_layer = Input(shape=(128,))\n",
        "\n",
        "# embedding layer\n",
        "embedding_layer = Embedding(len(word_index), 100, embeddings_initializer='glorot_uniform', input_length=128)(input_layer)\n",
        "\n",
        "# global average pooling layer\n",
        "global_average_pooling_layer = GlobalAveragePooling1D()(embedding_layer)\n",
        "\n",
        "# hidden layer\n",
        "hidden_layer = Dense(16, activation='relu')(global_average_pooling_layer)\n",
        "\n",
        "#output layer\n",
        "output_layer = Dense(3, activation='softmax')(hidden_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model.fit(x_train_pad, y_train, epochs=30, batch_size=512, validation_data=(x_dev_pad, y_dev))\n",
        "\n",
        "loss, accuracy = model.evaluate(x_test_pad, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FBpTc_rXGvQ"
      },
      "source": [
        "The accuracy of lab3 model2 in this task is around 46%. If you use the \"glorot_uniform\" initialization method, the accuracy can reach around 55%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iafDTygK28fv"
      },
      "source": [
        "##  Model 1-2: CNN or LSTM without pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2TnuiKb2-vE"
      },
      "source": [
        "Please try one more model (CNN or LSTM) without pre-trained word embeddings in here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXjbq6WcJosQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a3b0e1-a6d2-44d6-d15a-20de070f267d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 128, 100)          789800    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               80400     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                1616      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 871,867\n",
            "Trainable params: 871,867\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "22/22 [==============================] - 6s 98ms/step - loss: 1.0770 - accuracy: 0.4411 - val_loss: 1.0692 - val_accuracy: 0.4535\n",
            "Epoch 2/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0684 - accuracy: 0.4507 - val_loss: 1.0647 - val_accuracy: 0.4535\n",
            "Epoch 3/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0670 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 4/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0669 - accuracy: 0.4507 - val_loss: 1.0647 - val_accuracy: 0.4535\n",
            "Epoch 5/30\n",
            "22/22 [==============================] - 1s 65ms/step - loss: 1.0666 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 6/30\n",
            "22/22 [==============================] - 1s 65ms/step - loss: 1.0665 - accuracy: 0.4507 - val_loss: 1.0646 - val_accuracy: 0.4535\n",
            "Epoch 7/30\n",
            "22/22 [==============================] - 1s 65ms/step - loss: 1.0665 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 8/30\n",
            "22/22 [==============================] - 1s 65ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0648 - val_accuracy: 0.4535\n",
            "Epoch 9/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0667 - accuracy: 0.4507 - val_loss: 1.0647 - val_accuracy: 0.4535\n",
            "Epoch 10/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 11/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0667 - accuracy: 0.4507 - val_loss: 1.0649 - val_accuracy: 0.4535\n",
            "Epoch 12/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0663 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 13/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0666 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 14/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0671 - accuracy: 0.4507 - val_loss: 1.0655 - val_accuracy: 0.4535\n",
            "Epoch 15/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0666 - accuracy: 0.4507 - val_loss: 1.0647 - val_accuracy: 0.4535\n",
            "Epoch 16/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0669 - accuracy: 0.4507 - val_loss: 1.0649 - val_accuracy: 0.4535\n",
            "Epoch 17/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0665 - accuracy: 0.4507 - val_loss: 1.0647 - val_accuracy: 0.4535\n",
            "Epoch 18/30\n",
            "22/22 [==============================] - 1s 65ms/step - loss: 1.0665 - accuracy: 0.4507 - val_loss: 1.0650 - val_accuracy: 0.4535\n",
            "Epoch 19/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0665 - accuracy: 0.4507 - val_loss: 1.0646 - val_accuracy: 0.4535\n",
            "Epoch 20/30\n",
            "22/22 [==============================] - 1s 65ms/step - loss: 1.0665 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 21/30\n",
            "22/22 [==============================] - 1s 65ms/step - loss: 1.0663 - accuracy: 0.4507 - val_loss: 1.0646 - val_accuracy: 0.4535\n",
            "Epoch 22/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0646 - val_accuracy: 0.4535\n",
            "Epoch 23/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 24/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0666 - accuracy: 0.4507 - val_loss: 1.0646 - val_accuracy: 0.4535\n",
            "Epoch 25/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0666 - accuracy: 0.4507 - val_loss: 1.0648 - val_accuracy: 0.4535\n",
            "Epoch 26/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0668 - accuracy: 0.4507 - val_loss: 1.0656 - val_accuracy: 0.4535\n",
            "Epoch 27/30\n",
            "22/22 [==============================] - 1s 65ms/step - loss: 1.0669 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 28/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 29/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0665 - accuracy: 0.4507 - val_loss: 1.0647 - val_accuracy: 0.4535\n",
            "Epoch 30/30\n",
            "22/22 [==============================] - 1s 66ms/step - loss: 1.0665 - accuracy: 0.4507 - val_loss: 1.0649 - val_accuracy: 0.4535\n",
            "42/42 [==============================] - 0s 11ms/step - loss: 1.0651 - accuracy: 0.4543\n"
          ]
        }
      ],
      "source": [
        "# Try CNN or LSTM without pre-trained word embeddings in here:\n",
        "\n",
        "input_layer_1 = Input(shape=(128,))\n",
        "\n",
        "# embedding layer\n",
        "embedding_layer_1 = Embedding(len(word_index), 100, embeddings_initializer='glorot_uniform', input_length=128)(input_layer_1)\n",
        "\n",
        "# lstm layer\n",
        "lstm_layer_1 = LSTM(100)(embedding_layer_1)\n",
        "\n",
        "# hidden layer\n",
        "hidden_layer_1 = Dense(16, activation='relu')(lstm_layer_1)\n",
        "\n",
        "#output layer\n",
        "output_layer_1 = Dense(3, activation='softmax')(hidden_layer_1)\n",
        "\n",
        "model_1 = Model(inputs=input_layer_1, outputs=output_layer_1)\n",
        "model_1.summary()\n",
        "\n",
        "model_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_1.fit(x_train_pad, y_train, epochs=30, batch_size=512, validation_data=(x_dev_pad, y_dev))\n",
        "\n",
        "loss, accuracy = model_1.evaluate(x_test_pad, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try CNN or LSTM without pre-trained word embeddings in here:\n",
        "\n",
        "input_layer_2 = Input(shape=(128,))\n",
        "\n",
        "# embedding layer\n",
        "embedding_layer_2 = Embedding(len(word_index), 100, embeddings_initializer='glorot_uniform', input_length=128)(input_layer_2)\n",
        "\n",
        "# cnn layer\n",
        "cnn_layer = Conv1D(32, 3, activation='relu')(embedding_layer_2)\n",
        "\n",
        "global_average_pooling_layer_2 = GlobalAveragePooling1D()(cnn_layer)\n",
        "\n",
        "# hidden layer\n",
        "hidden_layer_2 = Dense(32, activation='relu')(global_average_pooling_layer_2)\n",
        "\n",
        "#output layer\n",
        "output_layer_2 = Dense(3, activation='softmax')(hidden_layer_2)\n",
        "\n",
        "model_2 = Model(inputs=input_layer_2, outputs=output_layer_2)\n",
        "model_2.summary()\n",
        "\n",
        "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_2.fit(x_train_pad, y_train, epochs=30, batch_size=512, validation_data=(x_dev_pad, y_dev))\n",
        "\n",
        "loss, accuracy = model_2.evaluate(x_test_pad, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag0reXIJa3cC",
        "outputId": "4f1d5795-b51c-4851-ae34-f57f17095c79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 128, 100)          789800    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 126, 32)           9632      \n",
            "                                                                 \n",
            " global_average_pooling1d_1   (None, 32)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 800,587\n",
            "Trainable params: 800,587\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "22/22 [==============================] - 7s 32ms/step - loss: 1.0784 - accuracy: 0.4490 - val_loss: 1.0651 - val_accuracy: 0.4535\n",
            "Epoch 2/30\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 1.0658 - accuracy: 0.4507 - val_loss: 1.0625 - val_accuracy: 0.4535\n",
            "Epoch 3/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.0610 - accuracy: 0.4507 - val_loss: 1.0559 - val_accuracy: 0.4535\n",
            "Epoch 4/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.0495 - accuracy: 0.4507 - val_loss: 1.0395 - val_accuracy: 0.4535\n",
            "Epoch 5/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.0204 - accuracy: 0.4526 - val_loss: 1.0032 - val_accuracy: 0.4617\n",
            "Epoch 6/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.9640 - accuracy: 0.5045 - val_loss: 0.9516 - val_accuracy: 0.5195\n",
            "Epoch 7/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.9025 - accuracy: 0.5538 - val_loss: 0.9141 - val_accuracy: 0.5458\n",
            "Epoch 8/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.8540 - accuracy: 0.5863 - val_loss: 0.8915 - val_accuracy: 0.5653\n",
            "Epoch 9/30\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.8144 - accuracy: 0.6133 - val_loss: 0.8820 - val_accuracy: 0.5653\n",
            "Epoch 10/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.7836 - accuracy: 0.6303 - val_loss: 0.8706 - val_accuracy: 0.5841\n",
            "Epoch 11/30\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.7556 - accuracy: 0.6495 - val_loss: 0.8638 - val_accuracy: 0.5811\n",
            "Epoch 12/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.7315 - accuracy: 0.6618 - val_loss: 0.8649 - val_accuracy: 0.5818\n",
            "Epoch 13/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.7119 - accuracy: 0.6755 - val_loss: 0.8695 - val_accuracy: 0.5743\n",
            "Epoch 14/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6898 - accuracy: 0.6882 - val_loss: 0.8723 - val_accuracy: 0.5788\n",
            "Epoch 15/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6732 - accuracy: 0.6962 - val_loss: 0.8854 - val_accuracy: 0.5878\n",
            "Epoch 16/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6566 - accuracy: 0.7055 - val_loss: 0.9019 - val_accuracy: 0.5788\n",
            "Epoch 17/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6428 - accuracy: 0.7181 - val_loss: 0.9085 - val_accuracy: 0.5938\n",
            "Epoch 18/30\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6270 - accuracy: 0.7260 - val_loss: 0.9226 - val_accuracy: 0.6014\n",
            "Epoch 19/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.6154 - accuracy: 0.7301 - val_loss: 0.9187 - val_accuracy: 0.5976\n",
            "Epoch 20/30\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.6008 - accuracy: 0.7435 - val_loss: 0.9288 - val_accuracy: 0.5961\n",
            "Epoch 21/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5896 - accuracy: 0.7483 - val_loss: 0.9501 - val_accuracy: 0.5991\n",
            "Epoch 22/30\n",
            "22/22 [==============================] - 0s 18ms/step - loss: 0.5783 - accuracy: 0.7568 - val_loss: 0.9503 - val_accuracy: 0.6029\n",
            "Epoch 23/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5690 - accuracy: 0.7599 - val_loss: 0.9709 - val_accuracy: 0.5976\n",
            "Epoch 24/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5571 - accuracy: 0.7676 - val_loss: 0.9866 - val_accuracy: 0.5991\n",
            "Epoch 25/30\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5473 - accuracy: 0.7710 - val_loss: 0.9971 - val_accuracy: 0.5976\n",
            "Epoch 26/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5377 - accuracy: 0.7776 - val_loss: 1.0077 - val_accuracy: 0.5916\n",
            "Epoch 27/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5313 - accuracy: 0.7827 - val_loss: 1.0324 - val_accuracy: 0.5856\n",
            "Epoch 28/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5266 - accuracy: 0.7796 - val_loss: 1.0319 - val_accuracy: 0.6029\n",
            "Epoch 29/30\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.5191 - accuracy: 0.7841 - val_loss: 1.0460 - val_accuracy: 0.5976\n",
            "Epoch 30/30\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.5075 - accuracy: 0.7906 - val_loss: 1.0597 - val_accuracy: 0.5908\n",
            "42/42 [==============================] - 0s 4ms/step - loss: 1.0641 - accuracy: 0.5853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--020hfG6rN2"
      },
      "source": [
        "# Model 2: Using pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GdY2-64YG1B"
      },
      "source": [
        "### Preparing pre-trained word embeddings (GLOVE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4gBeOyi4gkM"
      },
      "source": [
        "The Embedding layer can be used to load a pre-trained word embedding model. We are going to use GloVe embeddings, which you can read about it here (https://nlp.stanford.edu/projects/glove/). GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics. You can download GloVe and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your dataset.\n",
        "First, we need to read GloVe and map words to GloVe:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_PypdqG9Iis"
      },
      "outputs": [],
      "source": [
        "def readGloveFile(gloveFile):\n",
        "    with open(gloveFile, 'r') as f:\n",
        "        wordToGlove = {}  \n",
        "        wordToIndex = {}  \n",
        "        indexToWord = {}  \n",
        "\n",
        "        for line in f:\n",
        "            record = line.strip().split()\n",
        "            token = record[0] \n",
        "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) \n",
        "            \n",
        "        tokens = sorted(wordToGlove.keys())\n",
        "        for idx, tok in enumerate(tokens):\n",
        "            kerasIdx = idx + 1  \n",
        "            wordToIndex[tok] = kerasIdx \n",
        "            indexToWord[kerasIdx] = tok \n",
        "\n",
        "    return wordToIndex, indexToWord, wordToGlove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcIZ3dq59bCh"
      },
      "source": [
        "Now, we create our pre-trained Embedding layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gembn7VM3ex8"
      },
      "outputs": [],
      "source": [
        "from keras.initializers import Constant\n",
        "\n",
        "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
        "    vocabLen = len(wordToIndex) + 1  \n",
        "    embDim = next(iter(wordToGlove.values())).shape[0]  \n",
        "   \n",
        "    embeddingMatrix = np.zeros((vocabLen, embDim))  \n",
        "    for word, index in wordToIndex.items():\n",
        "        embeddingMatrix[index, :] = wordToGlove[word] \n",
        "\n",
        "    embeddingLayer = Embedding(vocabLen, embDim, embeddings_initializer=Constant(embeddingMatrix), trainable=isTrainable, name='GloVe_Embeddings')\n",
        "    return embeddingLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OC1wuctdFvA",
        "outputId": "83865d09-7302-43b2-a339-d93b2e6be4ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-24 12:57:39--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-02-24 12:57:39--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-02-24 12:57:40--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: â€˜glove.6B.zipâ€™\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.14MB/s    in 2m 41s  \n",
            "\n",
            "2022-02-24 13:00:21 (5.10 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
            "\n",
            "Archive:  /content/glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip '/content/glove.6B.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGxciLK4-xOr"
      },
      "source": [
        "We freeze the weights. To create the model: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZCPUM0W_Drc",
        "outputId": "e204b802-b4c7-4b7b-ef39-d508321ff526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Embedding:  300\n"
          ]
        }
      ],
      "source": [
        "# wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.50d.txt')\n",
        "# wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.100d.txt')\n",
        "wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.300d.txt')\n",
        "\n",
        "# vocabLen = len(wordToIndex) + 1 \n",
        "\n",
        "EMBED_SIZE = next(iter(wordToGlove.values())).shape[0]\n",
        "print('Size of Embedding: ',EMBED_SIZE)\n",
        "\n",
        "embeddingLayer=createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RyeDPimMW7c"
      },
      "source": [
        "### Convert the data to GLOVE word index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_X6nC57NGXv"
      },
      "source": [
        "The index in our vocabulary is different from that in GLOVE. For example, the word \"you\" corresponds to 394475 in GLOVE, while it corresponds to another index in our vocabulary. Thus we can not directly use the index data in the last section. We convert them from text tokens to GLOVE word index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODUoIts1NM6b",
        "outputId": "d64370cc-91f2-4731-bf30-ae2d117af56a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "394475\n",
            "you\n",
            "2358\n"
          ]
        }
      ],
      "source": [
        "print(wordToIndex[\"you\"])\n",
        "print(indexToWord[394475])\n",
        "print(word_index[\"you\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p037PLDyNXhV",
        "outputId": "5c831ecc-8bb7-4950-ca29-69667926152b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train_review_glove[0]:\n",
            "[357266, 118926, 192973, 264550, 338995, 62065, 51582, 87775, 357354, 151204, 54718, 53201, 292136, 231458, 373317, 151349, 193716]\n",
            "x_train_aspect_glove[0]:\n",
            "[118926]\n"
          ]
        }
      ],
      "source": [
        "# Please write your code to generate the following data\n",
        "x_train_review_glove = []\n",
        "x_dev_review_glove = []\n",
        "x_test_review_glove = []\n",
        "\n",
        "\n",
        "\n",
        "x_train_aspect_glove = []\n",
        "x_dev_aspect_glove = []\n",
        "x_test_aspect_glove = []\n",
        "\n",
        "for example in train:\n",
        "  text_tokens = text_to_word_sequence(example[0])\n",
        "  aspect_tokens = text_to_word_sequence(example[1])\n",
        "  review_int_glove = []\n",
        "  for word in text_tokens:\n",
        "    if word in wordToIndex:\n",
        "        review_int_glove.append(wordToIndex[word])\n",
        "    else:\n",
        "        review_int_glove.append(0)\n",
        "  x_train_review_glove.append(review_int_glove)\n",
        "  aspect_int_glove = []\n",
        "  for word in aspect_tokens:\n",
        "    if word in wordToIndex:\n",
        "        aspect_int_glove.append(wordToIndex[word])\n",
        "    else:\n",
        "        aspect_int_glove.append(0)\n",
        "  x_train_aspect_glove.append(aspect_int_glove)\n",
        "\n",
        "for example in val:\n",
        "  text_tokens = text_to_word_sequence(example[0])\n",
        "  aspect_tokens = text_to_word_sequence(example[1])\n",
        "  review_int_glove = []\n",
        "  for word in text_tokens:\n",
        "    if word in wordToIndex:\n",
        "        review_int_glove.append(wordToIndex[word])\n",
        "    else:\n",
        "        review_int_glove.append(0)\n",
        "  x_dev_review_glove.append(review_int_glove)\n",
        "  aspect_int_glove = []\n",
        "  for word in aspect_tokens:\n",
        "    if word in wordToIndex:\n",
        "        aspect_int_glove.append(wordToIndex[word])\n",
        "    else:\n",
        "        aspect_int_glove.append(0)\n",
        "  x_dev_aspect_glove.append(aspect_int_glove)\n",
        "\n",
        "for example in test:\n",
        "  text_tokens = text_to_word_sequence(example[0])\n",
        "  aspect_tokens = text_to_word_sequence(example[1])\n",
        "  review_int_glove = []\n",
        "  for word in text_tokens:\n",
        "    if word in wordToIndex:\n",
        "        review_int_glove.append(wordToIndex[word])\n",
        "    else:\n",
        "        review_int_glove.append(0)\n",
        "  x_test_review_glove.append(review_int_glove)\n",
        "  aspect_int_glove = []\n",
        "  for word in aspect_tokens:\n",
        "    if word in wordToIndex:\n",
        "        aspect_int_glove.append(wordToIndex[word])\n",
        "    else:\n",
        "        aspect_int_glove.append(0)\n",
        "  x_test_aspect_glove.append(aspect_int_glove)\n",
        "\n",
        "# You should get a print result like:\n",
        "assert len(x_train_review_glove) == len(train)\n",
        "assert len(x_train_aspect_glove) == len(x_train_aspect_int)\n",
        "assert len(x_test_review_glove) == len(test)\n",
        "assert len(x_test_aspect_glove) == len(x_test_aspect_int)\n",
        "print(\"x_train_review_glove[0]:\")\n",
        "print(x_train_review_glove[0])\n",
        "print(\"x_train_aspect_glove[0]:\")\n",
        "print(x_train_aspect_glove[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z--QOlb1vXtN"
      },
      "source": [
        "As before, we concatenate the tweets and topics for fitting in the previous model. Let us do it again for GLOVE version variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-D_szbDQ5X5",
        "outputId": "aae861bf-d2ef-43e0-aab5-14b588039a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before paded:\n",
            "[118926      1 357266 118926 192973 264550 338995  62065  51582  87775\n",
            " 357354 151204  54718  53201 292136 231458 373317 151349 193716]\n",
            "After paded:\n",
            "[118926      1 357266 118926 192973 264550 338995  62065  51582  87775\n",
            " 357354 151204  54718  53201 292136 231458 373317 151349 193716      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0]\n"
          ]
        }
      ],
      "source": [
        "# Please write your code to combine the x_*_review_glove and x_*_aspect_glove into the following varibles\n",
        "# x_train_glove\n",
        "# x_dev_glove\n",
        "# x_test_glove\n",
        "\n",
        "# Tips: \n",
        "# 1) There is no <START> token in GLOVE. Here we can use integer 1 to concatenate.\n",
        "# 2) After combine them, do not foget to pad the sequences.\n",
        "\n",
        "def combine_x(x_aspect_glove, x_review_glove):\n",
        "    x_combine = []\n",
        "    for i in range(len(x_review_glove)):\n",
        "        x_combine.append(np.concatenate((x_aspect_glove[i], [1], x_review_glove[i]), axis=0))\n",
        "    return x_combine\n",
        "\n",
        "#######################TODO: REMOVE FOLLOWING CODE####################\n",
        "\n",
        "x_train_glove = combine_x(x_train_aspect_glove,x_train_review_glove)\n",
        "x_dev_glove   = combine_x(x_dev_aspect_glove  ,x_dev_review_glove)\n",
        "x_test_glove  = combine_x(x_test_aspect_glove ,x_test_review_glove)\n",
        "\n",
        "\n",
        "x_train_pad_glove = keras.preprocessing.sequence.pad_sequences(x_train_glove,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=128)\n",
        "\n",
        "x_dev_pad_glove = keras.preprocessing.sequence.pad_sequences(x_dev_glove,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=128)\n",
        "\n",
        "x_test_pad_glove = keras.preprocessing.sequence.pad_sequences(x_test_glove,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=128)\n",
        "x_train_pad_glove = np.array(x_train_pad_glove)\n",
        "x_dev_pad_glove = np.array(x_dev_pad_glove)\n",
        "x_test_pad_glove = np.array(x_test_pad_glove)\n",
        "\n",
        "# Don't forget the to use np.array function to wrap the ouput of pad_sequences function, such as: x_train_pad = np.array(x_train_pad)\n",
        "# Only pad the *_int varibles\n",
        "print(\"Before paded:\")\n",
        "print(x_train_glove[0])\n",
        "print(\"After paded:\")\n",
        "print(x_train_pad_glove[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdZ4nl08vp9A"
      },
      "source": [
        "## Model 2-1: Neural bag of words using pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gyCwXFj_R5w"
      },
      "source": [
        "We use model3-1 in lab4 to deal with this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VICS9rY8C7KH",
        "outputId": "ebdfd604-5db9-49b4-e749-56dad3b78931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " GloVe_Embeddings (Embedding  (None, 128, 300)         120000300 \n",
            " )                                                               \n",
            "                                                                 \n",
            " global_average_pooling1d_2   (None, 300)              0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                4816      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120,005,167\n",
            "Trainable params: 4,867\n",
            "Non-trainable params: 120,000,300\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "22/22 [==============================] - 1s 16ms/step - loss: 1.0802 - accuracy: 0.4226 - val_loss: 1.0607 - val_accuracy: 0.4535\n",
            "Epoch 2/30\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0582 - accuracy: 0.4507 - val_loss: 1.0503 - val_accuracy: 0.4535\n",
            "Epoch 3/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.0477 - accuracy: 0.4507 - val_loss: 1.0386 - val_accuracy: 0.4535\n",
            "Epoch 4/30\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0374 - accuracy: 0.4507 - val_loss: 1.0276 - val_accuracy: 0.4535\n",
            "Epoch 5/30\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0275 - accuracy: 0.4509 - val_loss: 1.0181 - val_accuracy: 0.4542\n",
            "Epoch 6/30\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0191 - accuracy: 0.4540 - val_loss: 1.0101 - val_accuracy: 0.4595\n",
            "Epoch 7/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.0120 - accuracy: 0.4573 - val_loss: 1.0034 - val_accuracy: 0.4625\n",
            "Epoch 8/30\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 1.0058 - accuracy: 0.4624 - val_loss: 0.9975 - val_accuracy: 0.4715\n",
            "Epoch 9/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.0003 - accuracy: 0.4694 - val_loss: 0.9922 - val_accuracy: 0.4805\n",
            "Epoch 10/30\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9952 - accuracy: 0.4798 - val_loss: 0.9880 - val_accuracy: 0.4782\n",
            "Epoch 11/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9905 - accuracy: 0.4798 - val_loss: 0.9830 - val_accuracy: 0.4857\n",
            "Epoch 12/30\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9859 - accuracy: 0.4848 - val_loss: 0.9789 - val_accuracy: 0.4872\n",
            "Epoch 13/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9816 - accuracy: 0.4879 - val_loss: 0.9753 - val_accuracy: 0.4895\n",
            "Epoch 14/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9775 - accuracy: 0.4895 - val_loss: 0.9713 - val_accuracy: 0.5008\n",
            "Epoch 15/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9735 - accuracy: 0.4948 - val_loss: 0.9691 - val_accuracy: 0.4970\n",
            "Epoch 16/30\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9702 - accuracy: 0.4965 - val_loss: 0.9646 - val_accuracy: 0.5030\n",
            "Epoch 17/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9663 - accuracy: 0.5007 - val_loss: 0.9617 - val_accuracy: 0.5090\n",
            "Epoch 18/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9630 - accuracy: 0.5048 - val_loss: 0.9594 - val_accuracy: 0.5143\n",
            "Epoch 19/30\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9600 - accuracy: 0.5096 - val_loss: 0.9577 - val_accuracy: 0.5128\n",
            "Epoch 20/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9571 - accuracy: 0.5085 - val_loss: 0.9535 - val_accuracy: 0.5173\n",
            "Epoch 21/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9537 - accuracy: 0.5154 - val_loss: 0.9510 - val_accuracy: 0.5150\n",
            "Epoch 22/30\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9511 - accuracy: 0.5156 - val_loss: 0.9485 - val_accuracy: 0.5210\n",
            "Epoch 23/30\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9485 - accuracy: 0.5189 - val_loss: 0.9467 - val_accuracy: 0.5248\n",
            "Epoch 24/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9461 - accuracy: 0.5218 - val_loss: 0.9449 - val_accuracy: 0.5225\n",
            "Epoch 25/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9434 - accuracy: 0.5218 - val_loss: 0.9434 - val_accuracy: 0.5248\n",
            "Epoch 26/30\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9415 - accuracy: 0.5210 - val_loss: 0.9417 - val_accuracy: 0.5293\n",
            "Epoch 27/30\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9393 - accuracy: 0.5232 - val_loss: 0.9397 - val_accuracy: 0.5360\n",
            "Epoch 28/30\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9373 - accuracy: 0.5251 - val_loss: 0.9386 - val_accuracy: 0.5225\n",
            "Epoch 29/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9358 - accuracy: 0.5280 - val_loss: 0.9363 - val_accuracy: 0.5323\n",
            "Epoch 30/30\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9333 - accuracy: 0.5291 - val_loss: 0.9380 - val_accuracy: 0.5353\n",
            "42/42 [==============================] - 0s 3ms/step - loss: 0.9239 - accuracy: 0.5434\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "# Tips: Do not misuse the training data\n",
        "\n",
        "from keras.layers import Input, Embedding, Dense, LSTM, Reshape\n",
        "from keras.models import Model\n",
        "\n",
        "# input layer\n",
        "input_layer_3 = Input(shape=(128,))\n",
        "\n",
        "# embedding layer\n",
        "embedding_layer_3 = embeddingLayer(input_layer_3)\n",
        "\n",
        "# global average pooling layer\n",
        "global_average_pooling_layer_3 = GlobalAveragePooling1D()(embedding_layer_3)\n",
        "\n",
        "# hidden layer\n",
        "hidden_layer_3 = Dense(16, activation='relu')(global_average_pooling_layer_3)\n",
        "\n",
        "#output layer\n",
        "output_layer_3 = Dense(3, activation='softmax')(hidden_layer_3)\n",
        "\n",
        "model_3 = Model(inputs=input_layer_3, outputs=output_layer_3)\n",
        "model_3.summary()\n",
        "\n",
        "model_3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model_3.fit(x_train_pad_glove, y_train, epochs=30, batch_size=512, validation_data=(x_dev_pad_glove, y_dev))\n",
        "\n",
        "loss, accuracy = model_3.evaluate(x_test_pad_glove, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aTWUWQS2VFd"
      },
      "source": [
        "The accuracy is around 56%. In this version, the \"glorot_uniform\" initialization method does not improve model performance significantly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ1KWFKvcagS"
      },
      "source": [
        "##  Model 2-2: CNN or LSTM with pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYubGfP-2VEL"
      },
      "source": [
        "Please try one more model (CNN or LSTM) with pre-trained word embeddings in here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeLTrJ-3zKBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9f380e-4773-4ead-a24e-d336523beebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " GloVe_Embeddings (Embedding  (None, 128, 300)         120000300 \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 16)                1616      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 3)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120,162,367\n",
            "Trainable params: 162,067\n",
            "Non-trainable params: 120,000,300\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "22/22 [==============================] - 4s 109ms/step - loss: 1.0790 - accuracy: 0.4445 - val_loss: 1.0664 - val_accuracy: 0.4535\n",
            "Epoch 2/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0667 - accuracy: 0.4507 - val_loss: 1.0667 - val_accuracy: 0.4535\n",
            "Epoch 3/30\n",
            "22/22 [==============================] - 2s 74ms/step - loss: 1.0679 - accuracy: 0.4507 - val_loss: 1.0659 - val_accuracy: 0.4535\n",
            "Epoch 4/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0648 - val_accuracy: 0.4535\n",
            "Epoch 5/30\n",
            "22/22 [==============================] - 2s 74ms/step - loss: 1.0666 - accuracy: 0.4507 - val_loss: 1.0647 - val_accuracy: 0.4535\n",
            "Epoch 6/30\n",
            "22/22 [==============================] - 2s 79ms/step - loss: 1.0668 - accuracy: 0.4507 - val_loss: 1.0646 - val_accuracy: 0.4535\n",
            "Epoch 7/30\n",
            "22/22 [==============================] - 2s 78ms/step - loss: 1.0666 - accuracy: 0.4507 - val_loss: 1.0648 - val_accuracy: 0.4535\n",
            "Epoch 8/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0671 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 9/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0646 - val_accuracy: 0.4535\n",
            "Epoch 10/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0663 - accuracy: 0.4507 - val_loss: 1.0647 - val_accuracy: 0.4535\n",
            "Epoch 11/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 12/30\n",
            "22/22 [==============================] - 2s 74ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0647 - val_accuracy: 0.4535\n",
            "Epoch 13/30\n",
            "22/22 [==============================] - 2s 78ms/step - loss: 1.0668 - accuracy: 0.4507 - val_loss: 1.0650 - val_accuracy: 0.4535\n",
            "Epoch 14/30\n",
            "22/22 [==============================] - 2s 74ms/step - loss: 1.0665 - accuracy: 0.4507 - val_loss: 1.0646 - val_accuracy: 0.4535\n",
            "Epoch 15/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0667 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 16/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0665 - accuracy: 0.4507 - val_loss: 1.0648 - val_accuracy: 0.4535\n",
            "Epoch 17/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0665 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 18/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0668 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 19/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0665 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 20/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0646 - val_accuracy: 0.4535\n",
            "Epoch 21/30\n",
            "22/22 [==============================] - 2s 79ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0647 - val_accuracy: 0.4535\n",
            "Epoch 22/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0646 - val_accuracy: 0.4535\n",
            "Epoch 23/30\n",
            "22/22 [==============================] - 2s 76ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0648 - val_accuracy: 0.4535\n",
            "Epoch 24/30\n",
            "22/22 [==============================] - 2s 78ms/step - loss: 1.0663 - accuracy: 0.4507 - val_loss: 1.0646 - val_accuracy: 0.4535\n",
            "Epoch 25/30\n",
            "22/22 [==============================] - 2s 79ms/step - loss: 1.0666 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 26/30\n",
            "22/22 [==============================] - 2s 76ms/step - loss: 1.0663 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 27/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 28/30\n",
            "22/22 [==============================] - 2s 79ms/step - loss: 1.0665 - accuracy: 0.4507 - val_loss: 1.0646 - val_accuracy: 0.4535\n",
            "Epoch 29/30\n",
            "22/22 [==============================] - 2s 76ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "Epoch 30/30\n",
            "22/22 [==============================] - 2s 75ms/step - loss: 1.0664 - accuracy: 0.4507 - val_loss: 1.0645 - val_accuracy: 0.4535\n",
            "42/42 [==============================] - 1s 13ms/step - loss: 1.0646 - accuracy: 0.4543\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "\n",
        "\n",
        "input_layer_4 = Input(shape=(128,))\n",
        "\n",
        "# embedding layer\n",
        "embedding_layer_4 = embeddingLayer(input_layer_4)\n",
        "\n",
        "# lstm layer\n",
        "lstm_layer_4 = LSTM(100)(embedding_layer_4)\n",
        "\n",
        "# hidden layer\n",
        "hidden_layer_4 = Dense(16, activation='relu')(lstm_layer_4)\n",
        "\n",
        "#output layer\n",
        "output_layer_4 = Dense(3, activation='softmax')(hidden_layer_4)\n",
        "\n",
        "model_4 = Model(inputs=input_layer_4, outputs=output_layer_4)\n",
        "model_4.summary()\n",
        "\n",
        "model_4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model_4.fit(x_train_pad_glove, y_train, epochs=30, batch_size=512, validation_data=(x_dev_pad_glove, y_dev))\n",
        "\n",
        "loss, accuracy = model_4.evaluate(x_test_pad_glove, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer_5 = Input(shape=(128,))\n",
        "\n",
        "# embedding layer\n",
        "embedding_layer_5 = embeddingLayer(input_layer_5)\n",
        "\n",
        "# cnn layer\n",
        "cnn_layer_5 = Conv1D(32, 3, activation='relu')(embedding_layer_5)\n",
        "\n",
        "global_average_pooling_layer_5 = GlobalAveragePooling1D()(cnn_layer_5)\n",
        "\n",
        "# hidden layer\n",
        "hidden_layer_5 = Dense(32, activation='relu')(global_average_pooling_layer_5)\n",
        "\n",
        "#output layer\n",
        "output_layer_5 = Dense(3, activation='softmax')(hidden_layer_5)\n",
        "\n",
        "model_5 = Model(inputs=input_layer_5, outputs=output_layer_5)\n",
        "model_5.summary()\n",
        "\n",
        "model_5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model_5.fit(x_train_pad_glove, y_train, epochs=30, batch_size=512, validation_data=(x_dev_pad_glove, y_dev))\n",
        "\n",
        "loss, accuracy = model_5.evaluate(x_test_pad_glove, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7ZzdQJ2ZNM-",
        "outputId": "4259a5f2-95c8-4991-f9a7-16eeff639e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " GloVe_Embeddings (Embedding  (None, 128, 300)         120000300 \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 126, 32)           28832     \n",
            "                                                                 \n",
            " global_average_pooling1d_3   (None, 32)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 120,030,287\n",
            "Trainable params: 29,987\n",
            "Non-trainable params: 120,000,300\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "22/22 [==============================] - 2s 46ms/step - loss: 1.0621 - accuracy: 0.4498 - val_loss: 1.0354 - val_accuracy: 0.4535\n",
            "Epoch 2/30\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 1.0254 - accuracy: 0.4497 - val_loss: 1.0078 - val_accuracy: 0.4505\n",
            "Epoch 3/30\n",
            "22/22 [==============================] - 0s 23ms/step - loss: 1.0022 - accuracy: 0.4625 - val_loss: 0.9904 - val_accuracy: 0.4797\n",
            "Epoch 4/30\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.9862 - accuracy: 0.4819 - val_loss: 0.9746 - val_accuracy: 0.4910\n",
            "Epoch 5/30\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.9709 - accuracy: 0.4972 - val_loss: 0.9616 - val_accuracy: 0.5045\n",
            "Epoch 6/30\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9602 - accuracy: 0.5020 - val_loss: 0.9524 - val_accuracy: 0.5135\n",
            "Epoch 7/30\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9501 - accuracy: 0.5087 - val_loss: 0.9523 - val_accuracy: 0.5098\n",
            "Epoch 8/30\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.9425 - accuracy: 0.5130 - val_loss: 0.9422 - val_accuracy: 0.5210\n",
            "Epoch 9/30\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9344 - accuracy: 0.5186 - val_loss: 0.9373 - val_accuracy: 0.5105\n",
            "Epoch 10/30\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9308 - accuracy: 0.5197 - val_loss: 0.9326 - val_accuracy: 0.5248\n",
            "Epoch 11/30\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.9217 - accuracy: 0.5269 - val_loss: 0.9306 - val_accuracy: 0.5308\n",
            "Epoch 12/30\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9168 - accuracy: 0.5337 - val_loss: 0.9254 - val_accuracy: 0.5345\n",
            "Epoch 13/30\n",
            "22/22 [==============================] - 1s 24ms/step - loss: 0.9118 - accuracy: 0.5397 - val_loss: 0.9268 - val_accuracy: 0.5390\n",
            "Epoch 14/30\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9084 - accuracy: 0.5410 - val_loss: 0.9213 - val_accuracy: 0.5413\n",
            "Epoch 15/30\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9031 - accuracy: 0.5426 - val_loss: 0.9133 - val_accuracy: 0.5465\n",
            "Epoch 16/30\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.8962 - accuracy: 0.5510 - val_loss: 0.9116 - val_accuracy: 0.5465\n",
            "Epoch 17/30\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.8936 - accuracy: 0.5540 - val_loss: 0.9179 - val_accuracy: 0.5458\n",
            "Epoch 18/30\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.8889 - accuracy: 0.5601 - val_loss: 0.9111 - val_accuracy: 0.5450\n",
            "Epoch 19/30\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.8836 - accuracy: 0.5593 - val_loss: 0.9063 - val_accuracy: 0.5563\n",
            "Epoch 20/30\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.8756 - accuracy: 0.5733 - val_loss: 0.8946 - val_accuracy: 0.5698\n",
            "Epoch 21/30\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.8691 - accuracy: 0.5758 - val_loss: 0.8929 - val_accuracy: 0.5766\n",
            "Epoch 22/30\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.8626 - accuracy: 0.5827 - val_loss: 0.8872 - val_accuracy: 0.5773\n",
            "Epoch 23/30\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.8569 - accuracy: 0.5868 - val_loss: 0.8809 - val_accuracy: 0.5901\n",
            "Epoch 24/30\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.8509 - accuracy: 0.5916 - val_loss: 0.8794 - val_accuracy: 0.5901\n",
            "Epoch 25/30\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.8446 - accuracy: 0.6001 - val_loss: 0.8803 - val_accuracy: 0.5848\n",
            "Epoch 26/30\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.8401 - accuracy: 0.6013 - val_loss: 0.8699 - val_accuracy: 0.5998\n",
            "Epoch 27/30\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.8323 - accuracy: 0.6088 - val_loss: 0.8649 - val_accuracy: 0.5976\n",
            "Epoch 28/30\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.8243 - accuracy: 0.6172 - val_loss: 0.8630 - val_accuracy: 0.6059\n",
            "Epoch 29/30\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.8196 - accuracy: 0.6178 - val_loss: 0.8552 - val_accuracy: 0.5983\n",
            "Epoch 30/30\n",
            "22/22 [==============================] - 1s 23ms/step - loss: 0.8118 - accuracy: 0.6259 - val_loss: 0.8520 - val_accuracy: 0.6029\n",
            "42/42 [==============================] - 0s 5ms/step - loss: 0.8278 - accuracy: 0.6168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-bZ5SCHiIMl"
      },
      "source": [
        "#  Model 3: Model with multiple-input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G85QM3lSV7qp"
      },
      "source": [
        "Model 1 and 2 are copied from lab4. We build new models in this section. \n",
        "\n",
        "In models 1 and 2, we combine the reviews and aspects to input into the models. In model 3, we separately input these two data into the model and use different layers to analyze them. \n",
        "\n",
        "(This will give us a model similar to a simplified version of the Xue & Li model from the lectures - we have a separate paths through the network for the aspect embedding and the sentence, being combined - but we don't have to use gating like Xue & Li)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztiFcOWuA0xH"
      },
      "outputs": [],
      "source": [
        "# First of all, pad the review and aspect separately\n",
        "x_train_review_pad_glove = keras.preprocessing.sequence.pad_sequences(x_train_review_glove,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=128)\n",
        "\n",
        "x_dev_review_pad_glove = keras.preprocessing.sequence.pad_sequences(x_dev_review_glove,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=128)\n",
        "\n",
        "x_test_review_pad_glove = keras.preprocessing.sequence.pad_sequences(x_test_review_glove,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=128)\n",
        "\n",
        "x_train_aspect_pad_glove = keras.preprocessing.sequence.pad_sequences(x_train_aspect_glove,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=16)\n",
        "\n",
        "x_dev_aspect_pad_glove = keras.preprocessing.sequence.pad_sequences(x_dev_aspect_glove,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=16)\n",
        "\n",
        "x_test_aspect_pad_glove = keras.preprocessing.sequence.pad_sequences(x_test_aspect_glove,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExgX8bxpVgps"
      },
      "source": [
        "## Model 3-1 Neural bag of words model with multiple-input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-TN6yup01mC"
      },
      "source": [
        "Model 3-1 needs you to modify the model 2-1 to be compatible with multiple-input.\n",
        "You could find some tutorial examples from (https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/).\n",
        "\n",
        "Please print the model summary and visualize it using vis_utils."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTgD_gMzXa1z",
        "outputId": "ff859e4e-0897-4f3e-dfdc-2203bd458955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_layer_6 (InputLayer)     [(None, 16)]         0           []                               \n",
            "                                                                                                  \n",
            " input_layer_7 (InputLayer)     [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " GloVe_Embeddings (Embedding)   multiple             120000300   ['input_layer_6[0][0]',          \n",
            "                                                                  'input_layer_7[0][0]']          \n",
            "                                                                                                  \n",
            " global_average_pooling1d_4 (Gl  (None, 300)         0           ['GloVe_Embeddings[3][0]']       \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_5 (Gl  (None, 300)         0           ['GloVe_Embeddings[4][0]']       \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 16)           4816        ['global_average_pooling1d_4[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 16)           4816        ['global_average_pooling1d_5[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 32)           0           ['dense_12[0][0]',               \n",
            "                                                                  'dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 3)            99          ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 120,010,031\n",
            "Trainable params: 9,731\n",
            "Non-trainable params: 120,000,300\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "\n",
        "from keras.layers import concatenate\n",
        "\n",
        "input_layer_6 = Input(shape=(16,), dtype='float32', name='input_layer_6')\n",
        "\n",
        "# another input layer\n",
        "input_layer_7 = Input(shape=(128,), dtype='float32', name='input_layer_7')\n",
        "\n",
        "# embedding layer with 2 outputs\n",
        "embedding_layer_6 = embeddingLayer(input_layer_6)\n",
        "embedding_layer_7 = embeddingLayer(input_layer_7)\n",
        "\n",
        "# pooling layer\n",
        "pooling_layer_6 = GlobalAveragePooling1D()(embedding_layer_6)\n",
        "\n",
        "# pooling layer\n",
        "pooling_layer_7 = GlobalAveragePooling1D()(embedding_layer_7)\n",
        "\n",
        "# dense layer\n",
        "dense_layer_6 = Dense(16, activation='relu')(pooling_layer_6)\n",
        "dense_layer_7 = Dense(16, activation='relu')(pooling_layer_7)\n",
        "\n",
        "concatenated_layer = concatenate([dense_layer_6, dense_layer_7])\n",
        "\n",
        "output_layer_6_7 = Dense(3, activation='relu')(concatenated_layer)\n",
        "\n",
        "model_6_7 = Model(inputs=[input_layer_6, input_layer_7], outputs=output_layer_6_7)\n",
        "\n",
        "model_6_7.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "UlJbDRHeAMIh",
        "outputId": "236b116f-6575-446e-f783-08a821f9a34a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"392pt\" viewBox=\"0.00 0.00 915.50 470.00\" width=\"763pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 466)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-466 911.5,-466 911.5,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140387384020688 -->\n<g class=\"node\" id=\"node1\">\n<title>140387384020688</title>\n<polygon fill=\"none\" points=\"106,-415.5 106,-461.5 439,-461.5 439,-415.5 106,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"154.5\" y=\"-446.3\">input_layer_6</text>\n<polyline fill=\"none\" points=\"106,-438.5 203,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"154.5\" y=\"-423.3\">InputLayer</text>\n<polyline fill=\"none\" points=\"203,-415.5 203,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"203,-438.5 261,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"261,-415.5 261,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"305.5\" y=\"-434.8\">[(None, 16)]</text>\n<polyline fill=\"none\" points=\"350,-415.5 350,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394.5\" y=\"-434.8\">[(None, 16)]</text>\n</g>\n<!-- 140383273972816 -->\n<g class=\"node\" id=\"node3\">\n<title>140383273972816</title>\n<polygon fill=\"none\" points=\"332.5,-332.5 332.5,-378.5 570.5,-378.5 570.5,-332.5 332.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399.5\" y=\"-363.3\">GloVe_Embeddings</text>\n<polyline fill=\"none\" points=\"332.5,-355.5 466.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399.5\" y=\"-340.3\">Embedding</text>\n<polyline fill=\"none\" points=\"466.5,-332.5 466.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"495.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"466.5,-355.5 524.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"495.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"524.5,-332.5 524.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"536\" y=\"-351.8\">?</text>\n<polyline fill=\"none\" points=\"547.5,-332.5 547.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"559\" y=\"-351.8\">?</text>\n</g>\n<!-- 140387384020688&#45;&gt;140383273972816 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140387384020688-&gt;140383273972816</title>\n<path d=\"M322.3615,-415.3799C344.0913,-405.304 369.7278,-393.4167 392.4077,-382.9003\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"393.9959,-386.0219 401.5958,-378.6399 391.0513,-379.6714 393.9959,-386.0219\" stroke=\"#000000\"/>\n</g>\n<!-- 140387384020816 -->\n<g class=\"node\" id=\"node2\">\n<title>140387384020816</title>\n<polygon fill=\"none\" points=\"457,-415.5 457,-461.5 804,-461.5 804,-415.5 457,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"505.5\" y=\"-446.3\">input_layer_7</text>\n<polyline fill=\"none\" points=\"457,-438.5 554,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"505.5\" y=\"-423.3\">InputLayer</text>\n<polyline fill=\"none\" points=\"554,-415.5 554,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"583\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"554,-438.5 612,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"583\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"612,-415.5 612,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"660\" y=\"-434.8\">[(None, 128)]</text>\n<polyline fill=\"none\" points=\"708,-415.5 708,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"756\" y=\"-434.8\">[(None, 128)]</text>\n</g>\n<!-- 140387384020816&#45;&gt;140383273972816 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140387384020816-&gt;140383273972816</title>\n<path d=\"M580.6385,-415.3799C558.9087,-405.304 533.2722,-393.4167 510.5923,-382.9003\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"511.9487,-379.6714 501.4042,-378.6399 509.0041,-386.0219 511.9487,-379.6714\" stroke=\"#000000\"/>\n</g>\n<!-- 140383196808976 -->\n<g class=\"node\" id=\"node4\">\n<title>140383196808976</title>\n<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 441,-295.5 441,-249.5 0,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"93\" y=\"-280.3\">global_average_pooling1d_4</text>\n<polyline fill=\"none\" points=\"0,-272.5 186,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"93\" y=\"-257.3\">GlobalAveragePooling1D</text>\n<polyline fill=\"none\" points=\"186,-249.5 186,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"215\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"186,-272.5 244,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"215\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"244,-249.5 244,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"299\" y=\"-268.8\">(None, 16, 300)</text>\n<polyline fill=\"none\" points=\"354,-249.5 354,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-268.8\">(None, 300)</text>\n</g>\n<!-- 140383273972816&#45;&gt;140383196808976 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140383273972816-&gt;140383196808976</title>\n<path d=\"M387.4604,-332.4901C358.5445,-322.1004 324.2196,-309.7672 294.2347,-298.9934\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"295.2587,-295.6423 284.6642,-295.5547 292.8916,-302.23 295.2587,-295.6423\" stroke=\"#000000\"/>\n</g>\n<!-- 140380907581200 -->\n<g class=\"node\" id=\"node5\">\n<title>140380907581200</title>\n<polygon fill=\"none\" points=\"459.5,-249.5 459.5,-295.5 907.5,-295.5 907.5,-249.5 459.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"552.5\" y=\"-280.3\">global_average_pooling1d_5</text>\n<polyline fill=\"none\" points=\"459.5,-272.5 645.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"552.5\" y=\"-257.3\">GlobalAveragePooling1D</text>\n<polyline fill=\"none\" points=\"645.5,-249.5 645.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"674.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"645.5,-272.5 703.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"674.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"703.5,-249.5 703.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"762\" y=\"-268.8\">(None, 128, 300)</text>\n<polyline fill=\"none\" points=\"820.5,-249.5 820.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"864\" y=\"-268.8\">(None, 300)</text>\n</g>\n<!-- 140383273972816&#45;&gt;140380907581200 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140383273972816-&gt;140380907581200</title>\n<path d=\"M515.8168,-332.4901C544.8579,-322.1004 579.3314,-309.7672 609.4461,-298.9934\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"610.8215,-302.2187 619.058,-295.5547 608.4635,-295.6278 610.8215,-302.2187\" stroke=\"#000000\"/>\n</g>\n<!-- 140380908506960 -->\n<g class=\"node\" id=\"node6\">\n<title>140380908506960</title>\n<polygon fill=\"none\" points=\"146.5,-166.5 146.5,-212.5 442.5,-212.5 442.5,-166.5 146.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182\" y=\"-197.3\">dense_12</text>\n<polyline fill=\"none\" points=\"146.5,-189.5 217.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"182\" y=\"-174.3\">Dense</text>\n<polyline fill=\"none\" points=\"217.5,-166.5 217.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"217.5,-189.5 275.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"275.5,-166.5 275.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"319\" y=\"-185.8\">(None, 300)</text>\n<polyline fill=\"none\" points=\"362.5,-166.5 362.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"402.5\" y=\"-185.8\">(None, 16)</text>\n</g>\n<!-- 140383196808976&#45;&gt;140380908506960 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140383196808976-&gt;140380908506960</title>\n<path d=\"M241.1131,-249.3799C249.1444,-240.3718 258.4665,-229.916 267.052,-220.2863\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"269.6984,-222.5774 273.7408,-212.784 264.4735,-217.919 269.6984,-222.5774\" stroke=\"#000000\"/>\n</g>\n<!-- 140380908742032 -->\n<g class=\"node\" id=\"node7\">\n<title>140380908742032</title>\n<polygon fill=\"none\" points=\"497.5,-166.5 497.5,-212.5 793.5,-212.5 793.5,-166.5 497.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"533\" y=\"-197.3\">dense_13</text>\n<polyline fill=\"none\" points=\"497.5,-189.5 568.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"533\" y=\"-174.3\">Dense</text>\n<polyline fill=\"none\" points=\"568.5,-166.5 568.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"597.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"568.5,-189.5 626.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"597.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"626.5,-166.5 626.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"670\" y=\"-185.8\">(None, 300)</text>\n<polyline fill=\"none\" points=\"713.5,-166.5 713.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"753.5\" y=\"-185.8\">(None, 16)</text>\n</g>\n<!-- 140380907581200&#45;&gt;140380908742032 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140380907581200-&gt;140380908742032</title>\n<path d=\"M672.9149,-249.3799C669.0357,-240.907 664.5701,-231.1531 660.3844,-222.0107\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"663.5053,-220.4194 656.1601,-212.784 657.1406,-223.3334 663.5053,-220.4194\" stroke=\"#000000\"/>\n</g>\n<!-- 140380924119888 -->\n<g class=\"node\" id=\"node8\">\n<title>140380924119888</title>\n<polygon fill=\"none\" points=\"260,-83.5 260,-129.5 643,-129.5 643,-83.5 260,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"303\" y=\"-114.3\">concatenate</text>\n<polyline fill=\"none\" points=\"260,-106.5 346,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"303\" y=\"-91.3\">Concatenate</text>\n<polyline fill=\"none\" points=\"346,-83.5 346,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"375\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"346,-106.5 404,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"375\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"404,-83.5 404,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"483.5\" y=\"-102.8\">[(None, 16), (None, 16)]</text>\n<polyline fill=\"none\" points=\"563,-83.5 563,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"603\" y=\"-102.8\">(None, 32)</text>\n</g>\n<!-- 140380908506960&#45;&gt;140380924119888 -->\n<g class=\"edge\" id=\"edge7\">\n<title>140380908506960-&gt;140380924119888</title>\n<path d=\"M338.2333,-166.3799C357.0383,-156.4384 379.179,-144.7334 398.8734,-134.3217\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"400.5244,-137.4079 407.7293,-129.6399 397.2528,-131.2195 400.5244,-137.4079\" stroke=\"#000000\"/>\n</g>\n<!-- 140380908742032&#45;&gt;140380924119888 -->\n<g class=\"edge\" id=\"edge8\">\n<title>140380908742032-&gt;140380924119888</title>\n<path d=\"M591.7178,-166.4901C567.854,-156.2803 539.6035,-144.1938 514.7358,-133.5545\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"515.9575,-130.2703 505.3868,-129.5547 513.204,-136.7061 515.9575,-130.2703\" stroke=\"#000000\"/>\n</g>\n<!-- 140387384127056 -->\n<g class=\"node\" id=\"node9\">\n<title>140387384127056</title>\n<polygon fill=\"none\" points=\"311,-.5 311,-46.5 592,-46.5 592,-.5 311,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"346.5\" y=\"-31.3\">dense_14</text>\n<polyline fill=\"none\" points=\"311,-23.5 382,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"346.5\" y=\"-8.3\">Dense</text>\n<polyline fill=\"none\" points=\"382,-.5 382,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"411\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"382,-23.5 440,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"411\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"440,-.5 440,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"480\" y=\"-19.8\">(None, 32)</text>\n<polyline fill=\"none\" points=\"520,-.5 520,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"556\" y=\"-19.8\">(None, 3)</text>\n</g>\n<!-- 140380924119888&#45;&gt;140387384127056 -->\n<g class=\"edge\" id=\"edge9\">\n<title>140380924119888-&gt;140387384127056</title>\n<path d=\"M451.5,-83.3799C451.5,-75.1745 451.5,-65.7679 451.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"455.0001,-56.784 451.5,-46.784 448.0001,-56.784 455.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils import vis_utils\n",
        "SVG(vis_utils.model_to_dot(model_6_7, show_shapes=True, show_layer_names=True, dpi=60).create(prog='dot', format='svg'))  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVl_cKt903Z9"
      },
      "source": [
        "Train and evaluate your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEzz9deDF2rc",
        "outputId": "67f07c27-a331-493d-95eb-c2ec73ac6bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "22/22 [==============================] - 2s 16ms/step - loss: nan - accuracy: 0.3812 - val_loss: 1.1648 - val_accuracy: 0.3739\n",
            "Epoch 2/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.1261 - accuracy: 0.3534 - val_loss: 1.0654 - val_accuracy: 0.3746\n",
            "Epoch 3/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.0685 - accuracy: 0.3850 - val_loss: 1.0330 - val_accuracy: 0.4294\n",
            "Epoch 4/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 1.0425 - accuracy: 0.4358 - val_loss: 1.0126 - val_accuracy: 0.4632\n",
            "Epoch 5/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.0237 - accuracy: 0.4680 - val_loss: 0.9929 - val_accuracy: 0.4910\n",
            "Epoch 6/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 1.0069 - accuracy: 0.4948 - val_loss: 0.9736 - val_accuracy: 0.5150\n",
            "Epoch 7/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9896 - accuracy: 0.5138 - val_loss: 0.9556 - val_accuracy: 0.5330\n",
            "Epoch 8/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9733 - accuracy: 0.5291 - val_loss: 0.9359 - val_accuracy: 0.5458\n",
            "Epoch 9/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9551 - accuracy: 0.5466 - val_loss: 0.9159 - val_accuracy: 0.5593\n",
            "Epoch 10/200\n",
            "22/22 [==============================] - 0s 7ms/step - loss: 0.9372 - accuracy: 0.5584 - val_loss: 0.8976 - val_accuracy: 0.5653\n",
            "Epoch 11/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9240 - accuracy: 0.5682 - val_loss: 0.8835 - val_accuracy: 0.5721\n",
            "Epoch 12/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9208 - accuracy: 0.5732 - val_loss: 0.9037 - val_accuracy: 0.5796\n",
            "Epoch 13/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9149 - accuracy: 0.5778 - val_loss: 0.8825 - val_accuracy: 0.5908\n",
            "Epoch 14/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9033 - accuracy: 0.5830 - val_loss: 0.8682 - val_accuracy: 0.5968\n",
            "Epoch 15/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8972 - accuracy: 0.5881 - val_loss: 0.8683 - val_accuracy: 0.6021\n",
            "Epoch 16/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8877 - accuracy: 0.5923 - val_loss: 0.8621 - val_accuracy: 0.6051\n",
            "Epoch 17/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8850 - accuracy: 0.5955 - val_loss: 0.8547 - val_accuracy: 0.6074\n",
            "Epoch 18/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8743 - accuracy: 0.5965 - val_loss: 0.8485 - val_accuracy: 0.6119\n",
            "Epoch 19/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8689 - accuracy: 0.5995 - val_loss: 0.8476 - val_accuracy: 0.6089\n",
            "Epoch 20/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8613 - accuracy: 0.6022 - val_loss: 0.8536 - val_accuracy: 0.6119\n",
            "Epoch 21/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8542 - accuracy: 0.6083 - val_loss: 0.8415 - val_accuracy: 0.6179\n",
            "Epoch 22/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8448 - accuracy: 0.6124 - val_loss: 0.8639 - val_accuracy: 0.6164\n",
            "Epoch 23/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8502 - accuracy: 0.6126 - val_loss: 0.8307 - val_accuracy: 0.6216\n",
            "Epoch 24/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8389 - accuracy: 0.6155 - val_loss: 0.8231 - val_accuracy: 0.6224\n",
            "Epoch 25/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8303 - accuracy: 0.6184 - val_loss: 0.8463 - val_accuracy: 0.6246\n",
            "Epoch 26/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8258 - accuracy: 0.6225 - val_loss: 0.8393 - val_accuracy: 0.6216\n",
            "Epoch 27/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8204 - accuracy: 0.6248 - val_loss: 0.8404 - val_accuracy: 0.6239\n",
            "Epoch 28/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8173 - accuracy: 0.6273 - val_loss: 0.8568 - val_accuracy: 0.6209\n",
            "Epoch 29/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8111 - accuracy: 0.6263 - val_loss: 0.8377 - val_accuracy: 0.6224\n",
            "Epoch 30/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8133 - accuracy: 0.6284 - val_loss: 0.8614 - val_accuracy: 0.6209\n",
            "Epoch 31/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8127 - accuracy: 0.6308 - val_loss: 0.8569 - val_accuracy: 0.6216\n",
            "Epoch 32/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8042 - accuracy: 0.6318 - val_loss: 0.8591 - val_accuracy: 0.6201\n",
            "Epoch 33/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8013 - accuracy: 0.6343 - val_loss: 0.8521 - val_accuracy: 0.6186\n",
            "Epoch 34/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7978 - accuracy: 0.6343 - val_loss: 0.8637 - val_accuracy: 0.6164\n",
            "Epoch 35/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7970 - accuracy: 0.6321 - val_loss: 0.8504 - val_accuracy: 0.6224\n",
            "Epoch 36/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.9135 - accuracy: 0.5316 - val_loss: 1.0104 - val_accuracy: 0.3926\n",
            "Epoch 37/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 1.0060 - accuracy: 0.4230 - val_loss: 0.9777 - val_accuracy: 0.4925\n",
            "Epoch 38/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.9700 - accuracy: 0.5196 - val_loss: 0.9499 - val_accuracy: 0.5503\n",
            "Epoch 39/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9462 - accuracy: 0.5698 - val_loss: 0.9298 - val_accuracy: 0.5818\n",
            "Epoch 40/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.9258 - accuracy: 0.5936 - val_loss: 0.9126 - val_accuracy: 0.5961\n",
            "Epoch 41/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.9076 - accuracy: 0.6034 - val_loss: 0.8974 - val_accuracy: 0.6059\n",
            "Epoch 42/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8925 - accuracy: 0.6100 - val_loss: 0.8841 - val_accuracy: 0.6149\n",
            "Epoch 43/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8798 - accuracy: 0.6132 - val_loss: 0.8727 - val_accuracy: 0.6081\n",
            "Epoch 44/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8680 - accuracy: 0.6180 - val_loss: 0.8622 - val_accuracy: 0.6164\n",
            "Epoch 45/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8576 - accuracy: 0.6194 - val_loss: 0.8530 - val_accuracy: 0.6134\n",
            "Epoch 46/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8490 - accuracy: 0.6199 - val_loss: 0.8451 - val_accuracy: 0.6104\n",
            "Epoch 47/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8415 - accuracy: 0.6217 - val_loss: 0.8384 - val_accuracy: 0.6104\n",
            "Epoch 48/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8351 - accuracy: 0.6211 - val_loss: 0.8330 - val_accuracy: 0.6104\n",
            "Epoch 49/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8296 - accuracy: 0.6180 - val_loss: 0.8287 - val_accuracy: 0.6104\n",
            "Epoch 50/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8264 - accuracy: 0.6191 - val_loss: 0.8255 - val_accuracy: 0.6119\n",
            "Epoch 51/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8215 - accuracy: 0.6176 - val_loss: 0.8224 - val_accuracy: 0.6126\n",
            "Epoch 52/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8189 - accuracy: 0.6204 - val_loss: 0.8205 - val_accuracy: 0.6126\n",
            "Epoch 53/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8190 - accuracy: 0.6185 - val_loss: 0.8239 - val_accuracy: 0.6119\n",
            "Epoch 54/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8158 - accuracy: 0.6209 - val_loss: 0.8250 - val_accuracy: 0.6066\n",
            "Epoch 55/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8101 - accuracy: 0.6230 - val_loss: 0.8240 - val_accuracy: 0.6141\n",
            "Epoch 56/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8070 - accuracy: 0.6232 - val_loss: 0.8287 - val_accuracy: 0.6156\n",
            "Epoch 57/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8023 - accuracy: 0.6237 - val_loss: 0.8284 - val_accuracy: 0.6149\n",
            "Epoch 58/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8023 - accuracy: 0.6237 - val_loss: 0.8256 - val_accuracy: 0.6164\n",
            "Epoch 59/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7952 - accuracy: 0.6256 - val_loss: 0.8336 - val_accuracy: 0.6141\n",
            "Epoch 60/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7945 - accuracy: 0.6250 - val_loss: 0.8415 - val_accuracy: 0.6194\n",
            "Epoch 61/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7919 - accuracy: 0.6297 - val_loss: 0.8421 - val_accuracy: 0.6179\n",
            "Epoch 62/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7891 - accuracy: 0.6294 - val_loss: 0.8411 - val_accuracy: 0.6231\n",
            "Epoch 63/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7868 - accuracy: 0.6296 - val_loss: 0.8420 - val_accuracy: 0.6201\n",
            "Epoch 64/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7845 - accuracy: 0.6324 - val_loss: 0.8409 - val_accuracy: 0.6216\n",
            "Epoch 65/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7814 - accuracy: 0.6322 - val_loss: 0.8405 - val_accuracy: 0.6224\n",
            "Epoch 66/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.9411 - accuracy: 0.5794 - val_loss: 0.9992 - val_accuracy: 0.4955\n",
            "Epoch 67/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.9379 - accuracy: 0.5169 - val_loss: 0.8956 - val_accuracy: 0.5428\n",
            "Epoch 68/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8920 - accuracy: 0.5517 - val_loss: 0.8761 - val_accuracy: 0.5646\n",
            "Epoch 69/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8792 - accuracy: 0.5667 - val_loss: 0.8655 - val_accuracy: 0.5698\n",
            "Epoch 70/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8665 - accuracy: 0.5744 - val_loss: 0.8565 - val_accuracy: 0.5796\n",
            "Epoch 71/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8565 - accuracy: 0.5794 - val_loss: 0.8494 - val_accuracy: 0.5848\n",
            "Epoch 72/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8480 - accuracy: 0.5845 - val_loss: 0.8422 - val_accuracy: 0.5878\n",
            "Epoch 73/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8395 - accuracy: 0.5910 - val_loss: 0.8527 - val_accuracy: 0.5976\n",
            "Epoch 74/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8334 - accuracy: 0.5957 - val_loss: 0.8390 - val_accuracy: 0.6006\n",
            "Epoch 75/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8278 - accuracy: 0.6008 - val_loss: 0.8368 - val_accuracy: 0.6036\n",
            "Epoch 76/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8233 - accuracy: 0.6026 - val_loss: 0.8425 - val_accuracy: 0.6066\n",
            "Epoch 77/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8187 - accuracy: 0.6052 - val_loss: 0.8488 - val_accuracy: 0.6096\n",
            "Epoch 78/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8141 - accuracy: 0.6076 - val_loss: 0.8462 - val_accuracy: 0.6081\n",
            "Epoch 79/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8130 - accuracy: 0.6138 - val_loss: 0.8425 - val_accuracy: 0.6059\n",
            "Epoch 80/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8074 - accuracy: 0.6138 - val_loss: 0.8548 - val_accuracy: 0.6126\n",
            "Epoch 81/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.8045 - accuracy: 0.6177 - val_loss: 0.8787 - val_accuracy: 0.6149\n",
            "Epoch 82/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8007 - accuracy: 0.6236 - val_loss: 0.8580 - val_accuracy: 0.6171\n",
            "Epoch 83/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8003 - accuracy: 0.6247 - val_loss: 0.8607 - val_accuracy: 0.6209\n",
            "Epoch 84/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7953 - accuracy: 0.6292 - val_loss: 0.8835 - val_accuracy: 0.6216\n",
            "Epoch 85/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7932 - accuracy: 0.6342 - val_loss: 0.8801 - val_accuracy: 0.6276\n",
            "Epoch 86/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7892 - accuracy: 0.6386 - val_loss: 0.8841 - val_accuracy: 0.6284\n",
            "Epoch 87/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7871 - accuracy: 0.6391 - val_loss: 0.8953 - val_accuracy: 0.6291\n",
            "Epoch 88/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7848 - accuracy: 0.6401 - val_loss: 0.9260 - val_accuracy: 0.6299\n",
            "Epoch 89/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7826 - accuracy: 0.6416 - val_loss: 0.8834 - val_accuracy: 0.6239\n",
            "Epoch 90/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7800 - accuracy: 0.6417 - val_loss: 0.9002 - val_accuracy: 0.6254\n",
            "Epoch 91/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7775 - accuracy: 0.6412 - val_loss: 0.8932 - val_accuracy: 0.6269\n",
            "Epoch 92/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7767 - accuracy: 0.6407 - val_loss: 0.8906 - val_accuracy: 0.6299\n",
            "Epoch 93/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7737 - accuracy: 0.6428 - val_loss: 0.8901 - val_accuracy: 0.6291\n",
            "Epoch 94/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7737 - accuracy: 0.6418 - val_loss: 0.9069 - val_accuracy: 0.6261\n",
            "Epoch 95/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7722 - accuracy: 0.6443 - val_loss: 0.9142 - val_accuracy: 0.6254\n",
            "Epoch 96/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7707 - accuracy: 0.6446 - val_loss: 0.9186 - val_accuracy: 0.6269\n",
            "Epoch 97/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7686 - accuracy: 0.6461 - val_loss: 0.9532 - val_accuracy: 0.6299\n",
            "Epoch 98/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7665 - accuracy: 0.6448 - val_loss: 0.9501 - val_accuracy: 0.6284\n",
            "Epoch 99/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7649 - accuracy: 0.6452 - val_loss: 0.9742 - val_accuracy: 0.6284\n",
            "Epoch 100/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7623 - accuracy: 0.6456 - val_loss: 0.9923 - val_accuracy: 0.6314\n",
            "Epoch 101/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7606 - accuracy: 0.6462 - val_loss: 0.9756 - val_accuracy: 0.6299\n",
            "Epoch 102/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7594 - accuracy: 0.6473 - val_loss: 0.9521 - val_accuracy: 0.6306\n",
            "Epoch 103/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7568 - accuracy: 0.6464 - val_loss: 0.9640 - val_accuracy: 0.6344\n",
            "Epoch 104/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7549 - accuracy: 0.6470 - val_loss: 0.9959 - val_accuracy: 0.6314\n",
            "Epoch 105/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7536 - accuracy: 0.6485 - val_loss: 0.9890 - val_accuracy: 0.6336\n",
            "Epoch 106/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7517 - accuracy: 0.6480 - val_loss: 0.9791 - val_accuracy: 0.6344\n",
            "Epoch 107/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7520 - accuracy: 0.6473 - val_loss: 0.9774 - val_accuracy: 0.6321\n",
            "Epoch 108/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7497 - accuracy: 0.6492 - val_loss: 1.0151 - val_accuracy: 0.6291\n",
            "Epoch 109/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7480 - accuracy: 0.6484 - val_loss: 1.0000 - val_accuracy: 0.6366\n",
            "Epoch 110/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7500 - accuracy: 0.6505 - val_loss: 0.9807 - val_accuracy: 0.6351\n",
            "Epoch 111/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7461 - accuracy: 0.6509 - val_loss: 0.9752 - val_accuracy: 0.6344\n",
            "Epoch 112/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7451 - accuracy: 0.6509 - val_loss: 0.9601 - val_accuracy: 0.6351\n",
            "Epoch 113/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7539 - accuracy: 0.6491 - val_loss: 0.9325 - val_accuracy: 0.6336\n",
            "Epoch 114/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7464 - accuracy: 0.6522 - val_loss: nan - val_accuracy: 0.6329\n",
            "Epoch 115/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7426 - accuracy: 0.6547 - val_loss: 0.9802 - val_accuracy: 0.6329\n",
            "Epoch 116/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7387 - accuracy: 0.6550 - val_loss: 1.0080 - val_accuracy: 0.6314\n",
            "Epoch 117/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7380 - accuracy: 0.6540 - val_loss: nan - val_accuracy: 0.6321\n",
            "Epoch 118/200\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.7384 - accuracy: 0.6554 - val_loss: 0.9769 - val_accuracy: 0.6329\n",
            "Epoch 119/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6552 - val_loss: 1.0147 - val_accuracy: 0.6321\n",
            "Epoch 120/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6566 - val_loss: nan - val_accuracy: 0.6314\n",
            "Epoch 121/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6562 - val_loss: 0.9867 - val_accuracy: 0.6291\n",
            "Epoch 122/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6565 - val_loss: nan - val_accuracy: 0.6374\n",
            "Epoch 123/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7329 - accuracy: 0.6582 - val_loss: 0.9864 - val_accuracy: 0.6299\n",
            "Epoch 124/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7349 - accuracy: 0.6566 - val_loss: 0.9757 - val_accuracy: 0.6291\n",
            "Epoch 125/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6577 - val_loss: 0.9298 - val_accuracy: 0.6126\n",
            "Epoch 126/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7784 - accuracy: 0.6387 - val_loss: 0.9803 - val_accuracy: 0.6239\n",
            "Epoch 127/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7679 - accuracy: 0.6466 - val_loss: 0.9871 - val_accuracy: 0.6291\n",
            "Epoch 128/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7590 - accuracy: 0.6495 - val_loss: 0.9528 - val_accuracy: 0.6329\n",
            "Epoch 129/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7429 - accuracy: 0.6522 - val_loss: 1.0037 - val_accuracy: 0.6359\n",
            "Epoch 130/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7388 - accuracy: 0.6556 - val_loss: 1.0302 - val_accuracy: 0.6366\n",
            "Epoch 131/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7368 - accuracy: 0.6577 - val_loss: 0.9795 - val_accuracy: 0.6359\n",
            "Epoch 132/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7310 - accuracy: 0.6576 - val_loss: 1.0283 - val_accuracy: 0.6344\n",
            "Epoch 133/200\n",
            "22/22 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.6584 - val_loss: 0.8786 - val_accuracy: 0.6299\n",
            "Epoch 134/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7453 - accuracy: 0.6571 - val_loss: 0.9457 - val_accuracy: 0.6366\n",
            "Epoch 135/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7368 - accuracy: 0.6584 - val_loss: 0.9905 - val_accuracy: 0.6359\n",
            "Epoch 136/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7318 - accuracy: 0.6583 - val_loss: 0.9464 - val_accuracy: 0.6366\n",
            "Epoch 137/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7284 - accuracy: 0.6606 - val_loss: 1.0089 - val_accuracy: 0.6359\n",
            "Epoch 138/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6591 - val_loss: 1.0625 - val_accuracy: 0.6359\n",
            "Epoch 139/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6627 - val_loss: 1.0169 - val_accuracy: 0.6314\n",
            "Epoch 140/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6642 - val_loss: 1.0593 - val_accuracy: 0.6366\n",
            "Epoch 141/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6625 - val_loss: 1.0092 - val_accuracy: 0.6404\n",
            "Epoch 142/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6628 - val_loss: 1.0362 - val_accuracy: 0.6381\n",
            "Epoch 143/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6643 - val_loss: 1.0412 - val_accuracy: 0.6389\n",
            "Epoch 144/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6632 - val_loss: 1.0285 - val_accuracy: 0.6419\n",
            "Epoch 145/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6654 - val_loss: 1.0484 - val_accuracy: 0.6381\n",
            "Epoch 146/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6653 - val_loss: 1.0705 - val_accuracy: 0.6359\n",
            "Epoch 147/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6649 - val_loss: 1.0916 - val_accuracy: 0.6389\n",
            "Epoch 148/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7190 - accuracy: 0.6639 - val_loss: 0.9134 - val_accuracy: 0.6351\n",
            "Epoch 149/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7199 - accuracy: 0.6598 - val_loss: 1.0844 - val_accuracy: 0.6389\n",
            "Epoch 150/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6661 - val_loss: 1.0942 - val_accuracy: 0.6396\n",
            "Epoch 151/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6660 - val_loss: 1.0621 - val_accuracy: 0.6374\n",
            "Epoch 152/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7757 - accuracy: 0.6505 - val_loss: 0.8947 - val_accuracy: 0.6404\n",
            "Epoch 153/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7649 - accuracy: 0.6567 - val_loss: 0.9460 - val_accuracy: 0.6374\n",
            "Epoch 154/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7439 - accuracy: 0.6590 - val_loss: 1.0188 - val_accuracy: 0.6426\n",
            "Epoch 155/200\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.7342 - accuracy: 0.6591 - val_loss: 1.0213 - val_accuracy: 0.6336\n",
            "Epoch 156/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7302 - accuracy: 0.6634 - val_loss: 1.0944 - val_accuracy: 0.6336\n",
            "Epoch 157/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7266 - accuracy: 0.6647 - val_loss: 1.0409 - val_accuracy: 0.6351\n",
            "Epoch 158/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7823 - accuracy: 0.6534 - val_loss: 0.8690 - val_accuracy: 0.6359\n",
            "Epoch 159/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7605 - accuracy: 0.6563 - val_loss: 0.8711 - val_accuracy: 0.6351\n",
            "Epoch 160/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7389 - accuracy: 0.6641 - val_loss: 1.0082 - val_accuracy: 0.6329\n",
            "Epoch 161/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7282 - accuracy: 0.6657 - val_loss: 1.0311 - val_accuracy: 0.6344\n",
            "Epoch 162/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7240 - accuracy: 0.6666 - val_loss: 1.0695 - val_accuracy: 0.6351\n",
            "Epoch 163/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7167 - accuracy: 0.6685 - val_loss: 1.0939 - val_accuracy: 0.6366\n",
            "Epoch 164/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7128 - accuracy: 0.6688 - val_loss: 1.1281 - val_accuracy: 0.6374\n",
            "Epoch 165/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7123 - accuracy: 0.6658 - val_loss: 1.0133 - val_accuracy: 0.6314\n",
            "Epoch 166/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7266 - accuracy: 0.6563 - val_loss: 0.9365 - val_accuracy: 0.6359\n",
            "Epoch 167/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7226 - accuracy: 0.6607 - val_loss: 1.0166 - val_accuracy: 0.6426\n",
            "Epoch 168/200\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.7101 - accuracy: 0.6630 - val_loss: 1.0495 - val_accuracy: 0.6419\n",
            "Epoch 169/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7138 - accuracy: 0.6663 - val_loss: 1.0401 - val_accuracy: 0.6464\n",
            "Epoch 170/200\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.7238 - accuracy: 0.6605 - val_loss: 0.9437 - val_accuracy: 0.6456\n",
            "Epoch 171/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7118 - accuracy: 0.6683 - val_loss: 1.0509 - val_accuracy: 0.6456\n",
            "Epoch 172/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7558 - accuracy: 0.6526 - val_loss: 0.9782 - val_accuracy: 0.6329\n",
            "Epoch 173/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7521 - accuracy: 0.6565 - val_loss: 1.0139 - val_accuracy: 0.6381\n",
            "Epoch 174/200\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.7203 - accuracy: 0.6674 - val_loss: 1.0185 - val_accuracy: 0.6374\n",
            "Epoch 175/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7146 - accuracy: 0.6688 - val_loss: 1.0229 - val_accuracy: 0.6336\n",
            "Epoch 176/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6696 - val_loss: 1.0481 - val_accuracy: 0.6411\n",
            "Epoch 177/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7487 - accuracy: 0.6576 - val_loss: 0.8929 - val_accuracy: 0.6321\n",
            "Epoch 178/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7874 - accuracy: 0.6386 - val_loss: 0.8442 - val_accuracy: 0.6344\n",
            "Epoch 179/200\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.7452 - accuracy: 0.6606 - val_loss: 0.9180 - val_accuracy: 0.6396\n",
            "Epoch 180/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7197 - accuracy: 0.6652 - val_loss: 1.0702 - val_accuracy: 0.6344\n",
            "Epoch 181/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7110 - accuracy: 0.6674 - val_loss: 1.0895 - val_accuracy: 0.6389\n",
            "Epoch 182/200\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.7068 - accuracy: 0.6697 - val_loss: 1.1660 - val_accuracy: 0.6306\n",
            "Epoch 183/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 1.0306 - accuracy: 0.5461 - val_loss: 0.9106 - val_accuracy: 0.5398\n",
            "Epoch 184/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8591 - accuracy: 0.5806 - val_loss: 0.8987 - val_accuracy: 0.5856\n",
            "Epoch 185/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8426 - accuracy: 0.6014 - val_loss: 0.8848 - val_accuracy: 0.5916\n",
            "Epoch 186/200\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.8274 - accuracy: 0.6083 - val_loss: 0.8740 - val_accuracy: 0.5953\n",
            "Epoch 187/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8158 - accuracy: 0.6176 - val_loss: 0.8931 - val_accuracy: 0.6104\n",
            "Epoch 188/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8081 - accuracy: 0.6198 - val_loss: 0.8754 - val_accuracy: 0.6179\n",
            "Epoch 189/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7965 - accuracy: 0.6297 - val_loss: 0.8854 - val_accuracy: 0.6171\n",
            "Epoch 190/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7852 - accuracy: 0.6335 - val_loss: 0.9307 - val_accuracy: 0.6186\n",
            "Epoch 191/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8109 - accuracy: 0.6106 - val_loss: 0.8714 - val_accuracy: 0.5826\n",
            "Epoch 192/200\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.8153 - accuracy: 0.6049 - val_loss: 0.8674 - val_accuracy: 0.5991\n",
            "Epoch 193/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.8072 - accuracy: 0.6150 - val_loss: 0.8734 - val_accuracy: 0.6021\n",
            "Epoch 194/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7982 - accuracy: 0.6186 - val_loss: 0.8944 - val_accuracy: 0.6059\n",
            "Epoch 195/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7907 - accuracy: 0.6253 - val_loss: 0.9044 - val_accuracy: 0.6201\n",
            "Epoch 196/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7824 - accuracy: 0.6328 - val_loss: 0.9094 - val_accuracy: 0.6201\n",
            "Epoch 197/200\n",
            "22/22 [==============================] - 0s 10ms/step - loss: 0.7782 - accuracy: 0.6366 - val_loss: 0.8976 - val_accuracy: 0.6261\n",
            "Epoch 198/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7768 - accuracy: 0.6402 - val_loss: 0.9069 - val_accuracy: 0.6381\n",
            "Epoch 199/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7826 - accuracy: 0.6428 - val_loss: 0.8835 - val_accuracy: 0.6396\n",
            "Epoch 200/200\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.7731 - accuracy: 0.6413 - val_loss: 0.9153 - val_accuracy: 0.6351\n",
            "42/42 [==============================] - 0s 3ms/step - loss: 0.8893 - accuracy: 0.6355\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "\n",
        "model_6_7.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model_6_7.fit([x_train_aspect_pad_glove, x_train_review_pad_glove], y_train, epochs=200, batch_size=512, validation_data=([x_dev_aspect_pad_glove, x_dev_review_pad_glove], y_dev))\n",
        "\n",
        "loss, accuracy = model_6_7.evaluate([x_test_aspect_pad_glove, x_test_review_pad_glove], y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0npTvFuVt5R"
      },
      "source": [
        "## Model 3-2 CNN or LSTM model with multiple-input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCp9khOn0D2-"
      },
      "source": [
        "Modify the previous CNN or LSTM model to be compatible with multiple-input, similar to model 3-1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js5yZqmYywev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "145faa60-96ee-4201-899a-becdf138db16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_layer_8 (InputLayer)     [(None, 16)]         0           []                               \n",
            "                                                                                                  \n",
            " input_layer_9 (InputLayer)     [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " GloVe_Embeddings (Embedding)   multiple             120000300   ['input_layer_8[0][0]',          \n",
            "                                                                  'input_layer_9[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 14, 32)       28832       ['GloVe_Embeddings[5][0]']       \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 126, 32)      28832       ['GloVe_Embeddings[6][0]']       \n",
            "                                                                                                  \n",
            " global_average_pooling1d_6 (Gl  (None, 32)          0           ['conv1d_2[0][0]']               \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_7 (Gl  (None, 32)          0           ['conv1d_3[0][0]']               \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 16)           528         ['global_average_pooling1d_6[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 16)           528         ['global_average_pooling1d_7[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 32)           0           ['dense_15[0][0]',               \n",
            "                                                                  'dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 3)            99          ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 120,059,119\n",
            "Trainable params: 58,819\n",
            "Non-trainable params: 120,000,300\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "22/22 [==============================] - 2s 44ms/step - loss: 1.2283 - accuracy: 0.4763 - val_loss: 0.9909 - val_accuracy: 0.5135\n",
            "Epoch 2/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.9755 - accuracy: 0.5388 - val_loss: 0.9696 - val_accuracy: 0.5548\n",
            "Epoch 3/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.9399 - accuracy: 0.5720 - val_loss: 0.9038 - val_accuracy: 0.5938\n",
            "Epoch 4/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.9242 - accuracy: 0.5662 - val_loss: 0.9005 - val_accuracy: 0.5796\n",
            "Epoch 5/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.9089 - accuracy: 0.5963 - val_loss: 0.9666 - val_accuracy: 0.6306\n",
            "Epoch 6/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.9055 - accuracy: 0.5856 - val_loss: 0.8895 - val_accuracy: 0.5968\n",
            "Epoch 7/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.8789 - accuracy: 0.6073 - val_loss: 0.9371 - val_accuracy: 0.6186\n",
            "Epoch 8/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8660 - accuracy: 0.6135 - val_loss: 0.8923 - val_accuracy: 0.6299\n",
            "Epoch 9/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8651 - accuracy: 0.6202 - val_loss: 0.9088 - val_accuracy: 0.6299\n",
            "Epoch 10/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8463 - accuracy: 0.6260 - val_loss: 0.9017 - val_accuracy: 0.6201\n",
            "Epoch 11/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8255 - accuracy: 0.6221 - val_loss: 0.9156 - val_accuracy: 0.6426\n",
            "Epoch 12/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8911 - accuracy: 0.5455 - val_loss: 1.0127 - val_accuracy: 0.3641\n",
            "Epoch 13/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.9954 - accuracy: 0.4052 - val_loss: 0.9661 - val_accuracy: 0.5150\n",
            "Epoch 14/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.9464 - accuracy: 0.5664 - val_loss: 0.9187 - val_accuracy: 0.6201\n",
            "Epoch 15/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9101 - accuracy: 0.5894 - val_loss: 0.8937 - val_accuracy: 0.6201\n",
            "Epoch 16/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8857 - accuracy: 0.5965 - val_loss: 0.8680 - val_accuracy: 0.6224\n",
            "Epoch 17/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8667 - accuracy: 0.5982 - val_loss: 0.8705 - val_accuracy: 0.6336\n",
            "Epoch 18/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8572 - accuracy: 0.6142 - val_loss: 0.8780 - val_accuracy: 0.6329\n",
            "Epoch 19/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8468 - accuracy: 0.6152 - val_loss: 0.8855 - val_accuracy: 0.6299\n",
            "Epoch 20/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8338 - accuracy: 0.6188 - val_loss: 0.8832 - val_accuracy: 0.6381\n",
            "Epoch 21/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8195 - accuracy: 0.6242 - val_loss: 0.8757 - val_accuracy: 0.6329\n",
            "Epoch 22/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.8379 - accuracy: 0.6070 - val_loss: 0.8884 - val_accuracy: 0.6134\n",
            "Epoch 23/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8791 - accuracy: 0.5915 - val_loss: 0.8666 - val_accuracy: 0.6126\n",
            "Epoch 24/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8404 - accuracy: 0.6001 - val_loss: 0.8680 - val_accuracy: 0.6239\n",
            "Epoch 25/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8267 - accuracy: 0.6194 - val_loss: 0.8996 - val_accuracy: 0.6366\n",
            "Epoch 26/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8126 - accuracy: 0.6275 - val_loss: 0.8883 - val_accuracy: 0.6419\n",
            "Epoch 27/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8011 - accuracy: 0.6311 - val_loss: 0.9122 - val_accuracy: 0.6456\n",
            "Epoch 28/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7949 - accuracy: 0.6331 - val_loss: 0.9724 - val_accuracy: 0.6441\n",
            "Epoch 29/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7897 - accuracy: 0.6376 - val_loss: 0.9159 - val_accuracy: 0.6441\n",
            "Epoch 30/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7868 - accuracy: 0.6389 - val_loss: 0.9595 - val_accuracy: 0.6494\n",
            "Epoch 31/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7820 - accuracy: 0.6435 - val_loss: 0.9308 - val_accuracy: 0.6456\n",
            "Epoch 32/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.7783 - accuracy: 0.6470 - val_loss: 0.9305 - val_accuracy: 0.6547\n",
            "Epoch 33/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.7722 - accuracy: 0.6518 - val_loss: 0.9640 - val_accuracy: 0.6547\n",
            "Epoch 34/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7627 - accuracy: 0.6579 - val_loss: 1.0443 - val_accuracy: 0.6577\n",
            "Epoch 35/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.7591 - accuracy: 0.6609 - val_loss: 0.9530 - val_accuracy: 0.6532\n",
            "Epoch 36/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7557 - accuracy: 0.6653 - val_loss: 1.0558 - val_accuracy: 0.6502\n",
            "Epoch 37/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7523 - accuracy: 0.6644 - val_loss: 1.0464 - val_accuracy: 0.6396\n",
            "Epoch 38/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.7839 - accuracy: 0.6452 - val_loss: 1.0932 - val_accuracy: 0.4632\n",
            "Epoch 39/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9946 - accuracy: 0.4178 - val_loss: 1.0497 - val_accuracy: 0.3619\n",
            "Epoch 40/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.9729 - accuracy: 0.4380 - val_loss: 0.9593 - val_accuracy: 0.5008\n",
            "Epoch 41/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9294 - accuracy: 0.5688 - val_loss: 0.9246 - val_accuracy: 0.5908\n",
            "Epoch 42/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8933 - accuracy: 0.6102 - val_loss: 0.9030 - val_accuracy: 0.6134\n",
            "Epoch 43/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.8570 - accuracy: 0.6172 - val_loss: 0.8960 - val_accuracy: 0.5976\n",
            "Epoch 44/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8320 - accuracy: 0.6173 - val_loss: 0.9540 - val_accuracy: 0.6239\n",
            "Epoch 45/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8223 - accuracy: 0.6350 - val_loss: 0.9257 - val_accuracy: 0.6449\n",
            "Epoch 46/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7969 - accuracy: 0.6496 - val_loss: 0.9508 - val_accuracy: 0.6449\n",
            "Epoch 47/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8217 - accuracy: 0.6438 - val_loss: 0.9807 - val_accuracy: 0.5991\n",
            "Epoch 48/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.8440 - accuracy: 0.6242 - val_loss: 0.8965 - val_accuracy: 0.6239\n",
            "Epoch 49/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8139 - accuracy: 0.6296 - val_loss: 0.8972 - val_accuracy: 0.6351\n",
            "Epoch 50/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7892 - accuracy: 0.6404 - val_loss: 0.9668 - val_accuracy: 0.6426\n",
            "Epoch 51/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: nan - accuracy: 0.6394 - val_loss: 0.9396 - val_accuracy: 0.6321\n",
            "Epoch 52/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8316 - accuracy: 0.6293 - val_loss: 0.8597 - val_accuracy: 0.6314\n",
            "Epoch 53/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8072 - accuracy: 0.6419 - val_loss: 0.9042 - val_accuracy: 0.6344\n",
            "Epoch 54/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8028 - accuracy: 0.6413 - val_loss: 0.8817 - val_accuracy: 0.6389\n",
            "Epoch 55/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7912 - accuracy: 0.6474 - val_loss: 0.8949 - val_accuracy: 0.6396\n",
            "Epoch 56/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.7841 - accuracy: 0.6481 - val_loss: 0.9221 - val_accuracy: 0.6411\n",
            "Epoch 57/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7771 - accuracy: 0.6500 - val_loss: 0.9230 - val_accuracy: 0.6449\n",
            "Epoch 58/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7693 - accuracy: 0.6516 - val_loss: 0.9233 - val_accuracy: 0.6486\n",
            "Epoch 59/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.8154 - accuracy: 0.6346 - val_loss: 1.2458 - val_accuracy: 0.4737\n",
            "Epoch 60/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.9744 - accuracy: 0.4726 - val_loss: 0.9483 - val_accuracy: 0.5120\n",
            "Epoch 61/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.9403 - accuracy: 0.5570 - val_loss: 0.9293 - val_accuracy: 0.5586\n",
            "Epoch 62/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.9127 - accuracy: 0.5573 - val_loss: 0.9052 - val_accuracy: 0.5781\n",
            "Epoch 63/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8736 - accuracy: 0.5864 - val_loss: 0.9230 - val_accuracy: 0.5961\n",
            "Epoch 64/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.8517 - accuracy: 0.5909 - val_loss: 0.8879 - val_accuracy: 0.6021\n",
            "Epoch 65/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8350 - accuracy: 0.6048 - val_loss: 0.9600 - val_accuracy: 0.6111\n",
            "Epoch 66/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8279 - accuracy: 0.6277 - val_loss: 0.8881 - val_accuracy: 0.6299\n",
            "Epoch 67/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8308 - accuracy: 0.6369 - val_loss: 0.9734 - val_accuracy: 0.6344\n",
            "Epoch 68/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8191 - accuracy: 0.6423 - val_loss: 0.9459 - val_accuracy: 0.6411\n",
            "Epoch 69/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8821 - accuracy: 0.6142 - val_loss: 1.1265 - val_accuracy: 0.4302\n",
            "Epoch 70/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9568 - accuracy: 0.5009 - val_loss: 0.9507 - val_accuracy: 0.5818\n",
            "Epoch 71/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.9260 - accuracy: 0.6075 - val_loss: 0.9225 - val_accuracy: 0.6044\n",
            "Epoch 72/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.9038 - accuracy: 0.6107 - val_loss: 0.9125 - val_accuracy: 0.6059\n",
            "Epoch 73/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8860 - accuracy: 0.6208 - val_loss: 0.8963 - val_accuracy: 0.6164\n",
            "Epoch 74/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.8816 - accuracy: 0.6263 - val_loss: 0.8870 - val_accuracy: 0.6254\n",
            "Epoch 75/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8632 - accuracy: 0.6285 - val_loss: 0.8746 - val_accuracy: 0.6239\n",
            "Epoch 76/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.8462 - accuracy: 0.6316 - val_loss: 0.8630 - val_accuracy: 0.6231\n",
            "Epoch 77/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.8325 - accuracy: 0.6322 - val_loss: 0.8765 - val_accuracy: 0.6224\n",
            "Epoch 78/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.8254 - accuracy: 0.6361 - val_loss: 0.8868 - val_accuracy: 0.6284\n",
            "Epoch 79/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8136 - accuracy: 0.6384 - val_loss: 0.8846 - val_accuracy: 0.6306\n",
            "Epoch 80/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.8088 - accuracy: 0.6403 - val_loss: 0.9035 - val_accuracy: 0.6344\n",
            "Epoch 81/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8059 - accuracy: 0.6399 - val_loss: 0.8717 - val_accuracy: 0.6486\n",
            "Epoch 82/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7963 - accuracy: 0.6426 - val_loss: 0.8769 - val_accuracy: 0.6389\n",
            "Epoch 83/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7888 - accuracy: 0.6441 - val_loss: 0.8956 - val_accuracy: 0.6389\n",
            "Epoch 84/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7817 - accuracy: 0.6463 - val_loss: 0.8887 - val_accuracy: 0.6479\n",
            "Epoch 85/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7824 - accuracy: 0.6470 - val_loss: 0.8818 - val_accuracy: 0.6276\n",
            "Epoch 86/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7815 - accuracy: 0.6442 - val_loss: 0.9201 - val_accuracy: 0.6419\n",
            "Epoch 87/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7805 - accuracy: 0.6515 - val_loss: 0.9268 - val_accuracy: 0.6471\n",
            "Epoch 88/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7975 - accuracy: 0.6509 - val_loss: 0.8478 - val_accuracy: 0.6509\n",
            "Epoch 89/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7781 - accuracy: 0.6488 - val_loss: 0.8987 - val_accuracy: 0.6449\n",
            "Epoch 90/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7725 - accuracy: 0.6547 - val_loss: 0.8810 - val_accuracy: 0.6449\n",
            "Epoch 91/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7660 - accuracy: 0.6560 - val_loss: 0.9253 - val_accuracy: 0.6479\n",
            "Epoch 92/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.8854 - accuracy: 0.6399 - val_loss: 1.5939 - val_accuracy: 0.4880\n",
            "Epoch 93/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 1.1874 - accuracy: 0.3720 - val_loss: 1.1899 - val_accuracy: 0.2770\n",
            "Epoch 94/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 1.1171 - accuracy: 0.3215 - val_loss: 1.0772 - val_accuracy: 0.3626\n",
            "Epoch 95/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 1.0329 - accuracy: 0.4371 - val_loss: 1.0209 - val_accuracy: 0.4527\n",
            "Epoch 96/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.9869 - accuracy: 0.5101 - val_loss: 0.9752 - val_accuracy: 0.5330\n",
            "Epoch 97/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.9543 - accuracy: 0.5857 - val_loss: 0.9514 - val_accuracy: 0.5826\n",
            "Epoch 98/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.9377 - accuracy: 0.5892 - val_loss: 0.9376 - val_accuracy: 0.5841\n",
            "Epoch 99/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.9239 - accuracy: 0.5932 - val_loss: 0.9237 - val_accuracy: 0.5863\n",
            "Epoch 100/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.9087 - accuracy: 0.6020 - val_loss: 0.9093 - val_accuracy: 0.6059\n",
            "Epoch 101/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8921 - accuracy: 0.6199 - val_loss: 0.8954 - val_accuracy: 0.6149\n",
            "Epoch 102/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8758 - accuracy: 0.6229 - val_loss: 0.8849 - val_accuracy: 0.6171\n",
            "Epoch 103/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8656 - accuracy: 0.6271 - val_loss: 0.9065 - val_accuracy: 0.6156\n",
            "Epoch 104/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8605 - accuracy: 0.6284 - val_loss: 0.9124 - val_accuracy: 0.6194\n",
            "Epoch 105/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8555 - accuracy: 0.6337 - val_loss: 0.8927 - val_accuracy: 0.6269\n",
            "Epoch 106/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8496 - accuracy: 0.6313 - val_loss: 0.9047 - val_accuracy: 0.6276\n",
            "Epoch 107/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.8408 - accuracy: 0.6342 - val_loss: 0.9076 - val_accuracy: 0.6299\n",
            "Epoch 108/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8367 - accuracy: 0.6318 - val_loss: 0.8841 - val_accuracy: 0.6081\n",
            "Epoch 109/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.9256 - accuracy: 0.5701 - val_loss: 0.9605 - val_accuracy: 0.5263\n",
            "Epoch 110/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.9266 - accuracy: 0.5738 - val_loss: 0.9283 - val_accuracy: 0.5691\n",
            "Epoch 111/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.9012 - accuracy: 0.6008 - val_loss: 0.9063 - val_accuracy: 0.5848\n",
            "Epoch 112/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.8841 - accuracy: 0.6134 - val_loss: 0.8917 - val_accuracy: 0.6036\n",
            "Epoch 113/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8716 - accuracy: 0.6232 - val_loss: 0.8809 - val_accuracy: 0.6111\n",
            "Epoch 114/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8613 - accuracy: 0.6267 - val_loss: 0.8716 - val_accuracy: 0.6179\n",
            "Epoch 115/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.8529 - accuracy: 0.6319 - val_loss: 0.8644 - val_accuracy: 0.6239\n",
            "Epoch 116/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8441 - accuracy: 0.6343 - val_loss: 0.8664 - val_accuracy: 0.6306\n",
            "Epoch 117/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8370 - accuracy: 0.6369 - val_loss: 0.8612 - val_accuracy: 0.6336\n",
            "Epoch 118/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.8295 - accuracy: 0.6383 - val_loss: 0.8664 - val_accuracy: 0.6329\n",
            "Epoch 119/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.8240 - accuracy: 0.6407 - val_loss: 0.8797 - val_accuracy: 0.6351\n",
            "Epoch 120/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.8190 - accuracy: 0.6417 - val_loss: 0.8858 - val_accuracy: 0.6359\n",
            "Epoch 121/200\n",
            "22/22 [==============================] - 1s 25ms/step - loss: 0.8125 - accuracy: 0.6426 - val_loss: 0.8980 - val_accuracy: 0.6374\n",
            "Epoch 122/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.8095 - accuracy: 0.6440 - val_loss: 0.8946 - val_accuracy: 0.6404\n",
            "Epoch 123/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8062 - accuracy: 0.6435 - val_loss: 0.8933 - val_accuracy: 0.6396\n",
            "Epoch 124/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.8031 - accuracy: 0.6450 - val_loss: 0.8918 - val_accuracy: 0.6404\n",
            "Epoch 125/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.8004 - accuracy: 0.6466 - val_loss: 0.8919 - val_accuracy: 0.6381\n",
            "Epoch 126/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7976 - accuracy: 0.6473 - val_loss: 0.8982 - val_accuracy: 0.6396\n",
            "Epoch 127/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7943 - accuracy: 0.6485 - val_loss: 0.8978 - val_accuracy: 0.6411\n",
            "Epoch 128/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7914 - accuracy: 0.6491 - val_loss: 0.8971 - val_accuracy: 0.6456\n",
            "Epoch 129/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7886 - accuracy: 0.6496 - val_loss: 0.8991 - val_accuracy: 0.6449\n",
            "Epoch 130/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7860 - accuracy: 0.6505 - val_loss: 0.9130 - val_accuracy: 0.6426\n",
            "Epoch 131/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7834 - accuracy: 0.6522 - val_loss: 0.9124 - val_accuracy: 0.6441\n",
            "Epoch 132/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7808 - accuracy: 0.6522 - val_loss: 0.9121 - val_accuracy: 0.6426\n",
            "Epoch 133/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7783 - accuracy: 0.6530 - val_loss: 0.9114 - val_accuracy: 0.6404\n",
            "Epoch 134/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7763 - accuracy: 0.6552 - val_loss: 0.9119 - val_accuracy: 0.6404\n",
            "Epoch 135/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7740 - accuracy: 0.6540 - val_loss: 0.9219 - val_accuracy: 0.6404\n",
            "Epoch 136/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.7716 - accuracy: 0.6559 - val_loss: 0.9226 - val_accuracy: 0.6434\n",
            "Epoch 137/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7703 - accuracy: 0.6571 - val_loss: 0.9218 - val_accuracy: 0.6426\n",
            "Epoch 138/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7665 - accuracy: 0.6571 - val_loss: 0.9363 - val_accuracy: 0.6404\n",
            "Epoch 139/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7641 - accuracy: 0.6593 - val_loss: 0.9328 - val_accuracy: 0.6404\n",
            "Epoch 140/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7617 - accuracy: 0.6594 - val_loss: 0.9368 - val_accuracy: 0.6396\n",
            "Epoch 141/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7619 - accuracy: 0.6618 - val_loss: 0.9430 - val_accuracy: 0.6374\n",
            "Epoch 142/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7573 - accuracy: 0.6623 - val_loss: 0.9613 - val_accuracy: 0.6389\n",
            "Epoch 143/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7547 - accuracy: 0.6636 - val_loss: 0.9811 - val_accuracy: 0.6381\n",
            "Epoch 144/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7526 - accuracy: 0.6663 - val_loss: 1.0107 - val_accuracy: 0.6411\n",
            "Epoch 145/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 0.7476 - accuracy: 0.6662 - val_loss: 1.0078 - val_accuracy: 0.6396\n",
            "Epoch 146/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7434 - accuracy: 0.6685 - val_loss: 1.0301 - val_accuracy: 0.6389\n",
            "Epoch 147/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7875 - accuracy: 0.6492 - val_loss: 0.9650 - val_accuracy: 0.6306\n",
            "Epoch 148/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7839 - accuracy: 0.6514 - val_loss: 0.9552 - val_accuracy: 0.6359\n",
            "Epoch 149/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7700 - accuracy: 0.6621 - val_loss: 0.9925 - val_accuracy: 0.6411\n",
            "Epoch 150/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7597 - accuracy: 0.6632 - val_loss: 1.0435 - val_accuracy: 0.6426\n",
            "Epoch 151/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7533 - accuracy: 0.6657 - val_loss: 1.0893 - val_accuracy: 0.6486\n",
            "Epoch 152/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: nan - accuracy: 0.6674 - val_loss: 1.0929 - val_accuracy: 0.6471\n",
            "Epoch 153/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6681 - val_loss: nan - val_accuracy: 0.6479\n",
            "Epoch 154/200\n",
            "22/22 [==============================] - 1s 30ms/step - loss: nan - accuracy: 0.6701 - val_loss: 1.1327 - val_accuracy: 0.6449\n",
            "Epoch 155/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6712 - val_loss: 1.1246 - val_accuracy: 0.6441\n",
            "Epoch 156/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: nan - accuracy: 0.6735 - val_loss: 1.2036 - val_accuracy: 0.6434\n",
            "Epoch 157/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: nan - accuracy: 0.6751 - val_loss: 1.2166 - val_accuracy: 0.6419\n",
            "Epoch 158/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: nan - accuracy: 0.6778 - val_loss: nan - val_accuracy: 0.6366\n",
            "Epoch 159/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6774 - val_loss: nan - val_accuracy: 0.6381\n",
            "Epoch 160/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6812 - val_loss: nan - val_accuracy: 0.6381\n",
            "Epoch 161/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6823 - val_loss: nan - val_accuracy: 0.6366\n",
            "Epoch 162/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: nan - accuracy: 0.6833 - val_loss: nan - val_accuracy: 0.6404\n",
            "Epoch 163/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6759 - val_loss: nan - val_accuracy: 0.6374\n",
            "Epoch 164/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: nan - accuracy: 0.6792 - val_loss: nan - val_accuracy: 0.6441\n",
            "Epoch 165/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6842 - val_loss: nan - val_accuracy: 0.6494\n",
            "Epoch 166/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: 0.7531 - accuracy: 0.6640 - val_loss: 1.0086 - val_accuracy: 0.6494\n",
            "Epoch 167/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 0.7418 - accuracy: 0.6642 - val_loss: 0.9836 - val_accuracy: 0.6464\n",
            "Epoch 168/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7427 - accuracy: 0.6651 - val_loss: 0.9941 - val_accuracy: 0.6486\n",
            "Epoch 169/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: 0.7269 - accuracy: 0.6737 - val_loss: 1.1345 - val_accuracy: 0.6502\n",
            "Epoch 170/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: nan - accuracy: 0.6775 - val_loss: nan - val_accuracy: 0.6502\n",
            "Epoch 171/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: nan - accuracy: 0.6835 - val_loss: nan - val_accuracy: 0.6449\n",
            "Epoch 172/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6813 - val_loss: nan - val_accuracy: 0.6449\n",
            "Epoch 173/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6841 - val_loss: 1.1072 - val_accuracy: 0.6502\n",
            "Epoch 174/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6846 - val_loss: nan - val_accuracy: 0.6336\n",
            "Epoch 175/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: nan - accuracy: 0.6873 - val_loss: nan - val_accuracy: 0.6517\n",
            "Epoch 176/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: nan - accuracy: 0.6815 - val_loss: 1.1286 - val_accuracy: 0.6344\n",
            "Epoch 177/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: nan - accuracy: 0.6813 - val_loss: 1.1636 - val_accuracy: 0.6434\n",
            "Epoch 178/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6903 - val_loss: nan - val_accuracy: 0.6479\n",
            "Epoch 179/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: nan - accuracy: 0.6892 - val_loss: nan - val_accuracy: 0.6396\n",
            "Epoch 180/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: nan - accuracy: 0.6901 - val_loss: nan - val_accuracy: 0.6456\n",
            "Epoch 181/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: nan - accuracy: 0.6909 - val_loss: nan - val_accuracy: 0.6479\n",
            "Epoch 182/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: nan - accuracy: 0.6944 - val_loss: nan - val_accuracy: 0.6494\n",
            "Epoch 183/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6948 - val_loss: nan - val_accuracy: 0.6456\n",
            "Epoch 184/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6960 - val_loss: nan - val_accuracy: 0.6502\n",
            "Epoch 185/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: nan - accuracy: 0.6963 - val_loss: nan - val_accuracy: 0.6449\n",
            "Epoch 186/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: nan - accuracy: 0.6969 - val_loss: nan - val_accuracy: 0.6486\n",
            "Epoch 187/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: nan - accuracy: 0.6963 - val_loss: nan - val_accuracy: 0.6502\n",
            "Epoch 188/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: nan - accuracy: 0.6987 - val_loss: nan - val_accuracy: 0.6524\n",
            "Epoch 189/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: nan - accuracy: 0.7011 - val_loss: nan - val_accuracy: 0.6509\n",
            "Epoch 190/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: nan - accuracy: 0.6996 - val_loss: nan - val_accuracy: 0.6449\n",
            "Epoch 191/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.7008 - val_loss: nan - val_accuracy: 0.6449\n",
            "Epoch 192/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: nan - accuracy: 0.7037 - val_loss: nan - val_accuracy: 0.6351\n",
            "Epoch 193/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.7020 - val_loss: nan - val_accuracy: 0.6344\n",
            "Epoch 194/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6998 - val_loss: nan - val_accuracy: 0.6509\n",
            "Epoch 195/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: nan - accuracy: 0.7044 - val_loss: nan - val_accuracy: 0.6419\n",
            "Epoch 196/200\n",
            "22/22 [==============================] - 1s 27ms/step - loss: nan - accuracy: 0.6906 - val_loss: nan - val_accuracy: 0.6224\n",
            "Epoch 197/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: nan - accuracy: 0.6894 - val_loss: nan - val_accuracy: 0.6539\n",
            "Epoch 198/200\n",
            "22/22 [==============================] - 1s 26ms/step - loss: nan - accuracy: 0.6979 - val_loss: nan - val_accuracy: 0.6502\n",
            "Epoch 199/200\n",
            "22/22 [==============================] - 1s 28ms/step - loss: nan - accuracy: 0.6995 - val_loss: nan - val_accuracy: 0.6434\n",
            "Epoch 200/200\n",
            "22/22 [==============================] - 1s 29ms/step - loss: nan - accuracy: 0.7021 - val_loss: nan - val_accuracy: 0.6411\n",
            "42/42 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6407\n"
          ]
        }
      ],
      "source": [
        "input_layer_8 = Input(shape=(16,), dtype='float32', name='input_layer_8')\n",
        "\n",
        "# another input layer\n",
        "input_layer_9 = Input(shape=(128,), dtype='float32', name='input_layer_9')\n",
        "\n",
        "# embedding layer\n",
        "embedding_layer_8 = embeddingLayer(input_layer_8)\n",
        "embedding_layer_9 = embeddingLayer(input_layer_9)\n",
        "\n",
        "# cnn layer\n",
        "cnn_layer_8 = Conv1D(32, 3, activation='relu')(embedding_layer_8)\n",
        "cnn_layer_9 = Conv1D(32, 3, activation='relu')(embedding_layer_9)\n",
        "\n",
        "global_average_pooling_layer_8 = GlobalAveragePooling1D()(cnn_layer_8)\n",
        "global_average_pooling_layer_9 = GlobalAveragePooling1D()(cnn_layer_9)\n",
        "\n",
        "# dense layer\n",
        "dense_layer_8 = Dense(16, activation='relu')(global_average_pooling_layer_8)\n",
        "dense_layer_9 = Dense(16, activation='relu')(global_average_pooling_layer_9)\n",
        "\n",
        "concatenated_layer_8_9 = concatenate([dense_layer_8, dense_layer_9])\n",
        "\n",
        "output_layer_8_9 = Dense(3, activation='relu')(concatenated_layer_8_9)\n",
        "\n",
        "model_8_9 = Model(inputs=[input_layer_8, input_layer_9], outputs=output_layer_8_9)\n",
        "\n",
        "model_8_9.summary()\n",
        "\n",
        "model_8_9.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_8_9.fit([x_train_aspect_pad_glove, x_train_review_pad_glove], y_train, epochs=200, batch_size=512, validation_data=([x_dev_aspect_pad_glove, x_dev_review_pad_glove], y_dev))\n",
        "\n",
        "loss, accuracy = model_8_9.evaluate([x_test_aspect_pad_glove, x_test_review_pad_glove], y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_PPzd6R6gt8"
      },
      "source": [
        "#  Model 4: Another LSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IC85cIumM9V"
      },
      "source": [
        "If you study the data carefully, you can find that every aspect appears in the review sentence, which means we can extract the aspect information from the sentence. In most cases, the polarity of the aspect is determined by the content near it. Therefore, an LSTM can transfer the information of adjacent context to the aspect. We only need to extract the aspect vector to calculate its polarity, without analyzing the whole sentence.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "(This is a bit like a simplified version of the \"unknown target\" sequence tagging models discussed in lectures - but we're only trying to tag the one part of the sequence that we know corresponds to the aspect mention.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcxqucKN66sy",
        "outputId": "aafca044-497f-4977-bbb9-aaeef6f22291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "annotation error:\n",
            "['i love the food here, and although it is pricey, the entree comes with rice, naan, dal, and salad, which makes it worthwhile.', 'd al', 'neutral', '24', '28']\n",
            "\n",
            "\n",
            "\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'decor', 'negative', '4', '9']\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'food', 'positive', '42', '46']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'prices', 'positive', '59', '65']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "['when tables opened up, the manager sat another party before us.', 'tables', 'neutral', '5', '11']\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['the scene there are two distinct personalities to the place: the loud, seemingly always-crowded bar with hanging paper decorations and dim lighting, and the two main dining areas, where the noise level and decor is notably more subdued.', 'noise level', 'negative', '190', '201']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "def aspect_mask(reviews, aspects, dataset):\n",
        "  mask = []\n",
        "  for review,aspect,data in zip(reviews, aspects, dataset):\n",
        "    find_aspect = False\n",
        "    for j in range(5):\n",
        "      aspect_num = len(aspect)\n",
        "      aspect_str = \" \".join(aspect)\n",
        "      aspect_len = int(len(aspect_str) - 1) * (j+1)\n",
        "      offset = 0\n",
        "      for i,r in enumerate(review):\n",
        "        if i + aspect_num <= len(review):\n",
        "          r_context = \" \".join(review[i:i+aspect_num])\n",
        "          if r_context == aspect_str and offset + aspect_len >  int(data[3]) and offset + aspect_len <  int(data[4]):\n",
        "            find_aspect = True\n",
        "            sentence_mask = [0] * len(review)\n",
        "            sentence_mask[i:i+aspect_num] = [1] * aspect_num\n",
        "            mask.append(sentence_mask)\n",
        "            break\n",
        "          else:\n",
        "            offset += (len(r) + 1)\n",
        "      if find_aspect:\n",
        "        break\n",
        "\n",
        "    if not find_aspect:\n",
        "      for j in range(5):\n",
        "        aspect_len = int(len(aspect_str) - 1) * (j+1)\n",
        "        offset = 0\n",
        "        for i,r in enumerate(review):\n",
        "          if i + aspect_num <= len(review):\n",
        "            r_context = \" \".join(review[i:i+aspect_num])\n",
        "            if r_context.startswith(aspect_str) and offset + aspect_len >  int(data[3]) and offset + aspect_len <  int(data[4]):\n",
        "              find_aspect = True\n",
        "              sentence_mask = [0] * len(review)\n",
        "              sentence_mask[i:i+aspect_num] = [1] * aspect_num\n",
        "              mask.append(sentence_mask)\n",
        "              break\n",
        "            else:\n",
        "              offset += (len(r) + 1)\n",
        "        if find_aspect:\n",
        "          break\n",
        "\n",
        "    if not find_aspect:\n",
        "      for j in range(5):\n",
        "        aspect_len = int(len(aspect_str) - 1) * (j+1)\n",
        "        offset = 0\n",
        "        for i,r in enumerate(review):\n",
        "          if i + aspect_num <= len(review):\n",
        "            r_context = \" \".join(review[i:i+aspect_num])\n",
        "            if r_context.endswith(aspect_str) and offset + aspect_len >  int(data[3]) and offset + aspect_len <  int(data[4]):\n",
        "              find_aspect = True\n",
        "              sentence_mask = [0] * len(review)\n",
        "              sentence_mask[i:i+aspect_num] = [1] * aspect_num\n",
        "              mask.append(sentence_mask)\n",
        "              break\n",
        "            else:\n",
        "              offset += (len(r) + 1)\n",
        "        if find_aspect:\n",
        "          break\n",
        "\n",
        "    if not find_aspect:\n",
        "      print(\"annotation error:\")\n",
        "      print(data)\n",
        "      sentence_mask = [0] * len(review)\n",
        "      sentence_mask[16] = 1\n",
        "      mask.append(sentence_mask)\n",
        "\n",
        "    # if aspect_num > 1:\n",
        "    #   print(mask[-1])\n",
        "\n",
        "  return mask\n",
        "x_train_aspect_mask = aspect_mask(x_train_review, x_train_aspect, train)\n",
        "x_dev_aspect_mask = aspect_mask(x_dev_review, x_dev_aspect, val)\n",
        "x_test_aspect_mask = aspect_mask(x_test_review, x_test_aspect, test)\n",
        "\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "assert len(x_train_aspect_mask) == len(train)\n",
        "assert len(x_test_aspect_mask) == len(x_test_aspect)\n",
        "\n",
        "print(train[0])\n",
        "print(x_train_aspect_mask[0])\n",
        "print(train[1])\n",
        "print(x_train_aspect_mask[1])\n",
        "print(train[2])\n",
        "print(x_train_aspect_mask[2])\n",
        "print(train[3])\n",
        "print(x_train_aspect_mask[3])\n",
        "print(train[10319])\n",
        "print(x_train_aspect_mask[10319])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJK16El2yBQ0",
        "outputId": "e097b4ac-aa56-431c-de6f-f6f571c623ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "x_train_aspect_mask_pad = keras.preprocessing.sequence.pad_sequences(x_train_aspect_mask,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=128)\n",
        "x_dev_aspect_mask_pad = keras.preprocessing.sequence.pad_sequences(x_dev_aspect_mask,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=128)\n",
        "x_test_aspect_mask_pad = keras.preprocessing.sequence.pad_sequences(x_test_aspect_mask,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=128)\n",
        "x_train_aspect_mask_pad[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahUWMflW65jM",
        "outputId": "3026f8dd-d48e-4dca-aa54-61a7f768d4b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " GloVe_Embeddings (Embedding)   multiple             120000300   ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 128, 200)     320800      ['GloVe_Embeddings[7][0]']       \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " dot (Dot)                      (None, 200)          0           ['bidirectional[0][0]',          \n",
            "                                                                  'input_8[0][0]']                \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 16)           3216        ['dot[0][0]']                    \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 3)            51          ['dense_18[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 120,324,367\n",
            "Trainable params: 324,067\n",
            "Non-trainable params: 120,000,300\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Bidirectional, Dot\n",
        "\n",
        "input_layer_10 = Input(shape=(128,), dtype='float32')\n",
        "\n",
        "# embedding layer\n",
        "embedding_layer_10 = embeddingLayer(input_layer_10)\n",
        "\n",
        "# bi-lstm layer\n",
        "bi_lstm_layer_10 = Bidirectional(LSTM(100, return_sequences=True))(embedding_layer_10)\n",
        "\n",
        "# input layer\n",
        "input_layer_11 = Input(shape=(128,), dtype='float32')\n",
        "\n",
        "# dot layer\n",
        "dot_layer_11 = Dot(axes=1)([bi_lstm_layer_10, input_layer_11])\n",
        "\n",
        "# dense layer\n",
        "hidden_layer_11 = Dense(16, activation='relu')(dot_layer_11)\n",
        "\n",
        "#output layer\n",
        "output_layer_11 = Dense(3, activation='softmax')(hidden_layer_11)\n",
        "\n",
        "model_10_11 = Model([input_layer_10, input_layer_11], outputs=output_layer_11)\n",
        "\n",
        "model_10_11.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "f90SrYb4YHj3",
        "outputId": "7fcc0b19-9bd9-4996-8a6e-d5b77a53b436"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"392pt\" viewBox=\"0.00 0.00 786.00 470.00\" width=\"655pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 466)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-466 782,-466 782,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140387379390032 -->\n<g class=\"node\" id=\"node1\">\n<title>140387379390032</title>\n<polygon fill=\"none\" points=\"50,-415.5 50,-461.5 380,-461.5 380,-415.5 50,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-446.3\">input_7</text>\n<polyline fill=\"none\" points=\"50,-438.5 130,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-423.3\">InputLayer</text>\n<polyline fill=\"none\" points=\"130,-415.5 130,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"130,-438.5 188,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"188,-415.5 188,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236\" y=\"-434.8\">[(None, 128)]</text>\n<polyline fill=\"none\" points=\"284,-415.5 284,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"332\" y=\"-434.8\">[(None, 128)]</text>\n</g>\n<!-- 140383273972816 -->\n<g class=\"node\" id=\"node2\">\n<title>140383273972816</title>\n<polygon fill=\"none\" points=\"96,-332.5 96,-378.5 334,-378.5 334,-332.5 96,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-363.3\">GloVe_Embeddings</text>\n<polyline fill=\"none\" points=\"96,-355.5 230,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-340.3\">Embedding</text>\n<polyline fill=\"none\" points=\"230,-332.5 230,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"230,-355.5 288,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"288,-332.5 288,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"299.5\" y=\"-351.8\">?</text>\n<polyline fill=\"none\" points=\"311,-332.5 311,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-351.8\">?</text>\n</g>\n<!-- 140387379390032&#45;&gt;140383273972816 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140387379390032-&gt;140383273972816</title>\n<path d=\"M215,-415.3799C215,-407.1745 215,-397.7679 215,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"218.5001,-388.784 215,-378.784 211.5001,-388.784 218.5001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140387382002384 -->\n<g class=\"node\" id=\"node3\">\n<title>140387382002384</title>\n<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 430,-295.5 430,-249.5 0,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"69\" y=\"-280.3\">bidirectional(lstm_2)</text>\n<polyline fill=\"none\" points=\"0,-272.5 138,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"69\" y=\"-257.3\">Bidirectional(LSTM)</text>\n<polyline fill=\"none\" points=\"138,-249.5 138,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"138,-272.5 196,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"196,-249.5 196,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-268.8\">(None, 128, 300)</text>\n<polyline fill=\"none\" points=\"313,-249.5 313,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"371.5\" y=\"-268.8\">(None, 128, 200)</text>\n</g>\n<!-- 140383273972816&#45;&gt;140387382002384 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140383273972816-&gt;140387382002384</title>\n<path d=\"M215,-332.3799C215,-324.1745 215,-314.7679 215,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"218.5001,-305.784 215,-295.784 211.5001,-305.784 218.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140380838039952 -->\n<g class=\"node\" id=\"node5\">\n<title>140380838039952</title>\n<polygon fill=\"none\" points=\"220.5,-166.5 220.5,-212.5 607.5,-212.5 607.5,-166.5 220.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-197.3\">dot</text>\n<polyline fill=\"none\" points=\"220.5,-189.5 258.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-174.3\">Dot</text>\n<polyline fill=\"none\" points=\"258.5,-166.5 258.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"287.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"258.5,-189.5 316.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"287.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"316.5,-166.5 316.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418.5\" y=\"-185.8\">[(None, 128, 200), (None, 128)]</text>\n<polyline fill=\"none\" points=\"520.5,-166.5 520.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"564\" y=\"-185.8\">(None, 200)</text>\n</g>\n<!-- 140387382002384&#45;&gt;140380838039952 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140387382002384-&gt;140380838039952</title>\n<path d=\"M270.1683,-249.4901C294.755,-239.2353 323.8812,-227.0872 349.4713,-216.414\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"350.8423,-219.6345 358.7244,-212.5547 348.1476,-213.1739 350.8423,-219.6345\" stroke=\"#000000\"/>\n</g>\n<!-- 140387379390288 -->\n<g class=\"node\" id=\"node4\">\n<title>140387379390288</title>\n<polygon fill=\"none\" points=\"448,-249.5 448,-295.5 778,-295.5 778,-249.5 448,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"488\" y=\"-280.3\">input_8</text>\n<polyline fill=\"none\" points=\"448,-272.5 528,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"488\" y=\"-257.3\">InputLayer</text>\n<polyline fill=\"none\" points=\"528,-249.5 528,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"528,-272.5 586,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"586,-249.5 586,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"634\" y=\"-268.8\">[(None, 128)]</text>\n<polyline fill=\"none\" points=\"682,-249.5 682,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"730\" y=\"-268.8\">[(None, 128)]</text>\n</g>\n<!-- 140387379390288&#45;&gt;140380838039952 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140387379390288-&gt;140380838039952</title>\n<path d=\"M557.8317,-249.4901C533.245,-239.2353 504.1188,-227.0872 478.5287,-216.414\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"479.8524,-213.1739 469.2756,-212.5547 477.1577,-219.6345 479.8524,-213.1739\" stroke=\"#000000\"/>\n</g>\n<!-- 140387381255120 -->\n<g class=\"node\" id=\"node6\">\n<title>140387381255120</title>\n<polygon fill=\"none\" points=\"266,-83.5 266,-129.5 562,-129.5 562,-83.5 266,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-114.3\">dense_18</text>\n<polyline fill=\"none\" points=\"266,-106.5 337,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-91.3\">Dense</text>\n<polyline fill=\"none\" points=\"337,-83.5 337,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"337,-106.5 395,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"395,-83.5 395,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"438.5\" y=\"-102.8\">(None, 200)</text>\n<polyline fill=\"none\" points=\"482,-83.5 482,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"522\" y=\"-102.8\">(None, 16)</text>\n</g>\n<!-- 140380838039952&#45;&gt;140387381255120 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140380838039952-&gt;140387381255120</title>\n<path d=\"M414,-166.3799C414,-158.1745 414,-148.7679 414,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"417.5001,-139.784 414,-129.784 410.5001,-139.784 417.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140387378864272 -->\n<g class=\"node\" id=\"node7\">\n<title>140387378864272</title>\n<polygon fill=\"none\" points=\"273.5,-.5 273.5,-46.5 554.5,-46.5 554.5,-.5 273.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-31.3\">dense_19</text>\n<polyline fill=\"none\" points=\"273.5,-23.5 344.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-8.3\">Dense</text>\n<polyline fill=\"none\" points=\"344.5,-.5 344.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"344.5,-23.5 402.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"402.5,-.5 402.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"442.5\" y=\"-19.8\">(None, 16)</text>\n<polyline fill=\"none\" points=\"482.5,-.5 482.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"518.5\" y=\"-19.8\">(None, 3)</text>\n</g>\n<!-- 140387381255120&#45;&gt;140387378864272 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140387381255120-&gt;140387378864272</title>\n<path d=\"M414,-83.3799C414,-75.1745 414,-65.7679 414,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"417.5001,-56.784 414,-46.784 410.5001,-56.784 417.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils import vis_utils\n",
        "SVG(vis_utils.model_to_dot(model_10_11, show_shapes=True, show_layer_names=True, dpi=60).create(prog='dot', format='svg'))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF_x9s6mYbkv",
        "outputId": "6589c751-0e23-4fd2-ceac-8d637668af41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/16\n",
            "22/22 [==============================] - 8s 200ms/step - loss: 33894.8086 - accuracy: 0.4046 - val_loss: 1.0969 - val_accuracy: 0.4535\n",
            "Epoch 2/16\n",
            "22/22 [==============================] - 3s 148ms/step - loss: 1.2062 - accuracy: 0.4507 - val_loss: 1.0946 - val_accuracy: 0.4535\n",
            "Epoch 3/16\n",
            "22/22 [==============================] - 3s 148ms/step - loss: 1.0936 - accuracy: 0.4507 - val_loss: 1.0922 - val_accuracy: 0.4535\n",
            "Epoch 4/16\n",
            "22/22 [==============================] - 3s 148ms/step - loss: 1.0912 - accuracy: 0.4507 - val_loss: 1.0897 - val_accuracy: 0.4535\n",
            "Epoch 5/16\n",
            "22/22 [==============================] - 3s 148ms/step - loss: 1.0888 - accuracy: 0.4507 - val_loss: 1.0872 - val_accuracy: 0.4535\n",
            "Epoch 6/16\n",
            "22/22 [==============================] - 3s 150ms/step - loss: 1.0864 - accuracy: 0.4507 - val_loss: 1.0848 - val_accuracy: 0.4535\n",
            "Epoch 7/16\n",
            "22/22 [==============================] - 3s 148ms/step - loss: 1.0842 - accuracy: 0.4507 - val_loss: 1.0826 - val_accuracy: 0.4535\n",
            "Epoch 8/16\n",
            "22/22 [==============================] - 3s 148ms/step - loss: 1.0821 - accuracy: 0.4507 - val_loss: 1.0805 - val_accuracy: 0.4535\n",
            "Epoch 9/16\n",
            "22/22 [==============================] - 3s 150ms/step - loss: 1.0801 - accuracy: 0.4507 - val_loss: 1.0786 - val_accuracy: 0.4535\n",
            "Epoch 10/16\n",
            "22/22 [==============================] - 3s 148ms/step - loss: 1.0784 - accuracy: 0.4507 - val_loss: 1.0768 - val_accuracy: 0.4535\n",
            "Epoch 11/16\n",
            "22/22 [==============================] - 3s 150ms/step - loss: 1.0768 - accuracy: 0.4507 - val_loss: 1.0752 - val_accuracy: 0.4535\n",
            "Epoch 12/16\n",
            "22/22 [==============================] - 3s 149ms/step - loss: 1.0753 - accuracy: 0.4507 - val_loss: 1.0738 - val_accuracy: 0.4535\n",
            "Epoch 13/16\n",
            "22/22 [==============================] - 3s 149ms/step - loss: 1.0740 - accuracy: 0.4507 - val_loss: 1.0725 - val_accuracy: 0.4535\n",
            "Epoch 14/16\n",
            "22/22 [==============================] - 3s 149ms/step - loss: 1.0729 - accuracy: 0.4507 - val_loss: 1.0714 - val_accuracy: 0.4535\n",
            "Epoch 15/16\n",
            "22/22 [==============================] - 3s 151ms/step - loss: 1.0719 - accuracy: 0.4507 - val_loss: 1.0704 - val_accuracy: 0.4535\n",
            "Epoch 16/16\n",
            "22/22 [==============================] - 3s 152ms/step - loss: 1.0710 - accuracy: 0.4507 - val_loss: 1.0695 - val_accuracy: 0.4535\n",
            "42/42 [==============================] - 1s 26ms/step - loss: 1.0695 - accuracy: 0.4543\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "\n",
        "model_10_11.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model_10_11.fit([x_train_aspect_mask_pad, x_train_review_pad_glove], y_train, epochs=16, batch_size=512, validation_data=([x_dev_aspect_mask_pad, x_dev_review_pad_glove], y_dev))\n",
        "\n",
        "loss, accuracy = model_10_11.evaluate([x_test_aspect_mask_pad, x_test_review_pad_glove], y_test)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "7001_2021_22_lab4_Aspect-Based-Sentiment Analysis_without_answers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}