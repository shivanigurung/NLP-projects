{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWgujXmGqzIC"
      },
      "source": [
        "# Lab 8 - Aspect-Based Sentiment Analysis with BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr5PWYKGPi6R"
      },
      "source": [
        "In this lab we turn a pre-trained BERT model into a trainable Keras layer and apply it to the Aspect-Based Sentiment Analysis that we tackled in lab 4. BERT (Bidirectional Embedding Representations from Transformers) is a new model for pre-training language representations that obtains state-of-the-art results on many NLP tasks. We demonstrate how to integrate BERT as a custom Keras layer to simplify model prototyping using huggingface. In this lab, you will learn: \n",
        "\n",
        "1) How to use the huggingface package.\n",
        "\n",
        "2) How to integrate BERT in our previous model. \n",
        "\n",
        "3) How to use the TPU from Colab. (Note: Running BERT on the CPU would be very slow. Thus we recommend you to do this lab on Colab based on TPU provided by Google.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4spJ5PRhGJ1l"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers import LSTM, RNN, Dropout, Input, LeakyReLU, Bidirectional,Conv1D, GlobalMaxPooling1D\n",
        "from keras.layers.core import Dense\n",
        "from keras.models import Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oimYsLssuSs"
      },
      "source": [
        "Before start, we should install the huggingface transformer package. You can find the doc from its [website](https://huggingface.co/transformers/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AElpzsKFiZSo",
        "outputId": "a9d1ead9-7c1c-480d-b3df-5812b5bb634f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 33.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.50.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 45.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Collecting click==8.0\n",
            "  Downloading click-8.0.0-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.50-py3-none-any.whl size=895166 sha256=dae870bd6808c7c944bf330bb7f0d2478bed4ac4627394329827abb9eeabc9aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/72/54/519f0d5143cc6c73fa3297509123c86fc8586a7fdea8d25311\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: pyyaml, click, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.0.0 which is incompatible.\u001b[0m\n",
            "Successfully installed click-8.0.0 huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.50 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebbamBD0xu5z"
      },
      "source": [
        "## Preprocessing and Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbAMXQwqyVT8"
      },
      "source": [
        "In this lab we will use DistilBERT instead of BERT: DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, and runs 60% faster, while preserving over 95% of BERT’s performance as measured on the GLUE language understanding benchmark.\n",
        "\n",
        "It is easy to switch between DistilBERT and BERT using the huggingface transformer package. This huggingface package provides many pre-trained and pre-built models that are easy to use via a few lines of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-cUTxt02dQb"
      },
      "source": [
        "Before using DistilBERT or BERT, we need a tokenizer. Generally speaking, every BERT related model has its own tokenizer, trained for that model (see this week's lecture video on sub-word tokenization). \n",
        "We can get the DistilBERT tokenizer from **DistilBertTokenizer.from_pretrained** function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "f2b9c0e7eb8c4ddc97f8bc05794cdc5a",
            "9620819eedb449bb9d1b19e9377a4cb1",
            "87e79a20f7a64c0ab1712bf983bfa1bc",
            "c217b2ba58724052811bf12cc6e54412",
            "2b695618bfbc48ea94a9c63531d47fea",
            "d1161a38da5146c4a4ffc18f88167a1b",
            "09ba5008ad144bf8a43b97d5cd9bbe48",
            "0270154b104e404fbe8eab82dfa74ebf",
            "587ee74d28f04f0998c8e39b517ba396",
            "4a460dbf1f9e4ff396766ad478e1690e",
            "f3137c066acc4bbc8f3093301b4be703",
            "78b443ca8d0045f7a4ddd043770a73fa",
            "175ab4506ac9473ead3c1a6464b01f23",
            "dac14d6e91684aaaaf8ed1399419d13f",
            "0bb6fc70d0ee47d880cdce04022a3f9d",
            "732f0ea7b13d4d478f76cd50675a5929",
            "ddca485695144650954d48523c39702a",
            "7aabddaab4524ce782f6d5750582c73a",
            "42b5d6ff4bb945a5a5de723e97fb4d6e",
            "f6f3bd3ed7614967a2a7f66e4f8ecc32",
            "1a6e54cd0702442c9be0b32c6ebc699a",
            "2cf3ceabbfd74e5da5a4acb491fa8538",
            "ddaf33ee953440f1ac2da925025db458",
            "8764bc72fef7449998cc149264621f98",
            "b43050c8141544629a4ebd9f1426f12f",
            "0e132ea8fb47420197449dbdedc1df8c",
            "9a50241b2cb74f5194b4dee774cf8268",
            "35fdf42bf34a479e80284c5e0b1ac5bc",
            "99c2041603604408b382c62c1200f185",
            "4aab8d0e2ca141ad8e8ab90891869c87",
            "aa282d0732ca4dbe9cbf2a0f5471cb61",
            "ce7bf7c42052458f9be7cfec5ebb50f2",
            "ae0c768e14144137937a7b5838da632e"
          ]
        },
        "id": "hKj6Y_TydjeS",
        "outputId": "818eba79-a55d-49e2-db55-b7c171f9c649"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2b9c0e7eb8c4ddc97f8bc05794cdc5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78b443ca8d0045f7a4ddd043770a73fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddaf33ee953440f1ac2da925025db458"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import DistilBertTokenizer, RobertaTokenizer \n",
        "import tqdm\n",
        "distil_bert = 'distilbert-base-uncased' # Pick any desired pre-trained model\n",
        "\n",
        "# Defining DistilBERT tokonizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True,\n",
        "                                                max_length=128, pad_to_max_length=True)\n",
        "\n",
        "def tokenize(sentences, tokenizer, pad_length=128, pad_to_max_length=True ):\n",
        "    if type(sentences) == str:\n",
        "        inputs = tokenizer.encode_plus(sentences, add_special_tokens=True, max_length=pad_length, pad_to_max_length=pad_to_max_length, \n",
        "                                             return_attention_mask=True, return_token_type_ids=True)\n",
        "        return np.asarray(inputs['input_ids'], dtype='int32'), np.asarray(inputs['attention_mask'], dtype='int32'), np.asarray(inputs['token_type_ids'], dtype='int32')\n",
        "    input_ids, input_masks, input_segments = [],[],[]\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=pad_length, pad_to_max_length=pad_to_max_length, \n",
        "                                             return_attention_mask=True, return_token_type_ids=True)\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        input_masks.append(inputs['attention_mask'])\n",
        "        input_segments.append(inputs['token_type_ids'])        \n",
        "        \n",
        "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqMo6CPS3qJZ"
      },
      "source": [
        "Then we can use the tokenizer to tokenize the sentence. When working with word2vec and GloVe, we tokenized the sentence into words ourselves and then converted the tokens to GloVe word indices. But in BERT, we must use the BERT tokenizer: the tokens for BERT are different, and include whole words and sub-word tokens (see lecture video on sub-word tokenisation).\n",
        "\n",
        "For example, for the sentence: **This is a pretrained model.** our previous word-based tokenizer will generate the following tokens:\n",
        "\n",
        "**\"this\", \"is\", \"a\", \"pretrained\", \"model\", \".\"**\n",
        "\n",
        "Then you will find out that the word token \"pretrained\" is not in the GloVe word dictionary. Thus we can not assign a proper word vector for \"pretrained\".\n",
        "\n",
        "In BERT, the BERT tokenizer will separate the word \"pretrained\" into three sub-word tokens:\n",
        "\n",
        "**'pre', '##train', '##ed'**\n",
        "\n",
        "This way, BERT can use these three token vectors to represent the word \"pretrained\". Without the BERT tokenizer, it is hard to separate these unknown words properly.\n",
        "\n",
        "You will also see that the BERT tokenizer adds the special sentence [CLS] token and sentence separator [SEP] tokens (see this week's lecture videos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRoKe2DKyi41",
        "outputId": "112f91b2-42cc-4bf0-9b07-412560b047e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'capital', 'of', 'france', 'is', '[MASK]', '.'] \n",
            "\n",
            "['this', 'is', 'a', 'pre', '##train', '##ed', 'model', '.'] \n",
            "\n",
            "[ 101 1996 3007 1997 2605 2003  103 1012  102    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "['[CLS]', 'the', 'capital', 'of', 'france', 'is', '[MASK]', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] \n",
            "\n",
            "[ 101 1996 3007 1997 2605 2003  103 1012  102]\n",
            "[1 1 1 1 1 1 1 1 1]\n",
            "['[CLS]', 'the', 'capital', 'of', 'france', 'is', '[MASK]', '.', '[SEP]'] \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer.tokenize(\"The capital of France is [MASK].\")\n",
        "print(inputs,'\\n')\n",
        "\n",
        "inputs = tokenizer.tokenize(\"This is a pretrained model.\")\n",
        "print(inputs,'\\n')\n",
        "\n",
        "ids,masks,segments = tokenize(\"The capital of France is [MASK].\", tokenizer)\n",
        "print(ids)\n",
        "print(masks)\n",
        "print(tokenizer.convert_ids_to_tokens(ids),\"\\n\")\n",
        "\n",
        "ids,masks,segments = tokenize(\"The capital of France is [MASK].\", tokenizer, pad_to_max_length=False)\n",
        "print(ids)\n",
        "print(masks)\n",
        "print(tokenizer.convert_ids_to_tokens(ids),\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6OuZAA8sbdg"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqvPQvgvPv1W"
      },
      "source": [
        "### Downloading and preprocessing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EundMtGPpCdf"
      },
      "source": [
        "Similar to lab 4, we need to download and preprocess the data first. The data download code is consistent with lab 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyuSzkafqNca",
        "outputId": "e055cbb3-160b-42b5-b260-f313778c553e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training entries: 11186\n",
            "Test entries: 1336\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "def downloadfile(url):\n",
        "  rq = requests.get(url)\n",
        "  open(url.split('/')[-1], 'wb').write(rq.content)\n",
        "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/train.xml')\n",
        "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/val.xml')\n",
        "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/test.xml')\n",
        "\n",
        "\n",
        "# The code is modified from https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data_process/utils.py\n",
        "from xml.etree.ElementTree import parse\n",
        "\n",
        "def parse_sentence_term(path, lowercase=False):\n",
        "    tree = parse(path)\n",
        "    sentences = tree.getroot()\n",
        "    data = []\n",
        "    split_char = '__split__'\n",
        "    for sentence in sentences:\n",
        "        text = sentence.find('text')\n",
        "        if text is None:\n",
        "            continue\n",
        "        text = text.text\n",
        "        if lowercase:\n",
        "            text = text.lower()\n",
        "        aspectTerms = sentence.find('aspectTerms')\n",
        "        if aspectTerms is None:\n",
        "            continue\n",
        "        for aspectTerm in aspectTerms:\n",
        "            term = aspectTerm.get('term')\n",
        "            if lowercase:\n",
        "                term = term.lower()\n",
        "            polarity = aspectTerm.get('polarity')\n",
        "            start = aspectTerm.get('from')\n",
        "            end = aspectTerm.get('to')\n",
        "            piece = [text , term,  polarity , start , end]\n",
        "            data.append(piece)\n",
        "    return data\n",
        "train = parse_sentence_term(\"train.xml\",True)\n",
        "dev = parse_sentence_term(\"val.xml\",True)\n",
        "test = parse_sentence_term(\"test.xml\",True)\n",
        "\n",
        "print(\"Training entries: {}\".format(len(train)))\n",
        "print(\"Test entries: {}\".format(len(test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U4iCV9-rmay"
      },
      "source": [
        "We now can start playing around with the data, let’s first see some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-gjWRAuqg5s",
        "outputId": "42840520-2675-4e8e-d72d-8075ceab0348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SENTENCE \t ASPECT \t LABLE \t ASPECT-START-INDEX \t ASPECT-END-INDEX\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'decor', 'negative', '4', '9']\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'food', 'positive', '42', '46']\n",
            "['the decor is not special at all but their food and amazing prices make up for it.', 'prices', 'positive', '59', '65']\n",
            "['when tables opened up, the manager sat another party before us.', 'tables', 'neutral', '5', '11']\n",
            "['when tables opened up, the manager sat another party before us.', 'manager', 'negative', '27', '34']\n"
          ]
        }
      ],
      "source": [
        "print(\"SENTENCE \\t ASPECT \\t LABLE \\t ASPECT-START-INDEX \\t ASPECT-END-INDEX\")\n",
        "print(train[0])\n",
        "print(train[1])\n",
        "print(train[2])\n",
        "print(train[3])\n",
        "print(train[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvuu4KhStqei"
      },
      "source": [
        "According to the BERT tokenize function above, we can convert the tweet text and topic words to integers:\n",
        "\n",
        "(Note: the BERT tokenize function is different from lab 4.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMCH1OoDrSNR",
        "outputId": "6ed646ba-0529-467f-9571-4964c61ba5a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_dev_aspect_int[0]:\n",
            "[ 101 8974  102    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "x_dev_aspect_masks[0]:\n",
            "[1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "x_dev_review_int[0]:\n",
            "[  101  2044  1037  3232  1997  8974  1010  1996 18726  1011  1011  1045\n",
            "  2066  1996 27940  1013 24792  2621  4897  1998  1996 13675 11514  6508\n",
            " 26852  1011  1011  2175  2091  2307  1012   102     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "x_dev_review_masks[0]:\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# Please write your code to generate the following data\n",
        "\n",
        "#Define lists for savinf train,dev,test reviews, aspects and their int values\n",
        "\n",
        "x_train_review_int =[]\n",
        "x_train_review_masks =[]\n",
        "x_train_aspect_int =[]\n",
        "x_train_aspect_masks =[]\n",
        "\n",
        "x_dev_review_int =[]\n",
        "x_dev_review_masks =[]\n",
        "x_dev_aspect_int =[]\n",
        "x_dev_aspect_masks =[]\n",
        "\n",
        "x_test_review_int =[]\n",
        "x_test_review_masks =[]\n",
        "x_test_aspect_int =[]\n",
        "x_test_aspect_masks =[]\n",
        "\n",
        "# Function to create train dev and test dataset\n",
        "def create_train_dev_test(dataset):\n",
        "    \n",
        "    review_ids_list,review_masks_list,aspect_ids_list,aspect_masks_list = [],[],[],[] # temp lists to store review ,asptect and their int values\n",
        "\n",
        "    for example in dataset:\n",
        "        review_ids, review_masks, segments1 = tokenize(example[0], tokenizer) # Tokenize the review using tokenize function defined above\n",
        "        aspect_ids, aspect_masks, segments2 = tokenize(example[1], tokenizer) #Tekonize the aspect\n",
        "    \n",
        "        #Append tokenized review, aspect and their ids to temp lists\n",
        "        review_ids_list.append(review_ids)\n",
        "        review_masks_list.append(review_masks)\n",
        "        aspect_ids_list.append(aspect_ids)\n",
        "        aspect_masks_list.append(aspect_masks)\n",
        "\n",
        "    return review_ids_list, review_masks_list, aspect_ids_list, aspect_masks_list\n",
        "\n",
        "#for Train Dataset\n",
        "x_train_review_int, x_train_review_masks, x_train_aspect_int, x_train_aspect_masks = create_train_dev_test(train)\n",
        "\n",
        "#For Dev Dataset\n",
        "x_dev_review_int, x_dev_review_masks, x_dev_aspect_int, x_dev_aspect_masks = create_train_dev_test(dev)\n",
        "\n",
        "#for test Dataset\n",
        "x_test_review_int, x_test_review_masks, x_test_aspect_int, x_test_aspect_masks = create_train_dev_test(test)\n",
        "#######################TODO: REMOVE FOLLOWING CODE####################\n",
        "\n",
        "#######################TODO: REMOVE ABOVE CODE####################\n",
        "\n",
        "# If use the previous tokenize function, you can get a print result like:\n",
        "assert len(x_train_aspect_int) == len(train)\n",
        "assert len(x_train_aspect_masks) == len(x_train_aspect_int)\n",
        "assert len(x_test_aspect_int) == len(test)\n",
        "assert len(x_test_aspect_masks) == len(x_test_aspect_int)\n",
        "print(\"x_dev_aspect_int[0]:\")\n",
        "print(x_dev_aspect_int[0])\n",
        "print(\"x_dev_aspect_masks[0]:\")\n",
        "print(x_dev_aspect_masks[0])\n",
        "print(\"x_dev_review_int[0]:\")\n",
        "print(x_dev_review_int[0])\n",
        "print(\"x_dev_review_masks[0]:\")\n",
        "print(x_dev_review_masks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IreFXgruZot"
      },
      "source": [
        "We one-hot encode the labels, using 4 (Binary:100) to represent \"positive\", 2 (Binary:010) for \"neutral\", and 1 (Binary:001) for \"negative\". Then we can convert the labels to numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abIb7Fe5u3GQ",
        "outputId": "177684d3-9657-4d51-a76e-4de5e8583ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1]\n",
            "[1 0 0]\n",
            "[1 0 0]\n",
            "[0 1 0]\n",
            "[0 0 1]\n"
          ]
        }
      ],
      "source": [
        "def label2int(dataset):\n",
        "  y = []\n",
        "  for example in dataset:\n",
        "    if example[2].lower() == \"negative\":\n",
        "      y.append([0,0,1])\n",
        "    elif example[2].lower() == \"neutral\":\n",
        "      y.append([0,1,0])\n",
        "    else:\n",
        "      # assert example[2].lower() == \"positive\"\n",
        "      y.append([1,0,0])\n",
        "  return y\n",
        "  \n",
        "y_train = label2int(train)\n",
        "y_dev = label2int(dev)\n",
        "y_test = label2int(test)\n",
        "y_train = np.array(y_train)\n",
        "y_dev = np.array(y_dev)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train[1])\n",
        "print(y_train[2])\n",
        "print(y_train[3])\n",
        "print(y_train[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnnSuspvC5b"
      },
      "source": [
        "Now we have almost done the data preprocessing. Unlike the previous lab1-lab4, there are two x (review and aspect) to input the model. The easiest way is to combine the review and aspect into one sentence and then input it into the model. Thus we can use the previous model directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKOiVVXQu-_I",
        "outputId": "2dda5f95-39ed-418e-d17a-450898eb8ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  101  2044  1037  3232  1997  8974  1010  1996 18726  1011  1011  1045\n",
            "  2066  1996 27940  1013 24792  2621  4897  1998  1996 13675 11514  6508\n",
            " 26852  1011  1011  2175  2091  2307  1012  1026 19802  1028  8974   102\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] \n",
            "\n",
            "[  101  2044  1037  3232  1997  8974  1010  1996 18726  1011  1011  1045\n",
            "  2066  1996 27940  1013 24792  2621  4897  1998  1996 13675 11514  6508\n",
            " 26852  1011  1011  2175  2091  2307  1012  1026 19802  1028  8974   102\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# Please write your code to combine the tweet and topic into the following varibles\n",
        "# x_train_int\n",
        "# x_train_masks\n",
        "# x_dev_int\n",
        "# x_dev_masks\n",
        "# x_test_int\n",
        "# x_test_masks\n",
        "\n",
        "# Tips: \n",
        "# 1) We can use the special token <SEP> to concatenate the tweets and topics.\n",
        "# 2) After combine them, make sure they are paded.\n",
        "\n",
        "#Combine the int values of reviews and masks for train,dev and test dataset\n",
        "def combine_review_aspect(dataset):\n",
        "    \n",
        "    input_int,input_masks=[],[] # temp list to store int and masks values\n",
        "    \n",
        "    for example in dataset:\n",
        "        \n",
        "        # Tokenize the sentence and aspect and concatenate using \"<SEP>\" token\n",
        "        temp1, temp2, temp3 = tokenize(example[0]+\"<SEP>\"+example[1], tokenizer) \n",
        "        input_int.append(temp1) #Append the combined int values\n",
        "        input_masks.append(temp2) #Append the combined masks values\n",
        "        \n",
        "    return  input_int, input_masks\n",
        "\n",
        "#For Train DAtaset\n",
        "x_train_int, x_train_masks = combine_review_aspect(train)\n",
        "\n",
        "#For Dev Dataset\n",
        "x_dev_int, x_dev_masks = combine_review_aspect(dev)\n",
        "\n",
        "#For test Dataset\n",
        "x_test_int, x_test_masks = combine_review_aspect(test)\n",
        "#######################TODO: REMOVE FOLLOWING CODE####################\n",
        "\n",
        "\n",
        "#######################TODO: REMOVE ABOVE CODE####################\n",
        "\n",
        "# Don't forget the to use np.array function to wrap the ouput of pad_sequences function\n",
        "x_train_int_np = np.array(x_train_int)\n",
        "x_train_masks_np = np.array(x_train_masks)\n",
        "x_dev_int_np = np.array(x_dev_int)\n",
        "x_dev_masks_np = np.array(x_dev_masks)\n",
        "x_test_int_np = np.array(x_test_int)\n",
        "x_test_masks_np = np.array(x_test_masks)\n",
        "\n",
        "\n",
        "print(x_dev_int[0])\n",
        "print(x_dev_masks[0],'\\n')\n",
        "print(x_dev_int_np[0])\n",
        "print(x_dev_masks_np[0]) # senetnce + aspect\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqvUGIwwGJqu"
      },
      "source": [
        "## Model 1: Prebuilt Sequence Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSxC41ln07im"
      },
      "source": [
        "The huggingface transformer package provides many prebuilt models. Now let us try a sequence classification model based on distillBERT. \n",
        "\n",
        "The models with BERT are much bigger than our previous models. To run it faster, we can use TPU here. The detailed guideline about using TPU can be found from https://www.tensorflow.org/guide/tpu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867,
          "referenced_widgets": [
            "c255a326d5084e82a7428cbb76e4d832",
            "71f21906de884cb29def8592fa4ea89e",
            "fec7e66392544b81862ce9adaea636e5",
            "69b73b55a35c4df285022f7b26e6d235",
            "db2ebaa6b51045fd8be6d4d897540afc",
            "58f460acdcb54608923a36f950c0ce04",
            "8bc5a7c67f774a8ebee847ad9ef0c7b9",
            "5ef3cf5b2aa94c119e0501e1fb29b3d7",
            "8de9886e5cac49e8a6da6736161b5e59",
            "5e2dca9750e747d782da204c689615ae",
            "3d041122564d48b69d2654d43ed5c070"
          ]
        },
        "id": "1gXFbb2cxBlw",
        "outputId": "9119f435-0a5b-4258-bcdb-2ec74e0a7a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.5.202.186:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.5.202.186:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/347M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c255a326d5084e82a7428cbb76e4d832"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_19', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig\n",
        "import tensorflow as tf\n",
        "\n",
        "distil_bert = 'distilbert-base-uncased'\n",
        "\n",
        "config = DistilBertConfig(num_labels=3)\n",
        "config.output_hidden_states = False\n",
        "\n",
        "def create_TFDistilBertForSequenceClassification():\n",
        "  transformer_model = TFDistilBertForSequenceClassification.from_pretrained(distil_bert, config = config)\n",
        "  input_ids = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
        "  input_masks_ids = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32')\n",
        "  X = transformer_model(input_ids, input_masks_ids)\n",
        "  return tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs = X)\n",
        "\n",
        "use_tpu = True\n",
        "if use_tpu:\n",
        "  # Create distribution strategy\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "  # Create model on TPU:\n",
        "  with strategy.scope():\n",
        "    model = create_TFDistilBertForSequenceClassification()\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=5e-5)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "else:\n",
        "  model = create_TFDistilBertForSequenceClassification()\n",
        "  model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CPFj0CMx9mw",
        "outputId": "b84693b6-2369-44ac-94ae-c412fd06e834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_for_sequence_cl  TFSequenceClassifie  66955779   ['input_token[0][0]',            \n",
            " assification (TFDistilBertForS  rOutput(loss=None,               'masked_token[0][0]']           \n",
            " equenceClassification)         logits=(None, 3),                                                 \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 66,955,779\n",
            "Trainable params: 66,955,779\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQQH5lE_33Vn",
        "outputId": "f2a177f8-d45b-4e66-c262-4f9c5d680517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "22/22 [==============================] - 111s 3s/step - loss: 0.6977 - accuracy: 0.4304 - val_loss: 0.5902 - val_accuracy: 0.4535\n",
            "Epoch 2/30\n",
            "22/22 [==============================] - 5s 249ms/step - loss: 0.5803 - accuracy: 0.5107 - val_loss: 0.5310 - val_accuracy: 0.5818\n",
            "Epoch 3/30\n",
            "22/22 [==============================] - 5s 227ms/step - loss: 0.5284 - accuracy: 0.5996 - val_loss: 0.5126 - val_accuracy: 0.6276\n",
            "Epoch 4/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.4948 - accuracy: 0.6350 - val_loss: 0.5633 - val_accuracy: 0.6637\n",
            "Epoch 5/30\n",
            "22/22 [==============================] - 5s 232ms/step - loss: 0.4786 - accuracy: 0.6783 - val_loss: 0.5032 - val_accuracy: 0.7042\n",
            "Epoch 6/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.4009 - accuracy: 0.7482 - val_loss: 0.4687 - val_accuracy: 0.7492\n",
            "Epoch 7/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.3667 - accuracy: 0.7807 - val_loss: 0.4439 - val_accuracy: 0.7553\n",
            "Epoch 8/30\n",
            "22/22 [==============================] - 5s 225ms/step - loss: 0.3287 - accuracy: 0.8095 - val_loss: 0.6038 - val_accuracy: 0.6637\n",
            "Epoch 9/30\n",
            "22/22 [==============================] - 5s 228ms/step - loss: 0.5236 - accuracy: 0.6157 - val_loss: 0.5139 - val_accuracy: 0.6276\n",
            "Epoch 10/30\n",
            "22/22 [==============================] - 5s 227ms/step - loss: 0.4628 - accuracy: 0.6432 - val_loss: 0.5119 - val_accuracy: 0.6802\n",
            "Epoch 11/30\n",
            "22/22 [==============================] - 5s 227ms/step - loss: 0.3959 - accuracy: 0.7707 - val_loss: 0.4382 - val_accuracy: 0.7748\n",
            "Epoch 12/30\n",
            "22/22 [==============================] - 5s 230ms/step - loss: 0.3387 - accuracy: 0.8081 - val_loss: 0.4822 - val_accuracy: 0.6704\n",
            "Epoch 13/30\n",
            "22/22 [==============================] - 5s 228ms/step - loss: 0.3483 - accuracy: 0.8109 - val_loss: 0.4742 - val_accuracy: 0.7800\n",
            "Epoch 14/30\n",
            "22/22 [==============================] - 5s 227ms/step - loss: 0.5203 - accuracy: 0.7341 - val_loss: 0.4991 - val_accuracy: 0.6126\n",
            "Epoch 15/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.4364 - accuracy: 0.7044 - val_loss: 0.4611 - val_accuracy: 0.6959\n",
            "Epoch 16/30\n",
            "22/22 [==============================] - 5s 230ms/step - loss: 0.3594 - accuracy: 0.7714 - val_loss: 0.4811 - val_accuracy: 0.7703\n",
            "Epoch 17/30\n",
            "22/22 [==============================] - 5s 230ms/step - loss: 0.3132 - accuracy: 0.8288 - val_loss: 0.5020 - val_accuracy: 0.7793\n",
            "Epoch 18/30\n",
            "22/22 [==============================] - 5s 228ms/step - loss: 0.2837 - accuracy: 0.8495 - val_loss: 0.4540 - val_accuracy: 0.7875\n",
            "Epoch 19/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.2538 - accuracy: 0.8636 - val_loss: 0.5185 - val_accuracy: 0.7830\n",
            "Epoch 20/30\n",
            "22/22 [==============================] - 5s 228ms/step - loss: 0.2565 - accuracy: 0.8625 - val_loss: 0.4745 - val_accuracy: 0.7898\n",
            "Epoch 21/30\n",
            "22/22 [==============================] - 5s 227ms/step - loss: 0.2281 - accuracy: 0.8824 - val_loss: 0.5923 - val_accuracy: 0.7845\n",
            "Epoch 22/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.2248 - accuracy: 0.8854 - val_loss: 0.5523 - val_accuracy: 0.8018\n",
            "Epoch 23/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.1936 - accuracy: 0.9004 - val_loss: 0.5138 - val_accuracy: 0.7965\n",
            "Epoch 24/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.2192 - accuracy: 0.8885 - val_loss: 0.4951 - val_accuracy: 0.7928\n",
            "Epoch 25/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.1928 - accuracy: 0.8982 - val_loss: 0.6265 - val_accuracy: 0.8026\n",
            "Epoch 26/30\n",
            "22/22 [==============================] - 5s 230ms/step - loss: 0.1774 - accuracy: 0.9155 - val_loss: 0.7272 - val_accuracy: 0.7995\n",
            "Epoch 27/30\n",
            "22/22 [==============================] - 5s 227ms/step - loss: 0.2212 - accuracy: 0.8966 - val_loss: 0.5018 - val_accuracy: 0.7853\n",
            "Epoch 28/30\n",
            "22/22 [==============================] - 5s 232ms/step - loss: 0.1702 - accuracy: 0.9246 - val_loss: 0.6327 - val_accuracy: 0.8063\n",
            "Epoch 29/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.1438 - accuracy: 0.9337 - val_loss: 0.7905 - val_accuracy: 0.8093\n",
            "Epoch 30/30\n",
            "22/22 [==============================] - 5s 230ms/step - loss: 0.1406 - accuracy: 0.9361 - val_loss: 0.8661 - val_accuracy: 0.7770\n"
          ]
        }
      ],
      "source": [
        "history = model.fit([x_train_int_np,x_train_masks_np],\n",
        "                    y_train,\n",
        "                    epochs=30,\n",
        "                    batch_size=512,\n",
        "                    validation_data=([x_dev_int_np,x_dev_masks_np], y_dev),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQcytuBdaWVp",
        "outputId": "e19dbb56-d46e-4167-dae6-fa818a45eded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42/42 [==============================] - 8s 106ms/step - loss: 0.8935 - accuracy: 0.7732\n",
            "[0.8935271501541138, 0.7732036113739014]\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate([x_test_int_np,x_test_masks_np], y_test)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdZ4nl08vp9A"
      },
      "source": [
        "\n",
        "## Model 2: Neural bag of words using BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gyCwXFj_R5w"
      },
      "source": [
        "We use model3-1 from lab4 to integrate BERT, using BERT instead of the previous static word embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DStlnRQRf-4v"
      },
      "outputs": [],
      "source": [
        "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
        "    def call(self, x, mask=None):\n",
        "        if mask != None:\n",
        "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
        "        else:\n",
        "            return super().call(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8fTwmYDvNEyT"
      },
      "outputs": [],
      "source": [
        "from transformers import TFDistilBertModel, DistilBertConfig\n",
        "\n",
        "def get_BERT_layer():\n",
        "  distil_bert = 'distilbert-base-uncased'\n",
        "  config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
        "  config.output_hidden_states = False\n",
        "  return TFDistilBertModel.from_pretrained(distil_bert, config = config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VICS9rY8C7KH",
        "outputId": "69e7e785-e935-4cd5-db4e-b9f49cd574ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.5.202.186:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.5.202.186:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.5.202.186:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.5.202.186:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Model2_BERT\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  66362880   ['input_token[0][0]',            \n",
            " BertModel)                     ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 128, 768),                                                   \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_maske  (None, 768)         0           ['tf_distil_bert_model[0][0]']   \n",
            " d (GlobalAveragePooling1DMaske                                                                   \n",
            " d)                                                                                               \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 16)           12304       ['global_average_pooling1d_masked\n",
            "                                                                 [0][0]']                         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            51          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 66,375,235\n",
            "Trainable params: 66,375,235\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "hdepth=16\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "EMBED_SIZE=100\n",
        "\n",
        "\n",
        "def create_bag_of_words_BERT():\n",
        "  input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
        "  input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
        "\n",
        "  bert_embeddings = get_BERT_layer()\n",
        "  embedded_sent = bert_embeddings(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "\n",
        "  pooled_sent=GlobalAveragePooling1DMasked()(embedded_sent)\n",
        "  hidden_output=Dense(hdepth,input_shape=(MAX_SEQUENCE_LENGTH,EMBED_SIZE),activation='sigmoid',kernel_initializer='glorot_uniform')(pooled_sent) # Sigmoid\n",
        "  label=Dense(3,input_shape=(hdepth,),activation='softmax',kernel_initializer='glorot_uniform')(hidden_output)\n",
        "  return Model(inputs=[input_ids_in,input_masks_in], outputs=[label],name='Model2_BERT')\n",
        "\n",
        "use_tpu = True\n",
        "if use_tpu:\n",
        "  # Create distribution strategy\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "  # Create model\n",
        "  with strategy.scope():\n",
        "    model2 = create_bag_of_words_BERT()\n",
        "    optimizer2 = tf.keras.optimizers.Adam(lr=5e-5)\n",
        "    model2.compile(optimizer=optimizer2, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "else:\n",
        "  model2 = create_bag_of_words_BERT()\n",
        "  model2.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model2.summary() \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0SbsCsxF1zi",
        "outputId": "d44752d1-f229-43a3-b9a7-679f12051844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "22/22 [==============================] - 109s 3s/step - loss: 0.6213 - accuracy: 0.4321 - val_loss: 0.5913 - val_accuracy: 0.4595\n",
            "Epoch 2/30\n",
            "22/22 [==============================] - 5s 228ms/step - loss: 0.5596 - accuracy: 0.5651 - val_loss: 0.5288 - val_accuracy: 0.6201\n",
            "Epoch 3/30\n",
            "22/22 [==============================] - 5s 227ms/step - loss: 0.5119 - accuracy: 0.6584 - val_loss: 0.4896 - val_accuracy: 0.7312\n",
            "Epoch 4/30\n",
            "22/22 [==============================] - 5s 228ms/step - loss: 0.4745 - accuracy: 0.7472 - val_loss: 0.4659 - val_accuracy: 0.7680\n",
            "Epoch 5/30\n",
            "22/22 [==============================] - 5s 227ms/step - loss: 0.4456 - accuracy: 0.8087 - val_loss: 0.4534 - val_accuracy: 0.8026\n",
            "Epoch 6/30\n",
            "22/22 [==============================] - 5s 228ms/step - loss: 0.4319 - accuracy: 0.8367 - val_loss: 0.4478 - val_accuracy: 0.8086\n",
            "Epoch 7/30\n",
            "22/22 [==============================] - 5s 227ms/step - loss: 0.4184 - accuracy: 0.8628 - val_loss: 0.4479 - val_accuracy: 0.8078\n",
            "Epoch 8/30\n",
            "22/22 [==============================] - 5s 230ms/step - loss: 0.4090 - accuracy: 0.8776 - val_loss: 0.4437 - val_accuracy: 0.8123\n",
            "Epoch 9/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.4016 - accuracy: 0.8925 - val_loss: 0.4415 - val_accuracy: 0.8176\n",
            "Epoch 10/30\n",
            "22/22 [==============================] - 5s 228ms/step - loss: 0.3925 - accuracy: 0.9088 - val_loss: 0.4391 - val_accuracy: 0.8206\n",
            "Epoch 11/30\n",
            "22/22 [==============================] - 5s 228ms/step - loss: 0.3882 - accuracy: 0.9153 - val_loss: 0.4378 - val_accuracy: 0.8236\n",
            "Epoch 12/30\n",
            "22/22 [==============================] - 5s 230ms/step - loss: 0.3810 - accuracy: 0.9254 - val_loss: 0.4345 - val_accuracy: 0.8266\n",
            "Epoch 13/30\n",
            "22/22 [==============================] - 5s 230ms/step - loss: 0.3770 - accuracy: 0.9301 - val_loss: 0.4397 - val_accuracy: 0.8108\n",
            "Epoch 14/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.3742 - accuracy: 0.9325 - val_loss: 0.4348 - val_accuracy: 0.8221\n",
            "Epoch 15/30\n",
            "22/22 [==============================] - 5s 231ms/step - loss: 0.3690 - accuracy: 0.9414 - val_loss: 0.4356 - val_accuracy: 0.8161\n",
            "Epoch 16/30\n",
            "22/22 [==============================] - 5s 231ms/step - loss: 0.3661 - accuracy: 0.9428 - val_loss: 0.4402 - val_accuracy: 0.8086\n",
            "Epoch 17/30\n",
            "22/22 [==============================] - 5s 230ms/step - loss: 0.3641 - accuracy: 0.9450 - val_loss: 0.4367 - val_accuracy: 0.8161\n",
            "Epoch 18/30\n",
            "22/22 [==============================] - 5s 228ms/step - loss: 0.3596 - accuracy: 0.9504 - val_loss: 0.4294 - val_accuracy: 0.8266\n",
            "Epoch 19/30\n",
            "22/22 [==============================] - 5s 226ms/step - loss: 0.3569 - accuracy: 0.9531 - val_loss: 0.4296 - val_accuracy: 0.8258\n",
            "Epoch 20/30\n",
            "22/22 [==============================] - 5s 228ms/step - loss: 0.3548 - accuracy: 0.9541 - val_loss: 0.4255 - val_accuracy: 0.8296\n",
            "Epoch 21/30\n",
            "22/22 [==============================] - 5s 226ms/step - loss: 0.3517 - accuracy: 0.9571 - val_loss: 0.4247 - val_accuracy: 0.8281\n",
            "Epoch 22/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.3493 - accuracy: 0.9591 - val_loss: 0.4285 - val_accuracy: 0.8228\n",
            "Epoch 23/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.3461 - accuracy: 0.9624 - val_loss: 0.4293 - val_accuracy: 0.8131\n",
            "Epoch 24/30\n",
            "22/22 [==============================] - 5s 230ms/step - loss: 0.3455 - accuracy: 0.9609 - val_loss: 0.4190 - val_accuracy: 0.8326\n",
            "Epoch 25/30\n",
            "22/22 [==============================] - 6s 269ms/step - loss: 0.3415 - accuracy: 0.9655 - val_loss: 0.4187 - val_accuracy: 0.8348\n",
            "Epoch 26/30\n",
            "22/22 [==============================] - 5s 228ms/step - loss: 0.3400 - accuracy: 0.9663 - val_loss: 0.4177 - val_accuracy: 0.8356\n",
            "Epoch 27/30\n",
            "22/22 [==============================] - 5s 230ms/step - loss: 0.3384 - accuracy: 0.9665 - val_loss: 0.4191 - val_accuracy: 0.8288\n",
            "Epoch 28/30\n",
            "22/22 [==============================] - 5s 229ms/step - loss: 0.3360 - accuracy: 0.9683 - val_loss: 0.4214 - val_accuracy: 0.8243\n",
            "Epoch 29/30\n",
            "22/22 [==============================] - 5s 228ms/step - loss: 0.3332 - accuracy: 0.9707 - val_loss: 0.4220 - val_accuracy: 0.8221\n",
            "Epoch 30/30\n",
            "22/22 [==============================] - 5s 228ms/step - loss: 0.3314 - accuracy: 0.9712 - val_loss: 0.4207 - val_accuracy: 0.8236\n"
          ]
        }
      ],
      "source": [
        "\n",
        "history = model2.fit([x_train_int_np,x_train_masks_np],\n",
        "                    y_train,\n",
        "                    epochs=30,\n",
        "                    batch_size=512,\n",
        "                    validation_data=([x_dev_int_np,x_dev_masks_np], y_dev),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs0_vvG6UQtv",
        "outputId": "fec5fdc2-46f8-4a6f-b1d8-2490edc86acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42/42 [==============================] - 8s 108ms/step - loss: 0.4126 - accuracy: 0.8383\n",
            "[0.41258203983306885, 0.8383233547210693]\n"
          ]
        }
      ],
      "source": [
        "results = model2.evaluate([x_test_int_np,x_test_masks_np], y_test)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awOphcCnhEwv"
      },
      "source": [
        "## Model 3: CNN or LSTM with BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5codSzohQ_9"
      },
      "source": [
        "Please follow the same methods as model2 to construct a CNN or LSTM model on top of BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMiiWhW4hPRA",
        "outputId": "ab15a50b-a303-454b-ea78-db338e396053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.5.202.186:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.5.202.186:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.5.202.186:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.5.202.186:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Model3_LSTM\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_token (InputLayer)       [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " masked_token (InputLayer)      [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_model_1 (TFDist  TFBaseModelOutput(l  66362880   ['input_token[0][0]',            \n",
            " ilBertModel)                   ast_hidden_state=(N               'masked_token[0][0]']           \n",
            "                                one, 128, 768),                                                   \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 100)          347600      ['tf_distil_bert_model_1[0][0]'] \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 3)            303         ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 66,710,783\n",
            "Trainable params: 66,710,783\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# your code goes here\n",
        "import tensorflow as tf\n",
        "hdepth=16\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "EMBED_SIZE=100\n",
        "\n",
        "def create_LSTM_BERT():\n",
        "  #Input layers (input id and masked input)\n",
        "  input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
        "  input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
        "\n",
        "  #Bert Embedding Layer\n",
        "  bert_embeddings = get_BERT_layer()\n",
        "  embedded_sent = bert_embeddings(input_ids_in, attention_mask=input_masks_in)[0]\n",
        "\n",
        "  # LSTM layer having 100 lstm cells \n",
        "  LSTM_layer = LSTM(100)(embedded_sent) \n",
        "  \n",
        "  #Output Layer with 3 output cells and softmax activation function with glorot_uniform as kernel intializer\n",
        "  label = Dense(3,input_shape=(hdepth,),activation='softmax',kernel_initializer='glorot_uniform')(LSTM_layer)\n",
        "\n",
        "  return Model(inputs=[input_ids_in,input_masks_in], outputs=[label],name='Model3_LSTM')#Return Model Object\n",
        "\n",
        "use_tpu = True\n",
        "if use_tpu:\n",
        "  # Create distribution strategy\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "  # Create model\n",
        "  with strategy.scope():\n",
        "    model3 = create_LSTM_BERT()\n",
        "    optimizer3 = tf.keras.optimizers.Adam(lr=5e-5)\n",
        "    model3.compile(optimizer=optimizer3, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "else:\n",
        "  model3 = create_LSTM_BERT()\n",
        "  model3.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy']) \n",
        "\n",
        "\n",
        "model3.summary() \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9HmJNzNpIEM",
        "outputId": "09667a62-4d71-4c31-8375-04429d36aa90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "22/22 [==============================] - 116s 3s/step - loss: 0.5940 - accuracy: 0.4799 - val_loss: 0.5494 - val_accuracy: 0.5368\n",
            "Epoch 2/30\n",
            "22/22 [==============================] - 7s 297ms/step - loss: 0.4949 - accuracy: 0.6096 - val_loss: 0.4419 - val_accuracy: 0.6802\n",
            "Epoch 3/30\n",
            "22/22 [==============================] - 6s 294ms/step - loss: 0.3782 - accuracy: 0.7392 - val_loss: 0.3387 - val_accuracy: 0.7785\n",
            "Epoch 4/30\n",
            "22/22 [==============================] - 7s 297ms/step - loss: 0.2925 - accuracy: 0.8155 - val_loss: 0.3013 - val_accuracy: 0.8108\n",
            "Epoch 5/30\n",
            "22/22 [==============================] - 6s 297ms/step - loss: 0.2414 - accuracy: 0.8521 - val_loss: 0.3021 - val_accuracy: 0.8168\n",
            "Epoch 6/30\n",
            "22/22 [==============================] - 7s 297ms/step - loss: 0.2056 - accuracy: 0.8790 - val_loss: 0.3142 - val_accuracy: 0.8138\n",
            "Epoch 7/30\n",
            "22/22 [==============================] - 7s 299ms/step - loss: 0.1621 - accuracy: 0.9119 - val_loss: 0.3173 - val_accuracy: 0.8161\n",
            "Epoch 8/30\n",
            "22/22 [==============================] - 7s 301ms/step - loss: 0.1390 - accuracy: 0.9246 - val_loss: 0.3362 - val_accuracy: 0.8191\n",
            "Epoch 9/30\n",
            "22/22 [==============================] - 7s 299ms/step - loss: 0.1119 - accuracy: 0.9429 - val_loss: 0.3886 - val_accuracy: 0.8071\n",
            "Epoch 10/30\n",
            "22/22 [==============================] - 7s 298ms/step - loss: 0.0988 - accuracy: 0.9498 - val_loss: 0.3791 - val_accuracy: 0.8131\n",
            "Epoch 11/30\n",
            "22/22 [==============================] - 7s 300ms/step - loss: 0.0812 - accuracy: 0.9606 - val_loss: 0.3962 - val_accuracy: 0.8123\n",
            "Epoch 12/30\n",
            "22/22 [==============================] - 7s 297ms/step - loss: 0.0683 - accuracy: 0.9683 - val_loss: 0.3999 - val_accuracy: 0.8191\n",
            "Epoch 13/30\n",
            "22/22 [==============================] - 7s 298ms/step - loss: 0.0560 - accuracy: 0.9737 - val_loss: 0.4328 - val_accuracy: 0.8191\n",
            "Epoch 14/30\n",
            "22/22 [==============================] - 7s 297ms/step - loss: 0.0542 - accuracy: 0.9753 - val_loss: 0.4284 - val_accuracy: 0.8123\n",
            "Epoch 15/30\n",
            "22/22 [==============================] - 7s 299ms/step - loss: 0.0501 - accuracy: 0.9768 - val_loss: 0.4408 - val_accuracy: 0.8243\n",
            "Epoch 16/30\n",
            "22/22 [==============================] - 7s 299ms/step - loss: 0.0456 - accuracy: 0.9790 - val_loss: 0.4506 - val_accuracy: 0.8183\n",
            "Epoch 17/30\n",
            "22/22 [==============================] - 7s 297ms/step - loss: 0.0398 - accuracy: 0.9833 - val_loss: 0.4569 - val_accuracy: 0.8191\n",
            "Epoch 18/30\n",
            "22/22 [==============================] - 7s 300ms/step - loss: 0.0394 - accuracy: 0.9826 - val_loss: 0.4442 - val_accuracy: 0.8183\n",
            "Epoch 19/30\n",
            "22/22 [==============================] - 7s 299ms/step - loss: 0.0346 - accuracy: 0.9854 - val_loss: 0.4415 - val_accuracy: 0.8191\n",
            "Epoch 20/30\n",
            "22/22 [==============================] - 7s 299ms/step - loss: 0.0359 - accuracy: 0.9839 - val_loss: 0.4587 - val_accuracy: 0.8281\n",
            "Epoch 21/30\n",
            "22/22 [==============================] - 7s 300ms/step - loss: 0.0270 - accuracy: 0.9899 - val_loss: 0.4779 - val_accuracy: 0.8168\n",
            "Epoch 22/30\n",
            "22/22 [==============================] - 7s 300ms/step - loss: 0.0255 - accuracy: 0.9897 - val_loss: 0.4860 - val_accuracy: 0.8296\n",
            "Epoch 23/30\n",
            "22/22 [==============================] - 7s 299ms/step - loss: 0.0305 - accuracy: 0.9868 - val_loss: 0.4907 - val_accuracy: 0.8243\n",
            "Epoch 24/30\n",
            "22/22 [==============================] - 7s 298ms/step - loss: 0.0245 - accuracy: 0.9903 - val_loss: 0.4788 - val_accuracy: 0.8243\n",
            "Epoch 25/30\n",
            "22/22 [==============================] - 6s 297ms/step - loss: 0.0282 - accuracy: 0.9884 - val_loss: 0.4717 - val_accuracy: 0.8303\n",
            "Epoch 26/30\n",
            "22/22 [==============================] - 7s 298ms/step - loss: 0.0198 - accuracy: 0.9928 - val_loss: 0.4975 - val_accuracy: 0.8258\n",
            "Epoch 27/30\n",
            "22/22 [==============================] - 7s 300ms/step - loss: 0.0186 - accuracy: 0.9930 - val_loss: 0.5059 - val_accuracy: 0.8168\n",
            "Epoch 28/30\n",
            "22/22 [==============================] - 7s 300ms/step - loss: 0.0217 - accuracy: 0.9908 - val_loss: 0.5085 - val_accuracy: 0.8213\n",
            "Epoch 29/30\n",
            "22/22 [==============================] - 7s 300ms/step - loss: 0.0189 - accuracy: 0.9925 - val_loss: 0.5164 - val_accuracy: 0.8191\n",
            "Epoch 30/30\n",
            "22/22 [==============================] - 7s 299ms/step - loss: 0.0189 - accuracy: 0.9928 - val_loss: 0.5199 - val_accuracy: 0.8228\n"
          ]
        }
      ],
      "source": [
        "\n",
        "history = model3.fit([x_train_int_np,x_train_masks_np],\n",
        "                    y_train,\n",
        "                    epochs=30,\n",
        "                    batch_size=512,\n",
        "                    validation_data=([x_dev_int_np,x_dev_masks_np], y_dev),\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7TXLjQhpY--",
        "outputId": "2ca9582b-41cd-420c-adc3-b8e15261e09d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42/42 [==============================] - 10s 122ms/step - loss: 0.4912 - accuracy: 0.8271\n",
            "[0.4911702871322632, 0.8270958662033081]\n"
          ]
        }
      ],
      "source": [
        "results = model3.evaluate([x_test_int_np,x_test_masks_np], y_test)\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "7001_2021_22_lab8_Aspect_Based_Sentiment_Analysis_with_BERT_without_answer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f2b9c0e7eb8c4ddc97f8bc05794cdc5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9620819eedb449bb9d1b19e9377a4cb1",
              "IPY_MODEL_87e79a20f7a64c0ab1712bf983bfa1bc",
              "IPY_MODEL_c217b2ba58724052811bf12cc6e54412"
            ],
            "layout": "IPY_MODEL_2b695618bfbc48ea94a9c63531d47fea"
          }
        },
        "9620819eedb449bb9d1b19e9377a4cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1161a38da5146c4a4ffc18f88167a1b",
            "placeholder": "​",
            "style": "IPY_MODEL_09ba5008ad144bf8a43b97d5cd9bbe48",
            "value": "Downloading: 100%"
          }
        },
        "87e79a20f7a64c0ab1712bf983bfa1bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0270154b104e404fbe8eab82dfa74ebf",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_587ee74d28f04f0998c8e39b517ba396",
            "value": 231508
          }
        },
        "c217b2ba58724052811bf12cc6e54412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a460dbf1f9e4ff396766ad478e1690e",
            "placeholder": "​",
            "style": "IPY_MODEL_f3137c066acc4bbc8f3093301b4be703",
            "value": " 226k/226k [00:00&lt;00:00, 6.90kB/s]"
          }
        },
        "2b695618bfbc48ea94a9c63531d47fea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1161a38da5146c4a4ffc18f88167a1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09ba5008ad144bf8a43b97d5cd9bbe48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0270154b104e404fbe8eab82dfa74ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "587ee74d28f04f0998c8e39b517ba396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a460dbf1f9e4ff396766ad478e1690e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3137c066acc4bbc8f3093301b4be703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78b443ca8d0045f7a4ddd043770a73fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_175ab4506ac9473ead3c1a6464b01f23",
              "IPY_MODEL_dac14d6e91684aaaaf8ed1399419d13f",
              "IPY_MODEL_0bb6fc70d0ee47d880cdce04022a3f9d"
            ],
            "layout": "IPY_MODEL_732f0ea7b13d4d478f76cd50675a5929"
          }
        },
        "175ab4506ac9473ead3c1a6464b01f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddca485695144650954d48523c39702a",
            "placeholder": "​",
            "style": "IPY_MODEL_7aabddaab4524ce782f6d5750582c73a",
            "value": "Downloading: 100%"
          }
        },
        "dac14d6e91684aaaaf8ed1399419d13f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42b5d6ff4bb945a5a5de723e97fb4d6e",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6f3bd3ed7614967a2a7f66e4f8ecc32",
            "value": 28
          }
        },
        "0bb6fc70d0ee47d880cdce04022a3f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a6e54cd0702442c9be0b32c6ebc699a",
            "placeholder": "​",
            "style": "IPY_MODEL_2cf3ceabbfd74e5da5a4acb491fa8538",
            "value": " 28.0/28.0 [00:00&lt;00:00, 211B/s]"
          }
        },
        "732f0ea7b13d4d478f76cd50675a5929": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddca485695144650954d48523c39702a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aabddaab4524ce782f6d5750582c73a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42b5d6ff4bb945a5a5de723e97fb4d6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6f3bd3ed7614967a2a7f66e4f8ecc32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a6e54cd0702442c9be0b32c6ebc699a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cf3ceabbfd74e5da5a4acb491fa8538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ddaf33ee953440f1ac2da925025db458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8764bc72fef7449998cc149264621f98",
              "IPY_MODEL_b43050c8141544629a4ebd9f1426f12f",
              "IPY_MODEL_0e132ea8fb47420197449dbdedc1df8c"
            ],
            "layout": "IPY_MODEL_9a50241b2cb74f5194b4dee774cf8268"
          }
        },
        "8764bc72fef7449998cc149264621f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35fdf42bf34a479e80284c5e0b1ac5bc",
            "placeholder": "​",
            "style": "IPY_MODEL_99c2041603604408b382c62c1200f185",
            "value": "Downloading: 100%"
          }
        },
        "b43050c8141544629a4ebd9f1426f12f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aab8d0e2ca141ad8e8ab90891869c87",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa282d0732ca4dbe9cbf2a0f5471cb61",
            "value": 483
          }
        },
        "0e132ea8fb47420197449dbdedc1df8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce7bf7c42052458f9be7cfec5ebb50f2",
            "placeholder": "​",
            "style": "IPY_MODEL_ae0c768e14144137937a7b5838da632e",
            "value": " 483/483 [00:00&lt;00:00, 3.19kB/s]"
          }
        },
        "9a50241b2cb74f5194b4dee774cf8268": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35fdf42bf34a479e80284c5e0b1ac5bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99c2041603604408b382c62c1200f185": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4aab8d0e2ca141ad8e8ab90891869c87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa282d0732ca4dbe9cbf2a0f5471cb61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce7bf7c42052458f9be7cfec5ebb50f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae0c768e14144137937a7b5838da632e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c255a326d5084e82a7428cbb76e4d832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71f21906de884cb29def8592fa4ea89e",
              "IPY_MODEL_fec7e66392544b81862ce9adaea636e5",
              "IPY_MODEL_69b73b55a35c4df285022f7b26e6d235"
            ],
            "layout": "IPY_MODEL_db2ebaa6b51045fd8be6d4d897540afc"
          }
        },
        "71f21906de884cb29def8592fa4ea89e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58f460acdcb54608923a36f950c0ce04",
            "placeholder": "​",
            "style": "IPY_MODEL_8bc5a7c67f774a8ebee847ad9ef0c7b9",
            "value": "Downloading: 100%"
          }
        },
        "fec7e66392544b81862ce9adaea636e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ef3cf5b2aa94c119e0501e1fb29b3d7",
            "max": 363423424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8de9886e5cac49e8a6da6736161b5e59",
            "value": 363423424
          }
        },
        "69b73b55a35c4df285022f7b26e6d235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e2dca9750e747d782da204c689615ae",
            "placeholder": "​",
            "style": "IPY_MODEL_3d041122564d48b69d2654d43ed5c070",
            "value": " 347M/347M [00:14&lt;00:00, 24.4MB/s]"
          }
        },
        "db2ebaa6b51045fd8be6d4d897540afc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58f460acdcb54608923a36f950c0ce04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bc5a7c67f774a8ebee847ad9ef0c7b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ef3cf5b2aa94c119e0501e1fb29b3d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8de9886e5cac49e8a6da6736161b5e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e2dca9750e747d782da204c689615ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d041122564d48b69d2654d43ed5c070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}