{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pulSn6bHUOTV"
      },
      "source": [
        "# Lab 12 - Creating an End-To-End Dialogue System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb_Z5dCsUQnW"
      },
      "source": [
        "In the last lab we want to create end-to-end dialogue systems, following on from the seq2seq MT labs you've done. Customer support apps and online helpdesks are among the places where conversational models can be used. Retrieval-based models, which produce predefined responses to questions of specific types, are often used to power these models. In this lab, the seq2seq model is used to build a generative model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3q5lgWkVD6-"
      },
      "source": [
        "To begin, download the data ZIP file from [here](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) and place it under the current directory. \n",
        "\n",
        "After that, let’s import some necessities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9Dz-W5nwGydd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, GRU, TimeDistributed\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Bidirectional, Concatenate, Lambda\n",
        "np.random.seed(1)\n",
        "random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-bgxBxHHIDi",
        "outputId": "47d60d6f-fe2b-401f-e879-c6d47cdd7163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/NLP/lab_12/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVIb8_AK9xjD"
      },
      "source": [
        "# The Cornell Movie-Dialogs Corpus "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7hru-2uFRWy"
      },
      "source": [
        "The next move is to reformat our data file and load the data into functional structures. \n",
        "\n",
        "The Cornell Movie-Dialogs Corpus contains 220,579 conversational exchanges between 10,292 pairs of movie characters, 9,035 characters from 617 movies, and 304,713 total utterances. This dataset is large with a wide variety of language formality, time periods, and other variables. Our hope is that this variety will make our model responsive to a wide range of queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVCjdDP_HMIM",
        "outputId": "f32860b3-c18f-4790-8e4d-1673dd5dd158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
            "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
            "\n",
            "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
            "Not the hacking and gagging and spitting part.  Please.\n",
            "\n",
            "Not the hacking and gagging and spitting part.  Please.\n",
            "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
            "\n",
            "You're asking me out.  That's so cute. What's your name again?\n",
            "Forget it.\n",
            "\n",
            "No, no, it's my fault -- we didn't have a proper introduction ---\n",
            "Cameron.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221616"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#Loading the data\n",
        "lines = open('cornell movie-dialogs corpus/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "conv_lines = open('cornell movie-dialogs corpus/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n') # index of related lines\n",
        "\n",
        "# Create a dictionary to map each id with its line\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]\n",
        "        \n",
        "# Create a list of all of the ids.\n",
        "convs = [ ]\n",
        "for line in conv_lines[:-1]:\n",
        "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "    convs.append(_line.split(','))\n",
        "\n",
        "# Sort the sentences into questions (inputs) and answers (targets)\n",
        "pairs = []\n",
        "for conv in convs:\n",
        "    for i in range(len(conv)-1):\n",
        "        pairs.append([id2line[conv[i]],id2line[conv[i+1]]])\n",
        "        \n",
        "limit = 0\n",
        "for i in range(limit, limit+5):\n",
        "    print(pairs[i][0])\n",
        "    print(pairs[i][1])\n",
        "    print()\n",
        "    \n",
        "len(pairs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGhwqopLhZBu"
      },
      "source": [
        "We'll format data file with a question sentence and an answer sentence pair on each line for convenience.  Before we are ready to use this data, we must perform some preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0O-qSHrHYgU",
        "outputId": "04fa39b2-f41f-4827-befc-7a45497e8db7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['can we make this quick roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad again',\n",
              "  'well i thought we would start with pronunciation if that is okay with you'],\n",
              " ['well i thought we would start with pronunciation if that is okay with you',\n",
              "  'not the hacking and gagging and spitting part please'],\n",
              " ['not the hacking and gagging and spitting part please',\n",
              "  'okay then how about we try out some french cuisine saturday night'],\n",
              " ['you are asking me out that is so cute that is your name again',\n",
              "  'forget it'],\n",
              " ['no no it is my fault we did not have a proper introduction', 'cameron']]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "def preprocess(pairs):\n",
        "    p = pairs.copy()\n",
        "\n",
        "    for i in p:\n",
        "        for j in range(0,2):\n",
        "            i[j] = i[j].lower()\n",
        "            i[j] = re.sub(r\"there's\", \"there is\", i[j])\n",
        "            i[j] = re.sub(r\"i'm\", \"i am\", i[j])\n",
        "            i[j] = re.sub(r\"he's\", \"he is\", i[j])\n",
        "            i[j] = re.sub(r\"she's\", \"she is\", i[j])\n",
        "            i[j] = re.sub(r\"it's\", \"it is\", i[j])\n",
        "            i[j] = re.sub(r\"that's\", \"that is\", i[j])\n",
        "            i[j] = re.sub(r\"what's\", \"that is\", i[j])\n",
        "            i[j] = re.sub(r\"where's\", \"where is\", i[j])\n",
        "            i[j] = re.sub(r\"how's\", \"how is\", i[j])\n",
        "            i[j] = re.sub(r\"\\'ll\", \" will\", i[j])\n",
        "            i[j] = re.sub(r\"\\'ve\", \" have\", i[j])\n",
        "            i[j] = re.sub(r\"\\'re\", \" are\", i[j])\n",
        "            i[j] = re.sub(r\"\\'d\", \" would\", i[j])\n",
        "            i[j] = re.sub(r\"\\'re\", \" are\", i[j])\n",
        "            i[j] = re.sub(r\"won't\", \"will not\", i[j])\n",
        "            i[j] = re.sub(r\"can't\", \"cannot\", i[j])\n",
        "            i[j] = re.sub(r\"n't\", \" not\", i[j])\n",
        "            i[j] = re.sub(r\"n'\", \"ng\", i[j])\n",
        "            i[j] = re.sub(r\"'bout\", \"about\", i[j])\n",
        "            i[j] = re.sub(r\"'til\", \"until\", i[j])\n",
        "            i[j] = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", i[j])\n",
        "            i[j] = i[j].strip()\n",
        "    return p\n",
        "\n",
        "replaced_pairs = preprocess(pairs)\n",
        "def clean_data(pairs):\n",
        "    p = pairs.copy()\n",
        "    # prepare translation table \n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for i in p:\n",
        "        # tokenize\n",
        "        i[0], i[1] = i[0].split(), i[1].split()\n",
        "        # convert to lower case\n",
        "        i[0], i[1] = [word.lower() for word in i[0]], [word.lower() for word in i[1]]\n",
        "        # remove punctuations \n",
        "        i[0], i[1] = [w.translate(table) for w in i[0]], [w.translate(table) for w in i[1]]\n",
        "        # remove numbers \n",
        "        i[0], i[1] = [word for word in i[0] if word.isalpha()], [word for word in i[1] if word.isalpha()]\n",
        "        # store as string\n",
        "        i[0], i[1] =  ' '.join(i[0]), ' '.join(i[1])\n",
        "            \n",
        "    return p\n",
        "\n",
        "clean_pairs = clean_data(replaced_pairs)\n",
        "clean_pairs[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKTpzSOWnUgL"
      },
      "source": [
        "\n",
        "The parsing of the raw movie lines.txt data file is made simpler with the following functions. We add the start and end tokens to our sentences. We also need to find the maximum length.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_P2abDKHkbm",
        "outputId": "68503768-9af0-4462-f0f0-d4feccfbcebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max-len of questions for training:  18\n",
            "max-len of answers for training:  18\n"
          ]
        }
      ],
      "source": [
        "# adding the start and end tokens to our utterances\n",
        "start_token = '<startseq>'\n",
        "end_token = '<endseq>'\n",
        "\n",
        "def add_end_start_tokens(pairs):\n",
        "    p = pairs.copy()\n",
        "    for i in p:\n",
        "        i[0] = start_token + ' '  + i[0] + ' ' + end_token\n",
        "        i[1] = start_token + ' '  + i[1] + ' ' + end_token\n",
        "    return p\n",
        "\n",
        "tokenized_pairs = add_end_start_tokens(clean_pairs)\n",
        "tokenized_pairs[:5]\n",
        "\n",
        "# finding the maximum length for questions and answers\n",
        "# we caculate the max length that covers 80% of the data \n",
        "def max_length(pairs,prct):\n",
        "    # Create a list of all the utterances\n",
        "    questions = []\n",
        "    answers = []\n",
        "    for i in pairs:\n",
        "        questions.append(i[0])\n",
        "        answers.append(i[1])\n",
        "        \n",
        "    length_questions = list(len(d.split()) for d in questions)\n",
        "    length_answers = list(len(d.split()) for d in answers)\n",
        "\n",
        "    return int(np.percentile(length_questions, prct)),int(np.percentile(length_answers, prct))\n",
        "\n",
        "max_len_q,max_len_a = max_length(tokenized_pairs,80)\n",
        "\n",
        "print('max-len of questions for training: ', max_len_q)\n",
        "print('max-len of answers for training: ', max_len_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFsKDr1Wr6d3"
      },
      "source": [
        "The next step is to build a vocabulary and save query/response pairs. \n",
        "It's worth noting that we're dealing with word sequences that don't have an implicit mapping to a discrete numerical space. As a result, we must build one by assigning an index value to each unique word in our dataset.\n",
        "\n",
        "We are going to create our vocabulary. Trimming rarely used words from our vocabulary is another strategy for achieving faster convergence during preparation. The complexity of the function that the model must learn to approximate will be lowered as the feature space is reduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y20Cn9KPHrid",
        "outputId": "e5b24d6f-8e37-4434-e6fe-257966d1e39f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Short vocab size: 14524 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<endseq>', '<startseq>', 'a', 'aa', 'aaaah']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Remove questions and answers that are shorter than 2 words and longer than maxlen.\n",
        "min_line_len = 2 # two words are for tokens\n",
        "\n",
        "def set_length(tokenized_pairs):\n",
        "    pairs_final = []\n",
        "    for p in tokenized_pairs:\n",
        "        if (\n",
        "            len(p[0].split())>=min_line_len and len(p[1].split())>=min_line_len \n",
        "           and len(p[0].split())<=max_len_q and len(p[1].split())<=max_len_a):\n",
        "                \n",
        "            pairs_final.append(p)\n",
        "            \n",
        "    return pairs_final\n",
        "\n",
        "pairs_final = set_length(tokenized_pairs)\n",
        "len(pairs_final)\n",
        "\n",
        "# making a vocabulary of the words that occur more than word_count_threshold \n",
        "def create_reoccurring_vocab(pairs, word_count_threshold = 5):\n",
        "    p = pairs\n",
        "    all_captions = []\n",
        "    for i in p:\n",
        "        for j in i:\n",
        "            all_captions.append(j)\n",
        "\n",
        "    # Consider only words which occur at least 10 times in the corpus\n",
        "    word_counts = {}\n",
        "    nsents = 0\n",
        "    for sent in all_captions:\n",
        "        nsents += 1\n",
        "        for w in sent.split(' '):\n",
        "            word_counts[w] = word_counts.get(w, 0) + 1\n",
        "\n",
        "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "    vocab = list(set(vocab))\n",
        "    print('Short vocab size: %d ' % len(vocab))\n",
        "    return vocab\n",
        "\n",
        "# each word in the vocabulary must be used in the data at least 20 times\n",
        "new_vocab = create_reoccurring_vocab(pairs_final, word_count_threshold = 4)\n",
        "for v in new_vocab:\n",
        "    if len(v) == 1 and v!='a' and v!='i':\n",
        "        new_vocab.remove(v) \n",
        "\n",
        "new_vocab = sorted(new_vocab)[1:]\n",
        "new_vocab[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Gf7WtdIH1l7",
        "outputId": "9c182b82-083a-43c2-935e-8830ec22b2ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14500"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "vocab_len = len(new_vocab) + 1 # since index 0 is used as padding, we have to increase the vocab size\n",
        "vocab_len\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ1ai5lcibcU"
      },
      "source": [
        "We are going to create a dataset of pairs without the trimmed words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L0iu6Z3RmecS"
      },
      "outputs": [],
      "source": [
        "def progressBar(value, endvalue, bar_length=20, job=''):\n",
        "\n",
        "    percent = float(value) / endvalue\n",
        "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
        "    spaces = ' ' * (bar_length - len(arrow))\n",
        "\n",
        "    sys.stdout.write(\"\\r{0} : [{1}] {2}%\".format(job,arrow + spaces, int(round(percent * 100))))\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "def print_tensor(t):\n",
        "    print(K.get_value(t))\n",
        "    \n",
        "def to_tensor(t):\n",
        "    return tf.convert_to_tensor(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xWI1J7uH4II",
        "outputId": "e9ae615a-4fab-419d-dedc-3224312d61c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " : [------------------->] 100%\n",
            "Trimmed from 145905 pairs to 114938\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114938"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# keeping the pairs with words in the vocab\n",
        "def trimRareWords(voc, pairs):\n",
        "    # Filtering out the pairs with the oov words\n",
        "    keep_pairs = []\n",
        "    i=0\n",
        "    for pair in pairs:\n",
        "        i+=1\n",
        "        progressBar(value=i,endvalue=len(pairs))\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        #  input sentence\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc:\n",
        "                keep_input = False\n",
        "                break\n",
        "        #  output sentence\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"\\nTrimmed from {} pairs to {}\".format(len(pairs), len(keep_pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "\n",
        "# # Trim voc and pairs\n",
        "pairs_final = trimRareWords(new_vocab, pairs_final)\n",
        "with open ('final_pairs_v21.pkl','wb') as f:\n",
        "    pairs_final = pickle.dump(pairs_final,f)\n",
        "    \n",
        "with open ('final_pairs_v21.pkl','rb') as f:\n",
        "    pairs_final = pickle.load(f)\n",
        "    \n",
        "pairs_final_train = pairs_final\n",
        "len(pairs_final_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtP1tSpzf6FA",
        "outputId": "075ac8c3-a3a2-407f-84c0-b1fef9ca64e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114938"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "with open ('final_pairs_v21.pkl','rb') as f:\n",
        "    pairs_final = pickle.load(f)\n",
        "    \n",
        "pairs_final_train = pairs_final\n",
        "len(pairs_final_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szo-AVa_z1tR"
      },
      "source": [
        "# Embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf7PJEmwUywI"
      },
      "source": [
        "Our models will eventually expect numerical tensors as inputs, despite the fact that we put a lot of effort into preparing and massaging our data into a nice vocabulary object and list of sentence pairs. The Embedding layer can be used to load a pre-trained word embedding model. We are going to use GloVe embeddings. You can download GloVe and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrfd6TDxFKIL",
        "outputId": "f356f704-c7b6-410f-ef65-f669e8e6d959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-02 17:05:25--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-05-02 17:05:25--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-05-02 17:05:26--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.09MB/s    in 2m 40s  \n",
            "\n",
            "2022-05-02 17:08:06 (5.14 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d155hsA3FaIt",
        "outputId": "67f268d6-97d9-4dc8-d335-f0dbadb6b665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "!unzip glove*.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P8WElcGICYy",
        "outputId": "279e3485-4441-4ea6-dc11-1b390eacbb50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading glove...\n",
            "GloVe  50  loded!\n"
          ]
        }
      ],
      "source": [
        "test = False\n",
        "GRU_units = 50\n",
        "batch_size = 32\n",
        "emb_dim = 50\n",
        "init_lr = 0.001\n",
        "\n",
        "#Create an instance of the tokenizer object:\n",
        "tokenizer = Tokenizer(filters = [])\n",
        "tokenizer.fit_on_texts(new_vocab)\n",
        "\n",
        "ixtoword = {} # index to word dic\n",
        "wordtoix = tokenizer.word_index # word to index dic\n",
        "pad_token = 'pad0'\n",
        "ixtoword[0] = pad_token # no word in vocab has index 0,  padding is indicated with 0\n",
        "\n",
        "for w in tokenizer.word_index:\n",
        "    ixtoword[tokenizer.word_index[w]] = w\n",
        "\n",
        "# Making the embedding mtrix\n",
        "def make_embedding_layer(embedding_dim=50, glove=True):\n",
        "    if glove == False:\n",
        "        print('Just a zero matrix loaded')\n",
        "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # just a zero matrix \n",
        "    else:\n",
        "        print('Loading glove...')\n",
        "        embeddings_index = {} \n",
        "        f = open(os.path.join('glove.6B.50d.txt'), encoding=\"utf-8\")\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "        f.close()\n",
        "        print(\"GloVe \",embedding_dim, ' loded!')\n",
        "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # to import as weights for Keras Embedding layer\n",
        "        for word, i in wordtoix.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # Words that are not found in the embedding index will be all zeros\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    embedding_layer = Embedding(vocab_len, embedding_dim, mask_zero=True, trainable=False) # we have a limited vocab so we \n",
        "                                                                                           # do not train the embedding layer\n",
        "                                                                                           # we use 0 as padding so => mask_zero=True\n",
        "    embedding_layer.build((None,))\n",
        "    embedding_layer.set_weights([embedding_matrix])\n",
        "    \n",
        "    return embedding_layer\n",
        "\n",
        "embeddings = make_embedding_layer(embedding_dim=50, glove=not test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIZCs-L8-XWh"
      },
      "source": [
        "# Seq2Seq Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEcMRrQ3VVwA"
      },
      "source": [
        "A sequence-to-sequence (seq2seq) model is at the core of our model. The purpose of a seq2seq model is to use a fixed-sized sequence as an input and generate a variable-length sequence as an output.\n",
        "\n",
        "[Sutskever et al.](https://arxiv.org/abs/1409.3215) found that we can do this task by combining two different recurrent neural nets. One  RNN serves as an encoder, converting a variable-length input sequence to a fixed-length context vector. This context vector (the RNN's final hidden layer) contain semantic knowledge about the query sentence that the system receives.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhGdfvA0-jcE"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1fHwj0C-idB"
      },
      "source": [
        "The encoder RNN iterates through the input sentence one token at a time, producing an \"output\" vector and a \"hidden state\" vector at each time step.  The output vector is recorded while the hidden state vector is transferred to the next time step. The encoder converts the context it observed at each point in the sequence into a set of points in a high-dimensional space, which the decoder can use to produce a meaningful output for the task at hand.\n",
        "\n",
        "A multi-layered Gated Recurrent Unit, created by [Cho et al.](https://arxiv.org/pdf/1406.1078v3.pdf), is at the centre of our encoder. We'll use a bidirectional version of the GRU, which effectively means there are two separate RNNs: one fed the input sequence in regular sequential order and the other fed the input sequence in reverse order. At each time point, the outputs of each network are added together.\n",
        "\n",
        "***For the first task you need to define a bidirectional GRU and pass the embedding into the GRU. ***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cGy-CuL-IXKr"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_size\n",
        "        self.enc_units = enc_units\n",
        "\n",
        "        #  pass the embedding into a bidirectional version of the GRU - as you can see in the call() method below, you can use just 1 GRU layer but could experiment with more\n",
        "        self.embeddings = embeddings # Embedding layer                                                                            \n",
        "        self.dropout = Dropout(0.2) # Dropout Layer with 0.2 as dropout rate\n",
        "        self.Inp = Input(shape=(max_len_q,)) # size of questions\n",
        "        # Bidirectional GRU layer1 with 50 GRU units i.e 100 cells and return_sequences = True\n",
        "        self.Bidirectional1 = Bidirectional(GRU(self.enc_units, return_state= False, return_sequences= True))\n",
        "        # Bidirectional GRU layer2 with 50 GRU units i.e 100 cells and return_sequences = True and return_states = True\n",
        "        self.Bidirectional2 = Bidirectional(GRU(self.enc_units, return_state= True, return_sequences= True))\n",
        "            \n",
        "    def bidirectional(self, bidir, layer, inp, hidden):\n",
        "        return bidir(layer(inp, initial_state = hidden))\n",
        "    \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embeddings(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.Bidirectional1(x)\n",
        "        x = self.dropout(x)\n",
        "        output, state_f,state_b = self.Bidirectional2(x)\n",
        "        \n",
        "        return output, state_f, state_b\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HWX9HCkIIbed"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(vocab_len, 50, GRU_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80-YvbB--qbT"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ7GWss5YqUC"
      },
      "source": [
        "The response utterance is produced token by token by the decoder RNN. It generates the next word in the sequence using the encoder's context vectors and internal hidden states. It keeps producing words until it reaches the end of the sentence, which is represented by an end_token. A common issue with a standard seq2seq decoder is that relying solely on the context vector to encode the meaning of the complete input sequence would almost certainly result in information loss. This is particularly true when dealing with long input sequences, severely restricting our decoder's capabilities.\n",
        "\n",
        "[Bahdanau et al.](https://arxiv.org/abs/1409.0473) devised an \"attention mechanism\" that allows the decoder to focus on specific parts of the input sequence rather than using the whole set context at each step to deal with information loss. Attention is determined using the encoder's outputs and the decoder's current hidden state. Since the output attention weights have the same shape as the input sequence, we may multiply them by the encoder outputs to get a weighted amount that shows which sections of the encoder output to focus on.\n",
        "\n",
        "**For the second task you need to create the decoder with attention. Call the attention layer and use GRUs for decoding.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KssO1KLzId2d"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        self.units = units\n",
        "        \n",
        "    def call(self, query, values):\n",
        "        \n",
        "        # query hidden state shape == (batch_size, hidden size)\n",
        "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # values shape == (batch_size, max_len, hidden size)\n",
        "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Pyo_ifo5IhLx"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_size\n",
        "        self.embeddings = embeddings\n",
        "        self.units = 2 * dec_units # because we use bidirectional encoder\n",
        "        self.fc = Dense(vocab_len, activation='softmax', name='dense_layer')\n",
        "        # Create the decoder with attention - as you'll see in the call() method below, it will need two GRU layers\n",
        "        self.dropout = Dropout(0.2) # Dropout Layer with 0.2 as dropout rate\n",
        "        self.attention = BahdanauAttention(self.units) # BahdanauAttention Layer with 100 units\n",
        "        \n",
        "        # GRU Layer 1 with 100 GRU units and return_sequences = True\n",
        "        self.decoder_gru_l1 = GRU(self.units, return_sequences=True, return_state=False)\n",
        "        # GRU layer 2 with 100 GRU units and return_sequences = True and return_state = True\n",
        "        self.decoder_gru_l2 = GRU(self.units, return_sequences=False, return_state=True)\n",
        "\n",
        "\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embeddings(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1) # concat input and context vector together\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        x = self.decoder_gru_l1(x)\n",
        "        x = self.dropout(x)\n",
        "        output, state = self.decoder_gru_l2(x)\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UeMvjzzVIjVR"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(vocab_len, 50, GRU_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67WV-Fnb_VvJ"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJPZPKIpeGGN"
      },
      "source": [
        "We can now write functions to evaluate a string input sentence now that we've established our decoding process. The evaluate function is in charge of the low-level handling of the input sentence. The sentence is first formatted as an input batch of word indexes. To prepare the tensor for our models, we convert the words of the sentence to their corresponding indexes and transpose the dimensions. Our system's user interface is called answer. Our text is normalised in the same way that our training data is, and then fed into the evaluate function to generate a decoded output sentence and attention weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mVSbB_kLIo3p"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(5,5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def evaluate(sentence):\n",
        "    \n",
        "    attention_plot = np.zeros((max_len_a, max_len_q))\n",
        "\n",
        "    sentence = unicode_to_ascii(sentence.lower())\n",
        "    inputs = [wordtoix[i] for i in sentence.split(' ')]\n",
        "    inputs = [wordtoix[start_token]]+inputs+[wordtoix[end_token]]\n",
        "    inputs = pad_sequences([inputs],maxlen=max_len_q, padding='post')\n",
        "\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, GRU_units))]\n",
        "    enc_out, enc_hidden_f, enc_hidden_b = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
        "    dec_input = tf.expand_dims([wordtoix[start_token]], 1)\n",
        "\n",
        "    for t in range(max_len_a):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = K.get_value(attention_weights)\n",
        "        \n",
        "        predicted_id =  K.get_value(tf.argmax(predictions[0]))       \n",
        "\n",
        "        if ixtoword[predicted_id] == end_token:\n",
        "            return result, sentence, attention_plot\n",
        "        \n",
        "        result += ixtoword[predicted_id] + ' '\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 1)\n",
        "\n",
        "    return result, sentence, attention_plot\n",
        "\n",
        "def answer(sentence, training=False):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    \n",
        "    if training:\n",
        "        return result\n",
        "    \n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted answer: {}'.format(result))\n",
        "    attention_plot = attention_plot[1:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' ')[:-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl-ffZTS_dtP"
      },
      "source": [
        "# Greedy decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xacfBuH7dxUw"
      },
      "source": [
        "Greedy decoding is a decoding method in which we simply choose the highest softmax value word from decoder output for each time stage. On a single time-step stage, this decoding method is optimal. It is common in neural machine translation systems to use a beam-search to sample the probabilities for the words in the sequence output by the model.\n",
        "\n",
        "The wider the beam width, the more exhaustive the search, and, it is believed, the better the results.\n",
        "\n",
        "The results showed that a modest beam-width of 3-5 performed the best, which could be improved only very slightly through the use of length penalties. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pa46aAvvIvsx"
      },
      "outputs": [],
      "source": [
        "def beam_search(sentence, k=5, maxsample=max_len_a, use_unk=False, oov=None, eos=end_token):\n",
        "\n",
        "    \n",
        "    dead_k = 0 # samples that reached eos\n",
        "    dead_samples = []\n",
        "    dead_scores = []\n",
        "    live_k = 1 # samples that did not yet reached eos\n",
        "    live_samples = [[wordtoix[start_token]]]\n",
        "    live_scores = [0]\n",
        "\n",
        "    sentence = unicode_to_ascii(sentence.lower())\n",
        "    inputs = [wordtoix[i] for i in sentence.split(' ')]\n",
        "    inputs = [wordtoix[start_token]]+inputs+[wordtoix[end_token]]\n",
        "    inputs = pad_sequences([inputs],maxlen=max_len_q, padding='post')\n",
        "\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    hidden = [tf.zeros((1, GRU_units))]\n",
        "    enc_out, enc_hidden_f, enc_hidden_b = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
        "    dec_input = tf.expand_dims([wordtoix[start_token]], 0)\n",
        "        \n",
        "    while live_k and dead_k < k:\n",
        "        # for every possible live sample calc prob for every possible label \n",
        "        predictions, dec_hidden, _ = decoder(dec_input,dec_hidden,enc_out)\n",
        "        probs = K.get_value(predictions[0])\n",
        "        # total score for every sample is sum of -log of word prb\n",
        "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
        "        if not use_unk and oov is not None:\n",
        "            cand_scores[:,oov] = 1e20\n",
        "        cand_flat = cand_scores.flatten()\n",
        "\n",
        "        # find the best (lowest) scores we have from all possible samples and new words\n",
        "        ranks_flat = cand_flat.argsort()[:(k-dead_k)]\n",
        "        live_scores = cand_flat[ranks_flat]\n",
        "\n",
        "        # append the new words to their appropriate live sample\n",
        "        voc_size = vocab_len\n",
        "        live_samples = [live_samples[r//voc_size]+[r%voc_size] for r in ranks_flat]\n",
        "\n",
        "        # live samples that should be dead are...\n",
        "        zombie = [s[-1] == eos or len(s) >= maxsample for s in live_samples]\n",
        "        \n",
        "        # add zombies to the dead\n",
        "        dead_samples += [s for s,z in zip(live_samples,zombie) if z]  # remove first label == empty\n",
        "        dead_scores += [s for s,z in zip(live_scores,zombie) if z]\n",
        "        dead_k = len(dead_samples)\n",
        "        # remove zombies from the living \n",
        "        live_samples = [s for s,z in zip(live_samples,zombie) if not z]\n",
        "        live_scores = [s for s,z in zip(live_scores,zombie) if not z]\n",
        "        live_k = len(live_samples)\n",
        "\n",
        "    final_samples = dead_samples + live_samples\n",
        "    final_scores = dead_scores + live_scores   \n",
        "    \n",
        "    # cutting the strong where end_token is encounterd\n",
        "    for i in range(len(final_scores)):\n",
        "        final_scores[i] /= len(final_samples[i]) # normalizing the scores\n",
        "    \n",
        "    final_result =[]\n",
        "    \n",
        "    for i in range(len(final_scores)):\n",
        "        final_result.append((final_scores[i],final_samples[i]))\n",
        "    \n",
        "    final_list_ix = max(final_result)[1]\n",
        "    final_list_word = [ixtoword[f] for f in final_list_ix]\n",
        "    final_sentence = ' '.join(final_list_word[1:])\n",
        "    end_ix = final_sentence.find(end_token)\n",
        "    return final_sentence[:end_ix]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDhPN8kj-4Da"
      },
      "source": [
        "# Training procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYjEUAtO--f_"
      },
      "source": [
        "# Masked loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puOSiJijaOwW"
      },
      "source": [
        "We can't simply consider all elements of the tensor when evaluating loss because we're dealing with batches of padded sequences. Based on our decoder's output tensor, the target tensor, and a binary mask tensor describing the padding of the target tensor, we define a function to measure our loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xN3CisgKIy7W"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(init_lr)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = K.sparse_categorical_crossentropy(real, pred, from_logits= False)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "832td4vpI1Ok"
      },
      "outputs": [],
      "source": [
        "# checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(str(emb_dim)+\"-ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbaDnfr3_Elp"
      },
      "source": [
        "# Single training iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfRcNM1TbJcs"
      },
      "source": [
        "The algorithm for a single training iteration is contained in the train_step function (a single batch of inputs). To help with convergence, we'll use teacher forcing. This means that we use the current target word as the decoder's next input rather than the decoder's current guess in some probabilities. This technique serves as decoder training wheels, allowing for more effective training. However, since the decoder may not have had enough time to truly craft its own output sequences during training, teacher forcing can lead to model instability during inference. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7yeWUoboI8JK"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden_f, enc_hidden_b = encoder(inp, enc_hidden)\n",
        "        \n",
        "        dec_hidden = Concatenate(axis=-1)([enc_hidden_f, enc_hidden_b])\n",
        "        dec_input = tf.expand_dims([wordtoix[start_token]] * batch_size, 1) # dec_input initially == start_token\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            \n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(targ[:, t], predictions) # each time just use one timestep output\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1) # expected output at this time becomes input for next timestep\n",
        "            \n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eH-SlVY_JA3n"
      },
      "outputs": [],
      "source": [
        "history={'loss':[]}\n",
        "smallest_loss = np.inf\n",
        "best_ep = 1\n",
        "EPOCHS = 150 # but 150 is enough\n",
        "enc_hidden = encoder.initialize_hidden_state()\n",
        "steps_per_epoch = len(pairs_final_train)//batch_size # used for caculating number of batches\n",
        "current_ep = 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snJApfQJ4WmT"
      },
      "source": [
        "We are creating a test_bot to monitor our training in every time step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BsYIKbz1JC7Y"
      },
      "outputs": [],
      "source": [
        "def test_bot(k = 5, beam = False):\n",
        "    print('#'*20)\n",
        "    q = 'Hello'\n",
        "    print('Greedy| Q:',q,'  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "    q = 'How are you'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "\n",
        "    q = 'What are you doing'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "    q = 'What is your favorite restaurant'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "  \n",
        "    q = 'Do you want to go out'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('#'*20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPFs5cNX_OuX"
      },
      "source": [
        "# Training iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz9who24c69J"
      },
      "source": [
        "It's finally time to link the entire training procedure to the data. Given the passed models, optimizers, data, and so on, the function is responsible for running n iterations of training. We've already done the heavy lifting with the train_step function, so this function is self-explanatory.\n",
        "\n",
        "One thing to keep in mind is that when we save our model, the encoder and decoder parameters, the optimizer parameters, the loss, the iteration, and so on are all saved. This method of saving the model will give us the most flexibility with the checkpoint. We can use the model parameters to run inference after loading a checkpoint, or we can begin training where we left off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK75PGx14ytY"
      },
      "source": [
        "Now we can load our best model and chat with our system. We also plot the attention weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OHvEyKp8q5qI",
        "outputId": "edee865c-0d71-421a-b05b-dbfe892d765b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 598 Loss: 2.0928\n",
            "Epoch 1 Batch 1196 Loss: 2.3595\n",
            "Epoch 1 Batch 1794 Loss: 2.1612\n",
            "Epoch 1 Batch 2392 Loss: 2.1685\n",
            "Epoch 1 Batch 2990 Loss: 1.9225\n",
            "Epoch 1 Batch 3588 Loss: 2.3047\n",
            "\n",
            "*** Epoch 1 Loss 2.1114 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: i am not you are you \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not you are not you \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not you are not you \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not you are not you \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not you are not you \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  1\n",
            "Time  711.711 sec\n",
            "\n",
            "========================================\n",
            "Epoch 2 Batch 598 Loss: 1.7629\n",
            "Epoch 2 Batch 1196 Loss: 2.4027\n",
            "Epoch 2 Batch 1794 Loss: 2.2983\n",
            "Epoch 2 Batch 2392 Loss: 2.3483\n",
            "Epoch 2 Batch 2990 Loss: 1.7592\n",
            "Epoch 2 Batch 3588 Loss: 2.1962\n",
            "\n",
            "*** Epoch 2 Loss 1.8766 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: i am not a little time \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not a little time \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not a little time \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not a little time \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not a little time \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  2\n",
            "Time  645.870 sec\n",
            "\n",
            "========================================\n",
            "Epoch 3 Batch 598 Loss: 1.6533\n",
            "Epoch 3 Batch 1196 Loss: 2.4660\n",
            "Epoch 3 Batch 1794 Loss: 2.2292\n",
            "Epoch 3 Batch 2392 Loss: 2.2519\n",
            "Epoch 3 Batch 2990 Loss: 1.8556\n",
            "Epoch 3 Batch 3588 Loss: 2.3515\n",
            "\n",
            "*** Epoch 3 Loss 1.8105 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: yes \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not going to do \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  3\n",
            "Time  645.484 sec\n",
            "\n",
            "========================================\n",
            "Epoch 4 Batch 598 Loss: 1.4626\n",
            "Epoch 4 Batch 1196 Loss: 2.4979\n",
            "Epoch 4 Batch 1794 Loss: 2.0355\n",
            "Epoch 4 Batch 2392 Loss: 2.0857\n",
            "Epoch 4 Batch 2990 Loss: 1.8770\n",
            "Epoch 4 Batch 3588 Loss: 2.2315\n",
            "\n",
            "*** Epoch 4 Loss 1.7723 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not going to do \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  4\n",
            "Time  645.628 sec\n",
            "\n",
            "========================================\n",
            "Epoch 5 Batch 598 Loss: 1.4714\n",
            "Epoch 5 Batch 1196 Loss: 2.1481\n",
            "Epoch 5 Batch 1794 Loss: 1.9273\n",
            "Epoch 5 Batch 2392 Loss: 2.0420\n",
            "Epoch 5 Batch 2990 Loss: 1.8207\n",
            "Epoch 5 Batch 3588 Loss: 2.1801\n",
            "\n",
            "*** Epoch 5 Loss 1.7450 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best epoch so far:  5\n",
            "Time  646.135 sec\n",
            "\n",
            "========================================\n",
            "Epoch 6 Batch 598 Loss: 1.5533\n",
            "Epoch 6 Batch 1196 Loss: 1.9264\n",
            "Epoch 6 Batch 1794 Loss: 1.7900\n",
            "Epoch 6 Batch 2392 Loss: 1.9209\n",
            "Epoch 6 Batch 2990 Loss: 1.6262\n",
            "Epoch 6 Batch 3588 Loss: 1.9766\n",
            "\n",
            "*** Epoch 6 Loss 1.7229 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  6\n",
            "Time  645.964 sec\n",
            "\n",
            "========================================\n",
            "Epoch 7 Batch 598 Loss: 1.6286\n",
            "Epoch 7 Batch 1196 Loss: 2.1568\n",
            "Epoch 7 Batch 1794 Loss: 2.1013\n",
            "Epoch 7 Batch 2392 Loss: 2.1567\n",
            "Epoch 7 Batch 2990 Loss: 1.5030\n",
            "Epoch 7 Batch 3588 Loss: 1.9658\n",
            "\n",
            "*** Epoch 7 Loss 1.7057 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  7\n",
            "Time  642.136 sec\n",
            "\n",
            "========================================\n",
            "Epoch 8 Batch 598 Loss: 1.5736\n",
            "Epoch 8 Batch 1196 Loss: 2.2428\n",
            "Epoch 8 Batch 1794 Loss: 2.1020\n",
            "Epoch 8 Batch 2392 Loss: 2.0912\n",
            "Epoch 8 Batch 2990 Loss: 1.7250\n",
            "Epoch 8 Batch 3588 Loss: 2.0673\n",
            "\n",
            "*** Epoch 8 Loss 1.6908 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not a long time \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  8\n",
            "Time  644.462 sec\n",
            "\n",
            "========================================\n",
            "Epoch 9 Batch 598 Loss: 1.4564\n",
            "Epoch 9 Batch 1196 Loss: 2.3036\n",
            "Epoch 9 Batch 1794 Loss: 2.1302\n",
            "Epoch 9 Batch 2392 Loss: 1.9710\n",
            "Epoch 9 Batch 2990 Loss: 1.7769\n",
            "Epoch 9 Batch 3588 Loss: 2.0775\n",
            "\n",
            "*** Epoch 9 Loss 1.6781 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  9\n",
            "Time  645.278 sec\n",
            "\n",
            "========================================\n",
            "Epoch 10 Batch 598 Loss: 1.4607\n",
            "Epoch 10 Batch 1196 Loss: 2.1603\n",
            "Epoch 10 Batch 1794 Loss: 1.9894\n",
            "Epoch 10 Batch 2392 Loss: 1.8974\n",
            "Epoch 10 Batch 2990 Loss: 1.7791\n",
            "Epoch 10 Batch 3588 Loss: 2.1195\n",
            "\n",
            "*** Epoch 10 Loss 1.6669 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best epoch so far:  10\n",
            "Time  646.671 sec\n",
            "\n",
            "========================================\n",
            "Epoch 11 Batch 598 Loss: 1.5013\n",
            "Epoch 11 Batch 1196 Loss: 1.8611\n",
            "Epoch 11 Batch 1794 Loss: 1.6810\n",
            "Epoch 11 Batch 2392 Loss: 1.9067\n",
            "Epoch 11 Batch 2990 Loss: 1.5815\n",
            "Epoch 11 Batch 3588 Loss: 1.9104\n",
            "\n",
            "*** Epoch 11 Loss 1.6564 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  11\n",
            "Time  645.084 sec\n",
            "\n",
            "========================================\n",
            "Epoch 12 Batch 598 Loss: 1.6045\n",
            "Epoch 12 Batch 1196 Loss: 2.0398\n",
            "Epoch 12 Batch 1794 Loss: 1.9366\n",
            "Epoch 12 Batch 2392 Loss: 2.0899\n",
            "Epoch 12 Batch 2990 Loss: 1.4180\n",
            "Epoch 12 Batch 3588 Loss: 1.8425\n",
            "\n",
            "*** Epoch 12 Loss 1.6477 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  12\n",
            "Time  644.747 sec\n",
            "\n",
            "========================================\n",
            "Epoch 13 Batch 598 Loss: 1.5420\n",
            "Epoch 13 Batch 1196 Loss: 2.1105\n",
            "Epoch 13 Batch 1794 Loss: 2.0476\n",
            "Epoch 13 Batch 2392 Loss: 2.0990\n",
            "Epoch 13 Batch 2990 Loss: 1.6536\n",
            "Epoch 13 Batch 3588 Loss: 2.0053\n",
            "\n",
            "*** Epoch 13 Loss 1.6394 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  13\n",
            "Time  644.935 sec\n",
            "\n",
            "========================================\n",
            "Epoch 14 Batch 598 Loss: 1.5036\n",
            "Epoch 14 Batch 1196 Loss: 2.1800\n",
            "Epoch 14 Batch 1794 Loss: 2.0548\n",
            "Epoch 14 Batch 2392 Loss: 1.9243\n",
            "Epoch 14 Batch 2990 Loss: 1.6623\n",
            "Epoch 14 Batch 3588 Loss: 2.0007\n",
            "\n",
            "*** Epoch 14 Loss 1.6319 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  14\n",
            "Time  645.171 sec\n",
            "\n",
            "========================================\n",
            "Epoch 15 Batch 598 Loss: 1.3087\n",
            "Epoch 15 Batch 1196 Loss: 2.2400\n",
            "Epoch 15 Batch 1794 Loss: 1.9279\n",
            "Epoch 15 Batch 2392 Loss: 1.9511\n",
            "Epoch 15 Batch 2990 Loss: 1.6921\n",
            "Epoch 15 Batch 3588 Loss: 2.0585\n",
            "\n",
            "*** Epoch 15 Loss 1.6249 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best epoch so far:  15\n",
            "Time  645.470 sec\n",
            "\n",
            "========================================\n",
            "Epoch 16 Batch 598 Loss: 1.4217\n",
            "Epoch 16 Batch 1196 Loss: 1.9023\n",
            "Epoch 16 Batch 1794 Loss: 1.6797\n",
            "Epoch 16 Batch 2392 Loss: 1.9130\n",
            "Epoch 16 Batch 2990 Loss: 1.5957\n",
            "Epoch 16 Batch 3588 Loss: 1.7875\n",
            "\n",
            "*** Epoch 16 Loss 1.6184 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  16\n",
            "Time  643.310 sec\n",
            "\n",
            "========================================\n",
            "Epoch 17 Batch 598 Loss: 1.5081\n",
            "Epoch 17 Batch 1196 Loss: 1.7708\n",
            "Epoch 17 Batch 1794 Loss: 1.7241\n",
            "Epoch 17 Batch 2392 Loss: 1.7893\n",
            "Epoch 17 Batch 2990 Loss: 1.5326\n",
            "Epoch 17 Batch 3588 Loss: 1.5879\n",
            "\n",
            "*** Epoch 17 Loss 1.6119 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  17\n",
            "Time  642.210 sec\n",
            "\n",
            "========================================\n",
            "Epoch 18 Batch 598 Loss: 1.5239\n",
            "Epoch 18 Batch 1196 Loss: 1.9683\n",
            "Epoch 18 Batch 1794 Loss: 2.0237\n",
            "Epoch 18 Batch 2392 Loss: 2.0428\n",
            "Epoch 18 Batch 2990 Loss: 1.5479\n",
            "Epoch 18 Batch 3588 Loss: 1.8813\n",
            "\n",
            "*** Epoch 18 Loss 1.6065 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: no \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  18\n",
            "Time  642.270 sec\n",
            "\n",
            "========================================\n",
            "Epoch 19 Batch 598 Loss: 1.4993\n",
            "Epoch 19 Batch 1196 Loss: 2.1718\n",
            "Epoch 19 Batch 1794 Loss: 2.0204\n",
            "Epoch 19 Batch 2392 Loss: 2.0324\n",
            "Epoch 19 Batch 2990 Loss: 1.6406\n",
            "Epoch 19 Batch 3588 Loss: 2.0735\n",
            "\n",
            "*** Epoch 19 Loss 1.6011 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: no \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  19\n",
            "Time  642.675 sec\n",
            "\n",
            "========================================\n",
            "Epoch 20 Batch 598 Loss: 1.3456\n",
            "Epoch 20 Batch 1196 Loss: 2.2188\n",
            "Epoch 20 Batch 1794 Loss: 1.9011\n",
            "Epoch 20 Batch 2392 Loss: 1.9216\n",
            "Epoch 20 Batch 2990 Loss: 1.6948\n",
            "Epoch 20 Batch 3588 Loss: 1.9117\n",
            "\n",
            "*** Epoch 20 Loss 1.5969 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: no \n",
            "####################\n",
            "check point saved!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best epoch so far:  20\n",
            "Time  643.184 sec\n",
            "\n",
            "========================================\n",
            "Epoch 21 Batch 598 Loss: 1.3676\n",
            "Epoch 21 Batch 1196 Loss: 1.9265\n",
            "Epoch 21 Batch 1794 Loss: 1.8101\n",
            "Epoch 21 Batch 2392 Loss: 1.9058\n",
            "Epoch 21 Batch 2990 Loss: 1.6208\n",
            "Epoch 21 Batch 3588 Loss: 1.8583\n",
            "\n",
            "*** Epoch 21 Loss 1.5916 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: no \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  21\n",
            "Time  643.119 sec\n",
            "\n",
            "========================================\n",
            "Epoch 22 Batch 598 Loss: 1.4411\n",
            "Epoch 22 Batch 1196 Loss: 1.7573\n",
            "Epoch 22 Batch 1794 Loss: 1.6857\n",
            "Epoch 22 Batch 2392 Loss: 1.8007\n",
            "Epoch 22 Batch 2990 Loss: 1.4725\n",
            "Epoch 22 Batch 3588 Loss: 1.6382\n",
            "\n",
            "*** Epoch 22 Loss 1.5869 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: no \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  22\n",
            "Time  645.751 sec\n",
            "\n",
            "========================================\n",
            "Epoch 23 Batch 598 Loss: 1.5558\n",
            "Epoch 23 Batch 1196 Loss: 1.9238\n",
            "Epoch 23 Batch 1794 Loss: 1.9654\n",
            "Epoch 23 Batch 2392 Loss: 2.0106\n",
            "Epoch 23 Batch 2990 Loss: 1.4109\n",
            "Epoch 23 Batch 3588 Loss: 1.8230\n",
            "\n",
            "*** Epoch 23 Loss 1.5831 ***\n",
            "\n",
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i am not going to do \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: no \n",
            "####################\n",
            "check point saved!\n",
            "Best epoch so far:  23\n",
            "Time  642.131 sec\n",
            "\n",
            "========================================\n",
            "Epoch 24 Batch 598 Loss: 1.5176\n"
          ]
        }
      ],
      "source": [
        "batch_loss = K.constant(0)\n",
        "X, y = [], []\n",
        "def plot_history():\n",
        "    plt.figure(figsize=(4,3))\n",
        "    plt.plot(best_ep,smallest_loss,'ro')\n",
        "    plt.plot(history['loss'],'b-')\n",
        "    plt.legend(['best','loss'])\n",
        "    plt.show()\n",
        "\n",
        "for ep in range(current_ep,EPOCHS):\n",
        "    current_ep = ep    \n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    btch = 1\n",
        "\n",
        "    for p in pairs_final_train:     \n",
        "        \n",
        "        question = p[0]\n",
        "        label = p[1]\n",
        "        # find the index of each word of the caption in vocabulary\n",
        "        question_seq = [wordtoix[word] for word in question.split(' ') if word in wordtoix]\n",
        "        label_seq = [wordtoix[word] for word in label.split(' ') if word in wordtoix]\n",
        "        # encoder input and decoder input and label\n",
        "        enc_in_seq = pad_sequences([question_seq], maxlen=max_len_q, padding='post')[0]\n",
        "        dec_out_seq = pad_sequences([label_seq], maxlen=max_len_a, padding='post')[0]\n",
        "        \n",
        "        X.append(enc_in_seq)\n",
        "        y.append(dec_out_seq)\n",
        "\n",
        "        if len(X) == batch_size :\n",
        "            batch_loss = train_step(np.array(X), np.array(y), enc_hidden)\n",
        "            total_loss += batch_loss\n",
        "            X , y = [], []\n",
        "            btch += 1\n",
        "            if btch % (steps_per_epoch//6) == 0:\n",
        "                print('Epoch {} Batch {} Loss: {:.4f}'.format(ep , btch, K.get_value(batch_loss)))\n",
        "\n",
        "    epoch_loss =  K.get_value(total_loss) / steps_per_epoch\n",
        "    print('\\n*** Epoch {} Loss {:.4f} ***\\n'.format(ep ,epoch_loss))\n",
        "    history['loss'].append(epoch_loss)\n",
        "    \n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    test_bot(k=5)\n",
        "\n",
        "    if epoch_loss < smallest_loss:\n",
        "        smallest_loss = epoch_loss\n",
        "        best_ep = ep \n",
        "        print('check point saved!')\n",
        "    \n",
        "    if ep % 5 == 0:\n",
        "        plot_history()\n",
        "        \n",
        "    print('Best epoch so far: ',best_ep)\n",
        "    print('Time  {:.3f} sec\\n'.format(time.time() - start))\n",
        "\n",
        "    print('=' * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MnNalGJq-ru"
      },
      "source": [
        "Now we can load our best model and chat with our system. We also plot the attention weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpM1pMG9FpU8",
        "outputId": "7cfc4712-0b35-41c8-9636-58d125f0785f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd1d1ea1f90>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint.restore(str(emb_dim)+\"-ckpt-23\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr2-oAf0fBXT",
        "outputId": "2783a0c4-f290-4bd8-ff7c-9c2ade23b91e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not like a lot of repeat \n",
            "%\n",
            "Greedy| Q: What are you doing ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What is your favorite restaurant ?  A: i do not know \n",
            "%\n",
            "Greedy| Q: Do you want to go out ?  A: i am not going to do \n",
            "####################\n"
          ]
        }
      ],
      "source": [
        "test_bot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "FEu-hvlUfMh6",
        "outputId": "dc7cd0d4-8282-42f2-acae-897b1428a2cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: how old are you\n",
            "Predicted answer: six \n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "q = \"How old are you\"\n",
        "answer(q, training=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "TuRkjBWMFwnU",
        "outputId": "4c235c65-74c8-4da2-863d-d7762488a91f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: do you drink\n",
            "Predicted answer: no \n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "q = \"Do you drink\"\n",
        "answer(q, training=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6ztdvo_vD-i"
      },
      "source": [
        "**Task 3** Let's look at the attention weights and compare them after 5 and 23 epochs. Instead of evaluating by an automatic evaluation method, you can show us 10 predictions for each model. Answer the following questions based on your predictions, giving examples and/or explaining the evidence for your answers.\n",
        "\n",
        "\n",
        "*   Did the models learn to track local relations between words?\n",
        "\n",
        "*   Did the models attend to the least frequent tokens in an utterance? Can you see signs of overfitting in models that hang on to the least frequent words?\n",
        "\n",
        "*   Did the models learn to track some major syntactic relations in the utterances (e.g. subject-verb, verb-object)?\n",
        "\n",
        "*   Do they learn to encode some other linguistic features? Do they capture part-of-speech tags (POS tags)?\n",
        "\n",
        "*   What is the effect of more training on the length of responss?\n",
        "\n",
        "*   In some instances, by the time the decoder has to generate the beginning of a response, it may already forget the most relevant early query tokens. Can you suggest ways to change the training pipeline to make it easier for the model to remember the beginning of the query when it starts to generate the response?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71GKNnQ06GzQ"
      },
      "source": [
        "You now have a good understanding of how to build a generative conversational model. If you're interested, you can customise the chatbot's behaviour by adjusting the model and training parameters, as well as the data used to train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmu9OAyW_XQG"
      },
      "source": [
        "## Check Model Output after 5th and 20 epoch\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pzt5aThofJ0w"
      },
      "outputs": [],
      "source": [
        "def test_bot_final(k = 5, beam = False):\n",
        "    print('#'*20)\n",
        "    q = 'Hello'\n",
        "    print('Greedy| Q:',q,'  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "    q = 'How are you'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "\n",
        "    q = 'Which is your favourite place'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "\n",
        "    q = 'When will you come'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "  \n",
        "    q = 'Are you coming to the party'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "\n",
        "    q = 'which is your favourite restaurant'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "\n",
        "    q = 'What is your favorite film'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "\n",
        "    q = 'what is your age'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "\n",
        "    q = 'how is the temperature today'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('%')\n",
        "\n",
        "    q = 'What is the capital city of India'\n",
        "    print('Greedy| Q:',q,'?  A:',answer(q, training=True))\n",
        "    if beam:print('Beam ',k,'| ',q,'?  A:',beam_search(q,k=k))\n",
        "    print('#'*20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VkErDLZ_XQG"
      },
      "source": [
        "### For 5th Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BblkDb-bfJ0x",
        "outputId": "10fbe72b-c682-4838-9246-5246482c6a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "####################\n",
            "Greedy| Q: Hello   A: hi \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not \n",
            "%\n",
            "Greedy| Q: Which is your favourite place ?  A: i am not \n",
            "%\n",
            "Greedy| Q: When will you come ?  A: i am not \n",
            "%\n",
            "Greedy| Q: Are you coming to the party ?  A: i am not \n",
            "%\n",
            "Greedy| Q: which is your favourite restaurant ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What is your favorite film ?  A: i am not \n",
            "%\n",
            "Greedy| Q: what is your age ?  A: i am not \n",
            "%\n",
            "Greedy| Q: how is the temperature today ?  A: i am not \n",
            "%\n",
            "Greedy| Q: What is the capital city of India ?  A: i am not \n",
            "####################\n"
          ]
        }
      ],
      "source": [
        "checkpoint.restore(str(emb_dim)+\"-ckpt-5\")\n",
        "test_bot_final()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOrIlrDn_XQH"
      },
      "source": [
        "### For 20th Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u2usp24fJ0x",
        "outputId": "71af8f93-13e4-4451-95b8-302910650e26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "####################\n",
            "Greedy| Q: Hello   A: hello \n",
            "%\n",
            "Greedy| Q: How are you ?  A: i am not sure \n",
            "%\n",
            "Greedy| Q: Which is your favourite place ?  A: yes \n",
            "%\n",
            "Greedy| Q: When will you come ?  A: i am sorry \n",
            "%\n",
            "Greedy| Q: Are you coming to the party ?  A: no \n",
            "%\n",
            "Greedy| Q: which is your favourite restaurant ?  A: oh \n",
            "%\n",
            "Greedy| Q: What is your favorite film ?  A: i am going to do it \n",
            "%\n",
            "Greedy| Q: what is your age ?  A: i am a witch \n",
            "%\n",
            "Greedy| Q: how is the temperature today ?  A: i do not know \n",
            "%\n",
            "Greedy| Q: What is the capital city of India ?  A: i am sorry i am sorry i am sorry i am sorry i am sorry i am sorry \n",
            "####################\n"
          ]
        }
      ],
      "source": [
        "checkpoint.restore(str(emb_dim)+\"-ckpt-20\")\n",
        "test_bot_final()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "7001_2021_22_lab12_Seq2seq_Dialogue_Model_without_answers (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}